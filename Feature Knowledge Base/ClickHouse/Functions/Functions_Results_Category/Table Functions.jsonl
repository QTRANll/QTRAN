{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/azureBlobStorage"], "Title": ["Virtual Columns"], "Feature": ["Virtual Columns"], "Description": ["Virtual Columns", "_path \u2014 Path to the file. Type: LowCardinalty(String)._file \u2014 Name of the file. Type: LowCardinalty(String)._size \u2014 Size of the file in bytes. Type: Nullable(UInt64). If the file size is unknown, the value is NULL._time \u2014 Last modified time of the file. Type: Nullable(DateTime). If the time is unknown, the value is NULL.", "See Also", "AzureBlobStorage Table Engine"], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/azureBlobStorage"], "Title": ["Hive-style partitioning"], "Feature": ["Hive-style partitioning"], "Description": ["Hive-style partitioning", "When setting use_hive_partitioning is set to 1, ClickHouse will detect Hive-style partitioning in the path (/name=value/) and will allow to use partition columns as virtual columns in the query. These virtual columns will have the same names as in the partitioned path, but starting with _."], "Examples": ["SET use_hive_partitioning = 1;SELECT * from azureBlobStorage(config, storage_account_url='...', container='...', blob_path='http://data/path/date=*/country=*/code=*/*.parquet') where _date > '2020-01-01' and _country = 'Netherlands' and _code = 42;"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/deltalake"], "Title": ["Syntax"], "Feature": ["deltaLake(url [,aws_access_key_id, aws_secret_access_key] [,format] [,structure] [,compression])"], "Description": ["Syntax", "deltaLake(url [,aws_access_key_id, aws_secret_access_key] [,format] [,structure] [,compression])"], "Examples": ["deltaLake(url [,aws_access_key_id, aws_secret_access_key] [,format] [,structure] [,compression])"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/deltalake"], "Title": ["Arguments"], "Feature": ["Arguments"], "Description": ["Arguments", "url \u2014 Bucket url with path to existing Delta Lake table in S3.aws_access_key_id, aws_secret_access_key - Long-term credentials for the AWS account user.  You can use these to authenticate your requests. These parameters are optional. If credentials are not specified, they are used from the ClickHouse configuration. For more information see Using S3 for Data Storage.format \u2014 The format of the file.structure \u2014 Structure of the table. Format 'column1_name column1_type, column2_name column2_type, ...'.compression \u2014 Parameter is optional. Supported values: none, gzip/gz, brotli/br, xz/LZMA, zstd/zst. By default, compression will be autodetected by the file extension.", "Returned value", "A table with the specified structure for reading data in the specified Delta Lake table in S3."], "Examples": ["SELECT    URL,    UserAgentFROM deltaLake('https://clickhouse-public-datasets.s3.amazonaws.com/delta_lake/hits/')WHERE URL IS NOT NULLLIMIT 2"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/engines/table-functions/executable"], "Title": ["Syntax"], "Feature": ["executable(script_name, format, structure, [input_query...] [,SETTINGS ...])"], "Description": ["Syntax", "The executable table function requires three parameters and accepts an optional list of input queries:", "executable(script_name, format, structure, [input_query...] [,SETTINGS ...])", "script_name: the file name of the script. saved in the user_scripts folder (the default folder of the user_scripts_path setting)format: the format of the generated tablestructure: the table schema of the generated tableinput_query: an optional query (or collection or queries) whose results are passed to the script via stdin", "NoteIf you are going to invoke the same script repeatedly with the same input queries, consider using the Executable table engine.", "The following Python script is named generate_random.py and is saved in the user_scripts folder. It reads in a number i and prints i random strings, with each string preceded by a number that is separated by a tab:", "#!/usr/local/bin/python3.9import sysimport stringimport randomdef main():    # Read input value    for number in sys.stdin:        i = int(number)        # Generate some random rows        for id in range(0, i):            letters = string.ascii_letters            random_string =  ''.join(random.choices(letters ,k=10))            print(str(id) + '\\t' + random_string + '\\n', end='')        # Flush results to stdout        sys.stdout.flush()if __name__ == \"__main__\":    main()", "Let's invoke the script and have it generate 10 random strings:", "SELECT * FROM executable('generate_random.py', TabSeparated, 'id UInt32, random String', (SELECT 10))", "The response looks like:", "\u250c\u2500id\u2500\u252c\u2500random\u2500\u2500\u2500\u2500\u2500\u2510\u2502  0 \u2502 xheXXCiSkH \u2502\u2502  1 \u2502 AqxvHAoTrl \u2502\u2502  2 \u2502 JYvPCEbIkY \u2502\u2502  3 \u2502 sWgnqJwGRm \u2502\u2502  4 \u2502 fTZGrjcLon \u2502\u2502  5 \u2502 ZQINGktPnd \u2502\u2502  6 \u2502 YFSvGGoezb \u2502\u2502  7 \u2502 QyMJJZOOia \u2502\u2502  8 \u2502 NfiyDDhmcI \u2502\u2502  9 \u2502 REJRdJpWrg \u2502\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518"], "Examples": ["executable(script_name, format, structure, [input_query...] [,SETTINGS ...])", "SELECT * FROM executable('generate_random.py', TabSeparated, 'id UInt32, random String', (SELECT 10))"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/engines/table-functions/executable"], "Title": ["Settings"], "Feature": ["Settings"], "Description": ["Settings", "send_chunk_header - controls whether to send row count before sending a chunk of data to process. Default value is false.pool_size \u2014 Size of pool. If 0 is specified as pool_size then there is no pool size restrictions. Default value is 16.max_command_execution_time \u2014 Maximum executable script command execution time for processing block of data. Specified in seconds. Default value is 10.command_termination_timeout \u2014 executable script should contain main read-write loop. After table function is destroyed, pipe is closed, and executable file will have command_termination_timeout seconds to shutdown, before ClickHouse will send SIGTERM signal to child process. Specified in seconds. Default value is 10.command_read_timeout - timeout for reading data from command stdout in milliseconds. Default value 10000.command_write_timeout - timeout for writing data to command stdin in milliseconds. Default value 10000."], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/engines/table-functions/executable"], "Title": ["Passing Query Results to a Script"], "Feature": ["Passing Query Results to a Script"], "Description": ["Passing Query Results to a Script", "Be sure to check out the example in the Executable table engine on how to pass query results to a script. Here is how you execute the same script in that example using the executable table function:", "SELECT * FROM executable(    'sentiment.py',    TabSeparated,    'id UInt64, sentiment Float32',    (SELECT id, comment FROM hackernews WHERE id > 0 AND comment != '' LIMIT 20));"], "Examples": ["SELECT * FROM executable(    'sentiment.py',    TabSeparated,    'id UInt64, sentiment Float32',    (SELECT id, comment FROM hackernews WHERE id > 0 AND comment != '' LIMIT 20));"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/file"], "Title": ["Examples for Writing to a File"], "Feature": ["Examples for Writing to a File"], "Description": ["Examples for Writing to a File", "Write to a TSV file", "INSERT INTO TABLE FUNCTIONfile('test.tsv', 'TSV', 'column1 UInt32, column2 UInt32, column3 UInt32')VALUES (1, 2, 3), (3, 2, 1), (1, 3, 2)", "As a result, the data is written into the file test.tsv:", "# cat /var/lib/clickhouse/user_files/test.tsv1   2   33   2   11   3   2", "Partitioned write to multiple TSV files", "If you specify a PARTITION BY expression when inserting data into a table function of type file(), then a separate file is created for each partition. Splitting the data into separate files helps to improve performance of read operations.", "INSERT INTO TABLE FUNCTIONfile('test_{_partition_id}.tsv', 'TSV', 'column1 UInt32, column2 UInt32, column3 UInt32')PARTITION BY column3VALUES (1, 2, 3), (3, 2, 1), (1, 3, 2)", "As a result, the data is written into three files: test_1.tsv, test_2.tsv, and test_3.tsv.", "# cat /var/lib/clickhouse/user_files/test_1.tsv3   2   1# cat /var/lib/clickhouse/user_files/test_2.tsv1   3   2# cat /var/lib/clickhouse/user_files/test_3.tsv1   2   3"], "Examples": ["INSERT INTO TABLE FUNCTIONfile('test.tsv', 'TSV', 'column1 UInt32, column2 UInt32, column3 UInt32')VALUES (1, 2, 3), (3, 2, 1), (1, 3, 2)", "INSERT INTO TABLE FUNCTIONfile('test_{_partition_id}.tsv', 'TSV', 'column1 UInt32, column2 UInt32, column3 UInt32')PARTITION BY column3VALUES (1, 2, 3), (3, 2, 1), (1, 3, 2)"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/file"], "Title": ["Examples for Reading from a File"], "Feature": ["Examples for Reading from a File"], "Description": ["Examples for Reading from a File", "SELECT from a CSV file", "First, set user_files_path in the server configuration and prepare a file test.csv:", "$ grep user_files_path /etc/clickhouse-server/config.xml    <user_files_path>/var/lib/clickhouse/user_files/</user_files_path>$ cat /var/lib/clickhouse/user_files/test.csv    1,2,3    3,2,1    78,43,45", "Then, read data from test.csv into a table and select its first two rows:", "SELECT * FROMfile('test.csv', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32')LIMIT 2;", "\u250c\u2500column1\u2500\u252c\u2500column2\u2500\u252c\u2500column3\u2500\u2510\u2502       1 \u2502       2 \u2502       3 \u2502\u2502       3 \u2502       2 \u2502       1 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "Inserting data from a file into a table", "INSERT INTO FUNCTIONfile('test.csv', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32')VALUES (1, 2, 3), (3, 2, 1);", "SELECT * FROMfile('test.csv', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32');", "\u250c\u2500column1\u2500\u252c\u2500column2\u2500\u252c\u2500column3\u2500\u2510\u2502       1 \u2502       2 \u2502       3 \u2502\u2502       3 \u2502       2 \u2502       1 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "Reading data from table.csv, located in archive1.zip or/and archive2.zip:", "SELECT * FROM file('user_files/archives/archive{1..2}.zip :: table.csv');"], "Examples": ["SELECT * FROMfile('test.csv', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32')LIMIT 2;", "INSERT INTO FUNCTIONfile('test.csv', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32')VALUES (1, 2, 3), (3, 2, 1);", "SELECT * FROMfile('test.csv', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32');", "SELECT * FROM file('user_files/archives/archive{1..2}.zip :: table.csv');"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/file"], "Title": ["Globs in path"], "Feature": ["Globs in path"], "Description": ["Globs in path", "Paths may use globbing. Files must match the whole path pattern, not only the suffix or prefix. There is one exception that if the path refers to an existing\ndirectory and does not use globs, a * will be implicitly added to the path so\nall the files in the directory are selected.", "* \u2014 Represents arbitrarily many characters except / but including the empty string.? \u2014 Represents an arbitrary single character.{some_string,another_string,yet_another_one} \u2014 Substitutes any of strings 'some_string', 'another_string', 'yet_another_one'. The strings can contain the / symbol.{N..M} \u2014 Represents any number >= N and <= M.** - Represents all files inside a folder recursively.", "Constructions with {} are similar to the remote and hdfs table functions."], "Examples": ["SELECT count(*) FROM file('{some,another}_dir/some_file_{1..3}', 'TSV', 'name String, value UInt32');", "SELECT count(*) FROM file('{some,another}_dir/*', 'TSV', 'name String, value UInt32');", "SELECT count(*) FROM file('some_dir', 'TSV', 'name String, value UInt32');", "SELECT count(*) FROM file('big_dir/file{0..9}{0..9}{0..9}', 'CSV', 'name String, value UInt32');", "SELECT count(*) FROM file('big_dir/**', 'CSV', 'name String, value UInt32');", "SELECT count(*) FROM file('big_dir/**/file002', 'CSV', 'name String, value UInt32');"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/file"], "Title": ["Virtual Columns"], "Feature": ["Virtual Columns"], "Description": ["Virtual Columns", "_path \u2014 Path to the file. Type: LowCardinalty(String)._file \u2014 Name of the file. Type: LowCardinalty(String)._size \u2014 Size of the file in bytes. Type: Nullable(UInt64). If the file size is unknown, the value is NULL._time \u2014 Last modified time of the file. Type: Nullable(DateTime). If the time is unknown, the value is NULL."], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/file"], "Title": ["Hive-style partitioning"], "Feature": ["Hive-style partitioning"], "Description": ["Hive-style partitioning", "When setting use_hive_partitioning is set to 1, ClickHouse will detect Hive-style partitioning in the path (/name=value/) and will allow to use partition columns as virtual columns in the query. These virtual columns will have the same names as in the partitioned path, but starting with _."], "Examples": ["SET use_hive_partitioning = 1;SELECT * from file('data/path/date=*/country=*/code=*/*.parquet') where _date > '2020-01-01' and _country = 'Netherlands' and _code = 42;"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/file"], "Title": ["Settings"], "Feature": ["Settings"], "Description": ["Settings", "engine_file_empty_if_not_exists - allows to select empty data from a file that doesn't exist. Disabled by default.engine_file_truncate_on_insert - allows to truncate file before insert into it. Disabled by default.engine_file_allow_create_multiple_files - allows to create a new file on each insert if format has suffix. Disabled by default.engine_file_skip_empty_files - allows to skip empty files while reading. Disabled by default.storage_file_read_method - method of reading data from storage file, one of: read, pread, mmap (only for clickhouse-local). Default value: pread for clickhouse-server, mmap for clickhouse-local.", "See Also", "Virtual columnsRename files after processing"], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/fileCluster"], "Title": ["Globs in Path"], "Feature": ["Globs in Path"], "Description": ["Globs in Path", "All patterns supported by File table function are supported by FileCluster.", "See Also", "File table function"], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/format"], "Title": ["format"], "Feature": ["format(format_name, [structure], data)"], "Description": ["format", "Parses data from arguments according to specified input format. If structure argument is not specified, it's extracted from the data.", "Syntax", "format(format_name, [structure], data)", "Parameters", "format_name \u2014 The format of the data.structure - Structure of the table. Optional. Format 'column1_name column1_type, column2_name column2_type, ...'.data \u2014 String literal or constant expression that returns a string containing data in specified format", "Returned value", "A table with data parsed from data argument according to specified format and specified or extracted structure."], "Examples": ["SELECT * FROM format(JSONEachRow,$${\"a\": \"Hello\", \"b\": 111}{\"a\": \"World\", \"b\": 123}{\"a\": \"Hello\", \"b\": 112}{\"a\": \"World\", \"b\": 124}$$)", "DESC format(JSONEachRow,$${\"a\": \"Hello\", \"b\": 111}{\"a\": \"World\", \"b\": 123}{\"a\": \"Hello\", \"b\": 112}{\"a\": \"World\", \"b\": 124}$$)", "SELECT * FROM format(JSONEachRow, 'a String, b UInt32',$${\"a\": \"Hello\", \"b\": 111}{\"a\": \"World\", \"b\": 123}{\"a\": \"Hello\", \"b\": 112}{\"a\": \"World\", \"b\": 124}$$)"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/gcs"], "Title": ["Usage"], "Feature": ["Usage"], "Description": ["Usage", "Suppose that we have several files with following URIs on GCS:", "'https://storage.googleapis.com/my-test-bucket-768/some_prefix/some_file_1.csv''https://storage.googleapis.com/my-test-bucket-768/some_prefix/some_file_2.csv''https://storage.googleapis.com/my-test-bucket-768/some_prefix/some_file_3.csv''https://storage.googleapis.com/my-test-bucket-768/some_prefix/some_file_4.csv''https://storage.googleapis.com/my-test-bucket-768/another_prefix/some_file_1.csv''https://storage.googleapis.com/my-test-bucket-768/another_prefix/some_file_2.csv''https://storage.googleapis.com/my-test-bucket-768/another_prefix/some_file_3.csv''https://storage.googleapis.com/my-test-bucket-768/another_prefix/some_file_4.csv'", "Count the amount of rows in files ending with numbers from 1 to 3:", "SELECT count(*)FROM gcs('https://storage.googleapis.com/my-test-bucket-768/{some,another}_prefix/some_file_{1..3}.csv', 'CSV', 'name String, value UInt32')", "\u250c\u2500count()\u2500\u2510\u2502      18 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "Count the total amount of rows in all files in these two directories:", "SELECT count(*)FROM gcs('https://storage.googleapis.com/my-test-bucket-768/{some,another}_prefix/*', 'CSV', 'name String, value UInt32')", "\u250c\u2500count()\u2500\u2510\u2502      24 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "DangerIf your listing of files contains number ranges with leading zeros, use the construction with braces for each digit separately or use ?.", "Count the total amount of rows in files named file-000.csv, file-001.csv, ... , file-999.csv:", "SELECT count(*)FROM gcs('https://storage.googleapis.com/my-test-bucket-768/big_prefix/file-{000..999}.csv', 'CSV', 'name String, value UInt32');", "\u250c\u2500count()\u2500\u2510\u2502      12 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "Insert data into file test-data.csv.gz:", "INSERT INTO FUNCTION gcs('https://storage.googleapis.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip')VALUES ('test-data', 1), ('test-data-2', 2);", "Insert data into file test-data.csv.gz from existing table:", "INSERT INTO FUNCTION gcs('https://storage.googleapis.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip')SELECT name, value FROM existing_table;", "Glob ** can be used for recursive directory traversal. Consider the below example, it will fetch all files from my-test-bucket-768 directory recursively:", "SELECT * FROM gcs('https://storage.googleapis.com/my-test-bucket-768/**', 'CSV', 'name String, value UInt32', 'gzip');", "The below get data from all test-data.csv.gz files from any folder inside my-test-bucket directory recursively:", "SELECT * FROM gcs('https://storage.googleapis.com/my-test-bucket-768/**/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip');", "For production use cases it is recommended to use named collections. Here is the example:", "CREATE NAMED COLLECTION creds AS        access_key_id = '***',        secret_access_key = '***';SELECT count(*)FROM gcs(creds, url='https://s3-object-url.csv')"], "Examples": ["SELECT count(*)FROM gcs('https://storage.googleapis.com/my-test-bucket-768/{some,another}_prefix/some_file_{1..3}.csv', 'CSV', 'name String, value UInt32')", "SELECT count(*)FROM gcs('https://storage.googleapis.com/my-test-bucket-768/{some,another}_prefix/*', 'CSV', 'name String, value UInt32')", "SELECT count(*)FROM gcs('https://storage.googleapis.com/my-test-bucket-768/big_prefix/file-{000..999}.csv', 'CSV', 'name String, value UInt32');", "INSERT INTO FUNCTION gcs('https://storage.googleapis.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip')VALUES ('test-data', 1), ('test-data-2', 2);", "INSERT INTO FUNCTION gcs('https://storage.googleapis.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip')SELECT name, value FROM existing_table;", "SELECT * FROM gcs('https://storage.googleapis.com/my-test-bucket-768/**', 'CSV', 'name String, value UInt32', 'gzip');", "SELECT * FROM gcs('https://storage.googleapis.com/my-test-bucket-768/**/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip');", "CREATE NAMED COLLECTION creds AS        access_key_id = '***',        secret_access_key = '***';SELECT count(*)FROM gcs(creds, url='https://s3-object-url.csv')"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/gcs"], "Title": ["Partitioned Write"], "Feature": ["Partitioned Write"], "Description": ["Partitioned Write", "If you specify PARTITION BY expression when inserting data into GCS table, a separate file is created for each partition value. Splitting the data into separate files helps to improve reading operations efficiency."], "Examples": ["INSERT INTO TABLE FUNCTION    gcs('http://bucket.amazonaws.com/my_bucket/file_{_partition_id}.csv', 'CSV', 'a String, b UInt32, c UInt32')    PARTITION BY a VALUES ('x', 2, 3), ('x', 4, 5), ('y', 11, 12), ('y', 13, 14), ('z', 21, 22), ('z', 23, 24);", "INSERT INTO TABLE FUNCTION    gcs('http://bucket.amazonaws.com/my_bucket_{_partition_id}/file.csv', 'CSV', 'a UInt32, b UInt32, c UInt32')    PARTITION BY a VALUES (1, 2, 3), (1, 4, 5), (10, 11, 12), (10, 13, 14), (20, 21, 22), (20, 23, 24);"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/fuzzJSON"], "Title": ["Usage Example"], "Feature": ["Usage Example"], "Description": ["Usage Example", "CREATE NAMED COLLECTION json_fuzzer AS json_str='{}';SELECT * FROM fuzzJSON(json_fuzzer) LIMIT 3;", "{\"52Xz2Zd4vKNcuP2\":true}{\"UPbOhOQAdPKIg91\":3405264103600403024}{\"X0QUWu8yT\":[]}", "SELECT * FROM fuzzJSON(json_fuzzer, json_str='{\"name\" : \"value\"}', random_seed=1234) LIMIT 3;", "{\"key\":\"value\", \"mxPG0h1R5\":\"L-YQLv@9hcZbOIGrAn10%GA\"}{\"BRE3\":true}{\"key\":\"value\", \"SWzJdEJZ04nrpSfy\":[{\"3Q23y\":[]}]}", "SELECT * FROM fuzzJSON(json_fuzzer, json_str='{\"students\" : [\"Alice\", \"Bob\"]}', reuse_output=true) LIMIT 3;", "{\"students\":[\"Alice\", \"Bob\"], \"nwALnRMc4pyKD9Krv\":[]}{\"students\":[\"1rNY5ZNs0wU&82t_P\", \"Bob\"], \"wLNRGzwDiMKdw\":[{}]}{\"xeEk\":[\"1rNY5ZNs0wU&82t_P\", \"Bob\"], \"wLNRGzwDiMKdw\":[{}, {}]}", "SELECT * FROM fuzzJSON(json_fuzzer, json_str='{\"students\" : [\"Alice\", \"Bob\"]}', max_output_length=512) LIMIT 3;", "{\"students\":[\"Alice\", \"Bob\"], \"BREhhXj5\":true}{\"NyEsSWzJdeJZ04s\":[\"Alice\", 5737924650575683711, 5346334167565345826], \"BjVO2X9L\":true}{\"NyEsSWzJdeJZ04s\":[\"Alice\", 5737924650575683711, 5346334167565345826], \"BjVO2X9L\":true, \"k1SXzbSIz\":[{}]}", "SELECT * FROM fuzzJSON('{\"id\":1}', 1234) LIMIT 3;", "{\"id\":1, \"mxPG0h1R5\":\"L-YQLv@9hcZbOIGrAn10%GA\"}{\"BRjE\":16137826149911306846}{\"XjKE\":15076727133550123563}", "SELECT * FROM fuzzJSON(json_nc, json_str='{\"name\" : \"FuzzJSON\"}', random_seed=1337, malform_output=true) LIMIT 3;", "U\"name\":\"FuzzJSON*\"SpByjZKtr2VAyHCO\"falseh{\"name\"keFuzzJSON, \"g6vVO7TCIk\":jTt^{\"DBhz\":YFuzzJSON5}"], "Examples": ["CREATE NAMED COLLECTION json_fuzzer AS json_str='{}';SELECT * FROM fuzzJSON(json_fuzzer) LIMIT 3;", "SELECT * FROM fuzzJSON(json_fuzzer, json_str='{\"name\" : \"value\"}', random_seed=1234) LIMIT 3;", "SELECT * FROM fuzzJSON(json_fuzzer, json_str='{\"students\" : [\"Alice\", \"Bob\"]}', reuse_output=true) LIMIT 3;", "SELECT * FROM fuzzJSON(json_fuzzer, json_str='{\"students\" : [\"Alice\", \"Bob\"]}', max_output_length=512) LIMIT 3;", "SELECT * FROM fuzzJSON('{\"id\":1}', 1234) LIMIT 3;", "SELECT * FROM fuzzJSON(json_nc, json_str='{\"name\" : \"FuzzJSON\"}', random_seed=1337, malform_output=true) LIMIT 3;"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/fuzzQuery"], "Title": ["Usage Example"], "Feature": ["Usage Example"], "Description": ["Usage Example", "SELECT * FROM fuzzQuery('SELECT materialize(\\'a\\' AS key) GROUP BY key') LIMIT 2;", "   \u250c\u2500query\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25101. \u2502 SELECT 'a' AS key GROUP BY key                                 \u25022. \u2502 EXPLAIN PIPELINE compact = true SELECT 'a' AS key GROUP BY key \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518"], "Examples": ["SELECT * FROM fuzzQuery('SELECT materialize(\\'a\\' AS key) GROUP BY key') LIMIT 2;"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/generate"], "Title": ["Usage Example"], "Feature": ["Usage Example"], "Description": ["Usage Example", "SELECT * FROM generateRandom('a Array(Int8), d Decimal32(4), c Tuple(DateTime64(3), UUID)', 1, 10, 2) LIMIT 3;", "\u250c\u2500a\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500d\u2500\u252c\u2500c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 [77]     \u2502 -124167.6723 \u2502 ('2061-04-17 21:59:44.573','3f72f405-ec3e-13c8-44ca-66ef335f7835') \u2502\u2502 [32,110] \u2502 -141397.7312 \u2502 ('1979-02-09 03:43:48.526','982486d1-5a5d-a308-e525-7bd8b80ffa73') \u2502\u2502 [68]     \u2502  -67417.0770 \u2502 ('2080-03-12 14:17:31.269','110425e5-413f-10a6-05ba-fa6b3e929f15') \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "CREATE TABLE random (a Array(Int8), d Decimal32(4), c Tuple(DateTime64(3), UUID)) engine=Memory;INSERT INTO random SELECT * FROM generateRandom() LIMIT 2;SELECT * FROM random;", "\u250c\u2500a\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500d\u2500\u252c\u2500c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 []                           \u2502   68091.8197 \u2502 ('2037-10-02 12:44:23.368','039ecab7-81c2-45ee-208c-844e5c6c5652') \u2502\u2502 [8,-83,0,-22,65,9,-30,28,64] \u2502 -186233.4909 \u2502 ('2062-01-11 00:06:04.124','69563ea1-5ad1-f870-16d8-67061da0df25') \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "In combination with generateRandomStructure:", "SELECT * FROM generateRandom(generateRandomStructure(4, 101), 101) LIMIT 3;", "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500c1\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500c2\u2500\u252c\u2500c3\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 1996-04-15 06:40:05 \u2502 33954608387.2844801 \u2502 ['232.78.216.176','9.244.59.211','211.21.80.152','44.49.94.109','165.77.195.182','68.167.134.239','212.13.24.185','1.197.255.35','192.55.131.232'] \u2502 45d9:2b52:ab6:1c59:185b:515:c5b6:b781   \u2502\u2502 2063-01-13 01:22:27 \u2502 36155064970.9514454 \u2502 ['176.140.188.101']                                                                                                                                \u2502 c65a:2626:41df:8dee:ec99:f68d:c6dd:6b30 \u2502\u2502 2090-02-28 14:50:56 \u2502  3864327452.3901373 \u2502 ['155.114.30.32']                                                                                                                                  \u2502 57e9:5229:93ab:fbf3:aae7:e0e4:d1eb:86b  \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "With missing structure argument (in this case the structure is random):", "SELECT * FROM generateRandom() LIMIT 3;", "\u250c\u2500\u2500\u2500c1\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500c2\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500c3\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500c4\u2500\u252c\u2500c5\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 -128 \u2502  317300854 \u2502 2030-08-16 08:22:20.65 \u2502 1994-08-16 12:08:56.745 \u2502 R0qgiC46 \u2502\u2502   40 \u2502 -744906827 \u2502 2059-04-16 06:31:36.98 \u2502 1975-07-16 16:28:43.893 \u2502 PuH4M*MZ \u2502\u2502  -55 \u2502  698652232 \u2502 2052-08-04 20:13:39.68 \u2502 1998-09-20 03:48:29.279 \u2502          \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "With random seed both for random structure and random data:", "SELECT * FROM generateRandom(11) LIMIT 3;", "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500c1\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500c2\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500c3\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500c4\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500c5\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500c6\u2500\u252c\u2500c7\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500c8\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500c9\u2500\u2510\u2502  -77422512305044606600216318673365695785 \u2502   636812099959807642229.503817849012019401335326013846687285151335352272727523 \u2502 -34944452809785978175157829109276115789694605299387223845886143311647505037529 \u2502  544473976 \u2502 111220388331710079615337037674887514156741572807049614590010583571763691328563 \u2502       22016.22623506465 \u2502 {'2052-01-31 20:25:33':4306400876908509081044405485378623663,'1993-04-16 15:58:49':164367354809499452887861212674772770279,'2101-08-19 03:07:18':-60676948945963385477105077735447194811,'2039-12-22 22:31:39':-59227773536703059515222628111999932330} \u2502 a7b2:8f58:4d07:6707:4189:80cf:92f5:902d \u2502 1950-07-14 \u2502\u2502 -159940486888657488786004075627859832441 \u2502  629206527868163085099.8195700356331771569105231840157308480121506729741348442 \u2502 -53203761250367440823323469081755775164053964440214841464405368882783634063735 \u2502 2187136525 \u2502  94881662451116595672491944222189810087991610568040618106057495823910493624275 \u2502 1.3095786748458954e-104 \u2502 {}                                                                                                                                                                                                                                                      \u2502 a051:e3da:2e0a:c69:7835:aed6:e8b:3817   \u2502 1943-03-25 \u2502\u2502   -5239084224358020595591895205940528518 \u2502 -529937657954363597180.1709207212648004850138812370209091520162977548101577846 \u2502  47490343304582536176125359129223180987770215457970451211489086575421345731671 \u2502 1637451978 \u2502 101899445785010192893461828129714741298630410942962837910400961787305271699002 \u2502  2.4344456058391296e223 \u2502 {'2013-12-22 17:42:43':80271108282641375975566414544777036006,'2041-03-08 10:28:17':169706054082247533128707458270535852845,'1986-08-31 23:07:38':-54371542820364299444195390357730624136,'2094-04-23 21:26:50':7944954483303909347454597499139023465}  \u2502 1293:a726:e899:9bfc:8c6f:2aa1:22c9:b635 \u2502 1924-11-20 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "Note: generateRandom(generateRandomStructure(), [random seed], max_string_length, max_array_length) with large enough max_array_length can generate really huge output due to possible big nesting depth (up to 16) of complex types (Array, Tuple, Map, Nested)."], "Examples": ["SELECT * FROM generateRandom('a Array(Int8), d Decimal32(4), c Tuple(DateTime64(3), UUID)', 1, 10, 2) LIMIT 3;", "CREATE TABLE random (a Array(Int8), d Decimal32(4), c Tuple(DateTime64(3), UUID)) engine=Memory;INSERT INTO random SELECT * FROM generateRandom() LIMIT 2;SELECT * FROM random;", "SELECT * FROM generateRandom(generateRandomStructure(4, 101), 101) LIMIT 3;", "SELECT * FROM generateRandom() LIMIT 3;", "SELECT * FROM generateRandom(11) LIMIT 3;"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/generate"], "Title": ["Related content"], "Feature": ["Related content"], "Description": ["Related content", "Blog: Generating random data in ClickHouse"], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/mergeTreeIndex"], "Title": ["Usage Example"], "Feature": ["Usage Example"], "Description": ["Usage Example", "CREATE TABLE test_table(    `id` UInt64,    `n` UInt64,    `arr` Array(UInt64))ENGINE = MergeTreeORDER BY idSETTINGS index_granularity = 3, min_bytes_for_wide_part = 0, min_rows_for_wide_part = 8;INSERT INTO test_table SELECT number, number, range(number % 5) FROM numbers(5);INSERT INTO test_table SELECT number, number, range(number % 5) FROM numbers(10, 10);", "SELECT * FROM mergeTreeIndex(currentDatabase(), test_table, with_marks = true);", "\u250c\u2500part_name\u2500\u252c\u2500mark_number\u2500\u252c\u2500rows_in_granule\u2500\u252c\u2500id\u2500\u252c\u2500id.mark\u2500\u252c\u2500n.mark\u2500\u2500\u252c\u2500arr.size0.mark\u2500\u252c\u2500arr.mark\u2500\u2510\u2502 all_1_1_0 \u2502           0 \u2502               3 \u2502  0 \u2502 (0,0)   \u2502 (42,0)  \u2502 (NULL,NULL)    \u2502 (84,0)   \u2502\u2502 all_1_1_0 \u2502           1 \u2502               2 \u2502  3 \u2502 (133,0) \u2502 (172,0) \u2502 (NULL,NULL)    \u2502 (211,0)  \u2502\u2502 all_1_1_0 \u2502           2 \u2502               0 \u2502  4 \u2502 (271,0) \u2502 (271,0) \u2502 (NULL,NULL)    \u2502 (271,0)  \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u250c\u2500part_name\u2500\u252c\u2500mark_number\u2500\u252c\u2500rows_in_granule\u2500\u252c\u2500id\u2500\u252c\u2500id.mark\u2500\u252c\u2500n.mark\u2500\u252c\u2500arr.size0.mark\u2500\u252c\u2500arr.mark\u2500\u2510\u2502 all_2_2_0 \u2502           0 \u2502               3 \u2502 10 \u2502 (0,0)   \u2502 (0,0)  \u2502 (0,0)          \u2502 (0,0)    \u2502\u2502 all_2_2_0 \u2502           1 \u2502               3 \u2502 13 \u2502 (0,24)  \u2502 (0,24) \u2502 (0,24)         \u2502 (0,24)   \u2502\u2502 all_2_2_0 \u2502           2 \u2502               3 \u2502 16 \u2502 (0,48)  \u2502 (0,48) \u2502 (0,48)         \u2502 (0,80)   \u2502\u2502 all_2_2_0 \u2502           3 \u2502               1 \u2502 19 \u2502 (0,72)  \u2502 (0,72) \u2502 (0,72)         \u2502 (0,128)  \u2502\u2502 all_2_2_0 \u2502           4 \u2502               0 \u2502 19 \u2502 (0,80)  \u2502 (0,80) \u2502 (0,80)         \u2502 (0,160)  \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "DESCRIBE mergeTreeIndex(currentDatabase(), test_table, with_marks = true) SETTINGS describe_compact_output = 1;", "\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 part_name       \u2502 String                                                                                           \u2502\u2502 mark_number     \u2502 UInt64                                                                                           \u2502\u2502 rows_in_granule \u2502 UInt64                                                                                           \u2502\u2502 id              \u2502 UInt64                                                                                           \u2502\u2502 id.mark         \u2502 Tuple(offset_in_compressed_file Nullable(UInt64), offset_in_decompressed_block Nullable(UInt64)) \u2502\u2502 n.mark          \u2502 Tuple(offset_in_compressed_file Nullable(UInt64), offset_in_decompressed_block Nullable(UInt64)) \u2502\u2502 arr.size0.mark  \u2502 Tuple(offset_in_compressed_file Nullable(UInt64), offset_in_decompressed_block Nullable(UInt64)) \u2502\u2502 arr.mark        \u2502 Tuple(offset_in_compressed_file Nullable(UInt64), offset_in_decompressed_block Nullable(UInt64)) \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518"], "Examples": ["CREATE TABLE test_table(    `id` UInt64,    `n` UInt64,    `arr` Array(UInt64))ENGINE = MergeTreeORDER BY idSETTINGS index_granularity = 3, min_bytes_for_wide_part = 0, min_rows_for_wide_part = 8;INSERT INTO test_table SELECT number, number, range(number % 5) FROM numbers(5);INSERT INTO test_table SELECT number, number, range(number % 5) FROM numbers(10, 10);", "SELECT * FROM mergeTreeIndex(currentDatabase(), test_table, with_marks = true);", "DESCRIBE mergeTreeIndex(currentDatabase(), test_table, with_marks = true) SETTINGS describe_compact_output = 1;"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/hdfs"], "Title": ["Globs in path"], "Feature": ["Globs in path"], "Description": ["Globs in path", "Paths may use globbing. Files must match the whole path pattern, not only the suffix or prefix.", "* \u2014 Represents arbitrarily many characters except / but including the empty string.** \u2014 Represents all files inside a folder recursively.? \u2014 Represents an arbitrary single character.{some_string,another_string,yet_another_one} \u2014 Substitutes any of strings 'some_string', 'another_string', 'yet_another_one'. The strings can contain the / symbol.{N..M} \u2014 Represents any number >= N and <= M.", "Constructions with {} are similar to the remote and file table functions."], "Examples": ["SELECT count(*)FROM hdfs('hdfs://hdfs1:9000/{some,another}_dir/some_file_{1..3}', 'TSV', 'name String, value UInt32')", "SELECT count(*)FROM hdfs('hdfs://hdfs1:9000/{some,another}_dir/*', 'TSV', 'name String, value UInt32')", "SELECT count(*)FROM hdfs('hdfs://hdfs1:9000/big_dir/file{0..9}{0..9}{0..9}', 'CSV', 'name String, value UInt32')"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/hdfs"], "Title": ["Virtual Columns"], "Feature": ["Virtual Columns"], "Description": ["Virtual Columns", "_path \u2014 Path to the file. Type: LowCardinalty(String)._file \u2014 Name of the file. Type: LowCardinalty(String)._size \u2014 Size of the file in bytes. Type: Nullable(UInt64). If the size is unknown, the value is NULL._time \u2014 Last modified time of the file. Type: Nullable(DateTime). If the time is unknown, the value is NULL."], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/hdfs"], "Title": ["Hive-style partitioning"], "Feature": ["Hive-style partitioning"], "Description": ["Hive-style partitioning", "When setting use_hive_partitioning is set to 1, ClickHouse will detect Hive-style partitioning in the path (/name=value/) and will allow to use partition columns as virtual columns in the query. These virtual columns will have the same names as in the partitioned path, but starting with _."], "Examples": ["SET use_hive_partitioning = 1;SELECT * from HDFS('hdfs://hdfs1:9000/data/path/date=*/country=*/code=*/*.parquet') where _date > '2020-01-01' and _country = 'Netherlands' and _code = 42;"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/hdfs"], "Title": ["Storage Settings"], "Feature": ["Storage Settings"], "Description": ["Storage Settings", "hdfs_truncate_on_insert - allows to truncate file before insert into it. Disabled by default.hdfs_create_new_file_on_insert - allows to create a new file on each insert if format has suffix. Disabled by default.hdfs_skip_empty_files - allows to skip empty files while reading. Disabled by default.ignore_access_denied_multidirectory_globs - allows to ignore permission denied errors for multi-directory globs.", "See Also", "Virtual columns"], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/hdfsCluster"], "Title": ["hdfsCluster Table Function"], "Feature": ["hdfsCluster(cluster_name, URI, format, structure)"], "Description": ["hdfsCluster Table Function", "Allows processing files from HDFS in parallel from many nodes in a specified cluster. On initiator it creates a connection to all nodes in the cluster, discloses asterisks in HDFS file path, and dispatches each file dynamically. On the worker node it asks the initiator about the next task to process and processes it. This is repeated until all tasks are finished.", "Syntax", "hdfsCluster(cluster_name, URI, format, structure)", "Arguments", "cluster_name \u2014 Name of a cluster that is used to build a set of addresses and connection parameters to remote and local servers.URI \u2014 URI to a file or a bunch of files. Supports following wildcards in readonly mode: *, **, ?, {'abc','def'} and {N..M} where N, M \u2014 numbers, abc, def \u2014 strings. For more information see Wildcards In Path.format \u2014 The format of the file.structure \u2014 Structure of the table. Format 'column1_name column1_type, column2_name column2_type, ...'.", "Returned value", "A table with the specified structure for reading data in the specified file."], "Examples": ["SELECT count(*)FROM hdfsCluster('cluster_simple', 'hdfs://hdfs1:9000/{some,another}_dir/some_file_{1..3}', 'TSV', 'name String, value UInt32')", "SELECT count(*)FROM hdfsCluster('cluster_simple', 'hdfs://hdfs1:9000/{some,another}_dir/*', 'TSV', 'name String, value UInt32')"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/hudi"], "Title": ["Syntax"], "Feature": ["hudi(url [,aws_access_key_id, aws_secret_access_key] [,format] [,structure] [,compression])"], "Description": ["Syntax", "hudi(url [,aws_access_key_id, aws_secret_access_key] [,format] [,structure] [,compression])"], "Examples": ["hudi(url [,aws_access_key_id, aws_secret_access_key] [,format] [,structure] [,compression])"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/hudi"], "Title": ["Arguments"], "Feature": ["Arguments"], "Description": ["Arguments", "url \u2014 Bucket url with the path to an existing Hudi table in S3.aws_access_key_id, aws_secret_access_key - Long-term credentials for the AWS account user.  You can use these to authenticate your requests. These parameters are optional. If credentials are not specified, they are used from the ClickHouse configuration. For more information see Using S3 for Data Storage.format \u2014 The format of the file.structure \u2014 Structure of the table. Format 'column1_name column1_type, column2_name column2_type, ...'.compression \u2014 Parameter is optional. Supported values: none, gzip/gz, brotli/br, xz/LZMA, zstd/zst. By default, compression will be autodetected by the file extension.", "Returned value", "A table with the specified structure for reading data in the specified Hudi table in S3.", "See Also", "Hudi engine"], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/iceberg"], "Title": ["Syntax"], "Feature": ["icebergS3(url [, NOSIGN | access_key_id, secret_access_key, [session_token]] [,format] [,compression_method])icebergS3(named_collection[, option=value [,..]])icebergAzure(connection_string|storage_account_url, container_name, blobpath, [,account_name], [,account_key] [,format] [,compression_method])icebergAzure(named_collection[, option=value [,..]])icebergHDFS(path_to_table, [,format] [,compression_method])icebergHDFS(named_collection[, option=value [,..]])icebergLocal(path_to_table, [,format] [,compression_method])icebergLocal(named_collection[, option=value [,..]])"], "Description": ["Syntax", "icebergS3(url [, NOSIGN | access_key_id, secret_access_key, [session_token]] [,format] [,compression_method])icebergS3(named_collection[, option=value [,..]])icebergAzure(connection_string|storage_account_url, container_name, blobpath, [,account_name], [,account_key] [,format] [,compression_method])icebergAzure(named_collection[, option=value [,..]])icebergHDFS(path_to_table, [,format] [,compression_method])icebergHDFS(named_collection[, option=value [,..]])icebergLocal(path_to_table, [,format] [,compression_method])icebergLocal(named_collection[, option=value [,..]])"], "Examples": ["icebergS3(url [, NOSIGN | access_key_id, secret_access_key, [session_token]] [,format] [,compression_method])icebergS3(named_collection[, option=value [,..]])icebergAzure(connection_string|storage_account_url, container_name, blobpath, [,account_name], [,account_key] [,format] [,compression_method])icebergAzure(named_collection[, option=value [,..]])icebergHDFS(path_to_table, [,format] [,compression_method])icebergHDFS(named_collection[, option=value [,..]])icebergLocal(path_to_table, [,format] [,compression_method])icebergLocal(named_collection[, option=value [,..]])"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/iceberg"], "Title": ["Arguments"], "Feature": ["Arguments"], "Description": ["Arguments", "Description of the arguments coincides with description of arguments in table functions s3, azureBlobStorage, HDFS and file correspondingly.\nformat stands for the format of data files in the Iceberg table.", "Returned value\nA table with the specified structure for reading data in the specified Iceberg table."], "Examples": ["SELECT * FROM icebergS3('http://test.s3.amazonaws.com/clickhouse-bucket/test_table', 'test', 'test')"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/iceberg"], "Title": ["Defining a named collection"], "Feature": ["Defining a named collection"], "Description": ["Defining a named collection", "Here is an example of configuring a named collection for storing the URL and credentials:", "<clickhouse>    <named_collections>        <iceberg_conf>            <url>http://test.s3.amazonaws.com/clickhouse-bucket/</url>            <access_key_id>test<access_key_id>            <secret_access_key>test</secret_access_key>            <format>auto</format>            <structure>auto</structure>        </iceberg_conf>    </named_collections></clickhouse>", "SELECT * FROM icebergS3(iceberg_conf, filename = 'test_table')DESCRIBE icebergS3(iceberg_conf, filename = 'test_table')", "Aliases", "Table function iceberg is an alias to icebergS3 now.", "See Also", "Iceberg engine"], "Examples": ["SELECT * FROM icebergS3(iceberg_conf, filename = 'test_table')DESCRIBE icebergS3(iceberg_conf, filename = 'test_table')"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/input"], "Title": ["input"], "Feature": ["input"], "Description": ["input", "input(structure) - table function that allows effectively convert and insert data sent to the\nserver with given structure to the table with another structure.", "structure - structure of data sent to the server in following format 'column1_name column1_type, column2_name column2_type, ...'.\nFor example, 'id UInt32, name String'.", "This function can be used only in INSERT SELECT query and only once but otherwise behaves like ordinary table function\n(for example, it can be used in subquery, etc.).", "Data can be sent in any way like for ordinary INSERT query and passed in any available format\nthat must be specified in the end of query (unlike ordinary INSERT SELECT).", "The main feature of this function is that when server receives data from client it simultaneously converts it\naccording to the list of expressions in the SELECT clause and inserts into the target table. Temporary table\nwith all transferred data is not created."], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/jdbc"], "Title": ["jdbc"], "Feature": ["jdbc"], "Description": ["jdbc", "Noteclickhouse-jdbc-bridge contains experimental codes and is no longer supported. It may contain reliability issues and security vulnerabilities. Use it at your own risk.\nClickHouse recommend using built-in table functions in ClickHouse which provide a better alternative for ad-hoc querying scenarios (Postgres, MySQL, MongoDB, etc).", "jdbc(datasource, schema, table) - returns table that is connected via JDBC driver.", "This table function requires separate clickhouse-jdbc-bridge program to be running.\nIt supports Nullable types (based on DDL of remote table that is queried)."], "Examples": ["SELECT * FROM jdbc('jdbc:mysql://localhost:3306/?user=root&password=root', 'schema', 'table')", "SELECT * FROM jdbc('mysql://localhost:3306/?user=root&password=root', 'select * from schema.table')", "SELECT * FROM jdbc('mysql-dev?p1=233', 'num Int32', 'select toInt32OrZero(''{{p1}}'') as num')", "SELECT *FROM jdbc('mysql-dev?p1=233', 'num Int32', 'select toInt32OrZero(''{{p1}}'') as num')", "SELECT a.datasource AS server1, b.datasource AS server2, b.name AS dbFROM jdbc('mysql-dev?datasource_column', 'show databases') aINNER JOIN jdbc('self?datasource_column', 'show databases') b ON a.Database = b.name"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/merge"], "Title": ["merge"], "Feature": ["merge(['db_name',] 'tables_regexp')"], "Description": ["merge", "Creates a temporary Merge table. The table structure is taken from the first table encountered that matches the regular expression.", "Syntax", "merge(['db_name',] 'tables_regexp')", "Arguments", "db_name \u2014 Possible values (optional, default is currentDatabase()):database name,constant expression that returns a string with a database name, for example, currentDatabase(),REGEXP(expression), where expression is a regular expression to match the DB names.tables_regexp \u2014 A regular expression to match the table names in the specified DB or DBs.", "See Also", "Merge table engine"], "Examples": ["merge(['db_name',] 'tables_regexp')"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/mongodb"], "Title": ["mongodb"], "Feature": ["mongodb(host:port, database, collection, user, password, structure [, options])"], "Description": ["mongodb", "Allows SELECT queries to be performed on data that is stored on a remote MongoDB server.", "Syntax", "mongodb(host:port, database, collection, user, password, structure [, options])", "Arguments", "host:port \u2014 MongoDB server address.database \u2014 Remote database name.collection \u2014 Remote collection name.user \u2014 MongoDB user.password \u2014 User password.structure - The schema for the ClickHouse table returned from this function.options - MongoDB connection string options (optional parameter).", "TipIf you are using the MongoDB Atlas cloud offering please add these options:'connectTimeoutMS=10000&ssl=true&authSource=admin'", "Also, you can connect by URI:", "mongodb(uri, collection, structure)", "Arguments", "uri \u2014 Connection string.collection \u2014 Remote collection name.structure \u2014 The schema for the ClickHouse table returned from this function.", "Returned Value", "A table object with the same columns as the original MongoDB table."], "Examples": ["db.createUser({user:\"test_user\",pwd:\"password\",roles:[{role:\"readWrite\",db:\"test\"}]})db.createCollection(\"my_collection\")db.my_collection.insertOne(    { log_type: \"event\", host: \"120.5.33.9\", command: \"check-cpu-usage -w 75 -c 90\" })db.my_collection.insertOne(    { log_type: \"event\", host: \"120.5.33.4\", command: \"system-check\"})", "SELECT * FROM mongodb(    '127.0.0.1:27017',    'test',    'my_collection',    'test_user',    'password',    'log_type String, host String, command String',    'connectTimeoutMS=10000')", "SELECT * FROM mongodb(    'mongodb://test_user:password@127.0.0.1:27017/test?connectionTimeoutMS=10000',    'my_collection',    'log_type String, host String, command String')"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/mysql"], "Title": ["mysql"], "Feature": ["mysql({host:port, database, table, user, password[, replace_query, on_duplicate_clause] | named_collection[, option=value [,..]]})"], "Description": ["mysql", "Allows SELECT and INSERT queries to be performed on data that is stored on a remote MySQL server.", "Syntax", "mysql({host:port, database, table, user, password[, replace_query, on_duplicate_clause] | named_collection[, option=value [,..]]})", "Parameters", "host:port \u2014 MySQL server address.database \u2014 Remote database name.table \u2014 Remote table name.user \u2014 MySQL user.password \u2014 User password.replace_query \u2014 Flag that converts INSERT INTO queries to REPLACE INTO. Possible values:0 - The query is executed as INSERT INTO.1 - The query is executed as REPLACE INTO.on_duplicate_clause \u2014 The ON DUPLICATE KEY on_duplicate_clause expression that is added to the INSERT query. Can be specified only with replace_query = 0 (if you simultaneously pass replace_query = 1 and on_duplicate_clause, ClickHouse generates an exception).\nExample: INSERT INTO t (c1,c2) VALUES ('a', 2) ON DUPLICATE KEY UPDATE c2 = c2 + 1;on_duplicate_clause here is UPDATE c2 = c2 + 1. See the MySQL documentation to find which on_duplicate_clause you can use with the ON DUPLICATE KEY clause.", "Arguments also can be passed using named collections. In this case host and port should be specified separately. This approach is recommended for production environment.", "Simple WHERE clauses such as =, !=, >, >=, <, <= are currently executed on the MySQL server.", "The rest of the conditions and the LIMIT sampling constraint are executed in ClickHouse only after the query to MySQL finishes.", "Supports multiple replicas that must be listed by |. For example:", "SELECT name FROM mysql(`mysql{1|2|3}:3306`, 'mysql_database', 'mysql_table', 'user', 'password');", "or", "SELECT name FROM mysql(`mysql1:3306|mysql2:3306|mysql3:3306`, 'mysql_database', 'mysql_table', 'user', 'password');", "Returned Value", "A table object with the same columns as the original MySQL table.", "NoteIn the INSERT query to distinguish table function mysql(...) from table name with column names list, you must use keywords FUNCTION or TABLE FUNCTION. See examples below."], "Examples": ["SELECT * FROM mysql('localhost:3306', 'test', 'test', 'bayonet', '123');", "CREATE NAMED COLLECTION creds AS        host = 'localhost',        port = 3306,        database = 'test',        user = 'bayonet',        password = '123';SELECT * FROM mysql(creds, table='test');", "INSERT INTO FUNCTION mysql('localhost:3306', 'test', 'test', 'bayonet', '123', 1) (int_id, float) VALUES (1, 3);INSERT INTO TABLE FUNCTION mysql('localhost:3306', 'test', 'test', 'bayonet', '123', 0, 'UPDATE int_id = int_id + 1') (int_id, float) VALUES (1, 4);SELECT * FROM mysql('localhost:3306', 'test', 'test', 'bayonet', '123');", "CREATE TABLE mysql_copy(   `id` UInt64,   `datetime` DateTime('UTC'),   `description` String,)ENGINE = MergeTreeORDER BY (id,datetime);INSERT INTO mysql_copySELECT * FROM mysql('host:port', 'database', 'table', 'user', 'password');", "INSERT INTO mysql_copySELECT * FROM mysql('host:port', 'database', 'table', 'user', 'password')WHERE id > (SELECT max(id) from mysql_copy);"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/numbers"], "Title": ["numbers"], "Feature": ["SELECT * FROM numbers(10);SELECT * FROM numbers(0, 10);SELECT * FROM system.numbers LIMIT 10;SELECT * FROM system.numbers WHERE number BETWEEN 0 AND 9;SELECT * FROM system.numbers WHERE number IN (0, 1, 2, 3, 4, 5, 6, 7, 8, 9);"], "Description": ["numbers", "numbers(N) \u2013 Returns a table with the single \u2018number\u2019 column (UInt64) that contains integers from 0 to N-1.\nnumbers(N, M) - Returns a table with the single \u2018number\u2019 column (UInt64) that contains integers from N to (N + M - 1).\nnumbers(N, M, S) - Returns a table with the single \u2018number\u2019 column (UInt64) that contains integers from N to (N + M - 1) with step S.", "Similar to the system.numbers table, it can be used for testing and generating successive values, numbers(N, M) more efficient than system.numbers.", "The following queries are equivalent:", "SELECT * FROM numbers(10);SELECT * FROM numbers(0, 10);SELECT * FROM system.numbers LIMIT 10;SELECT * FROM system.numbers WHERE number BETWEEN 0 AND 9;SELECT * FROM system.numbers WHERE number IN (0, 1, 2, 3, 4, 5, 6, 7, 8, 9);", "And the following queries are equivalent:", "SELECT number * 2 FROM numbers(10);SELECT (number - 10) * 2 FROM numbers(10, 10);SELECT * FROM numbers(0, 20, 2);"], "Examples": ["-- Generate a sequence of dates from 2010-01-01 to 2010-12-31select toDate('2010-01-01') + number as d FROM numbers(365);"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/timeSeriesData"], "Title": ["timeSeriesData"], "Feature": ["timeSeriesData"], "Description": ["timeSeriesData", "timeSeriesData(db_name.time_series_table) - Returns the data table\nused by table db_name.time_series_table which table engine is TimeSeries:", "CREATE TABLE db_name.time_series_table ENGINE=TimeSeries DATA data_table", "The function also works if the data table is inner:", "CREATE TABLE db_name.time_series_table ENGINE=TimeSeries DATA INNER UUID '01234567-89ab-cdef-0123-456789abcdef'", "The following queries are equivalent:", "SELECT * FROM timeSeriesData(db_name.time_series_table);SELECT * FROM timeSeriesData('db_name.time_series_table');SELECT * FROM timeSeriesData('db_name', 'time_series_table');"], "Examples": ["CREATE TABLE db_name.time_series_table ENGINE=TimeSeries DATA data_table", "CREATE TABLE db_name.time_series_table ENGINE=TimeSeries DATA INNER UUID '01234567-89ab-cdef-0123-456789abcdef'", "SELECT * FROM timeSeriesData(db_name.time_series_table);SELECT * FROM timeSeriesData('db_name.time_series_table');SELECT * FROM timeSeriesData('db_name', 'time_series_table');"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/timeSeriesMetrics"], "Title": ["timeSeriesMetrics"], "Feature": ["timeSeriesMetrics"], "Description": ["timeSeriesMetrics", "timeSeriesMetrics(db_name.time_series_table) - Returns the metrics table\nused by table db_name.time_series_table which table engine is TimeSeries:", "CREATE TABLE db_name.time_series_table ENGINE=TimeSeries METRICS metrics_table", "The function also works if the metrics table is inner:", "CREATE TABLE db_name.time_series_table ENGINE=TimeSeries METRICS INNER UUID '01234567-89ab-cdef-0123-456789abcdef'", "The following queries are equivalent:", "SELECT * FROM timeSeriesMetrics(db_name.time_series_table);SELECT * FROM timeSeriesMetrics('db_name.time_series_table');SELECT * FROM timeSeriesMetrics('db_name', 'time_series_table');"], "Examples": ["CREATE TABLE db_name.time_series_table ENGINE=TimeSeries METRICS metrics_table", "CREATE TABLE db_name.time_series_table ENGINE=TimeSeries METRICS INNER UUID '01234567-89ab-cdef-0123-456789abcdef'", "SELECT * FROM timeSeriesMetrics(db_name.time_series_table);SELECT * FROM timeSeriesMetrics('db_name.time_series_table');SELECT * FROM timeSeriesMetrics('db_name', 'time_series_table');"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/timeSeriesTags"], "Title": ["timeSeriesTags"], "Feature": ["timeSeriesTags"], "Description": ["timeSeriesTags", "timeSeriesTags(db_name.time_series_table) - Returns the tags table\nused by table db_name.time_series_table which table engine is TimeSeries:", "CREATE TABLE db_name.time_series_table ENGINE=TimeSeries TAGS tags_table", "The function also works if the tags table is inner:", "CREATE TABLE db_name.time_series_table ENGINE=TimeSeries TAGS INNER UUID '01234567-89ab-cdef-0123-456789abcdef'", "The following queries are equivalent:", "SELECT * FROM timeSeriesTags(db_name.time_series_table);SELECT * FROM timeSeriesTags('db_name.time_series_table');SELECT * FROM timeSeriesTags('db_name', 'time_series_table');"], "Examples": ["CREATE TABLE db_name.time_series_table ENGINE=TimeSeries TAGS tags_table", "CREATE TABLE db_name.time_series_table ENGINE=TimeSeries TAGS INNER UUID '01234567-89ab-cdef-0123-456789abcdef'", "SELECT * FROM timeSeriesTags(db_name.time_series_table);SELECT * FROM timeSeriesTags('db_name.time_series_table');SELECT * FROM timeSeriesTags('db_name', 'time_series_table');"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/generate_series"], "Title": ["generate_series"], "Feature": ["generate_series"], "Description": ["generate_series", "generate_series(START, STOP) - Returns a table with the single \u2018generate_series\u2019 column (UInt64) that contains integers from start to stop inclusively.", "generate_series(START, STOP, STEP) - Returns a table with the single \u2018generate_series\u2019 column (UInt64) that contains integers from start to stop inclusively with spacing between values given by STEP. ", "The following queries return tables with the same content but different column names:", "SELECT * FROM numbers(10, 5);SELECT * FROM generate_series(10, 14);", "And the following queries return tables with the same content but different column names (but the second option is more efficient):", "SELECT * FROM numbers(10, 11) WHERE number % 3 == (10 % 3);SELECT * FROM generate_series(10, 20, 3) ;"], "Examples": ["SELECT * FROM numbers(10, 5);SELECT * FROM generate_series(10, 14);", "SELECT * FROM numbers(10, 11) WHERE number % 3 == (10 % 3);SELECT * FROM generate_series(10, 20, 3) ;"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/odbc"], "Title": ["Usage Example"], "Feature": ["Usage Example"], "Description": ["Usage Example", "Getting data from the local MySQL installation via ODBC", "This example is checked for Ubuntu Linux 18.04 and MySQL server 5.7.", "Ensure that unixODBC and MySQL Connector are installed.", "By default (if installed from packages), ClickHouse starts as user clickhouse. Thus you need to create and configure this user in the MySQL server.", "$ sudo mysql", "mysql> CREATE USER 'clickhouse'@'localhost' IDENTIFIED BY 'clickhouse';mysql> GRANT ALL PRIVILEGES ON *.* TO 'clickhouse'@'clickhouse' WITH GRANT OPTION;", "Then configure the connection in /etc/odbc.ini.", "$ cat /etc/odbc.ini[mysqlconn]DRIVER = /usr/local/lib/libmyodbc5w.soSERVER = 127.0.0.1PORT = 3306DATABASE = testUSERNAME = clickhousePASSWORD = clickhouse", "You can check the connection using the isql utility from the unixODBC installation.", "$ isql -v mysqlconn+-------------------------+| Connected!                            ||                                       |...", "Table in MySQL:", "mysql> CREATE TABLE `test`.`test` (    ->   `int_id` INT NOT NULL AUTO_INCREMENT,    ->   `int_nullable` INT NULL DEFAULT NULL,    ->   `float` FLOAT NOT NULL,    ->   `float_nullable` FLOAT NULL DEFAULT NULL,    ->   PRIMARY KEY (`int_id`));Query OK, 0 rows affected (0,09 sec)mysql> insert into test (`int_id`, `float`) VALUES (1,2);Query OK, 1 row affected (0,00 sec)mysql> select * from test;+------+----------+-----+----------+| int_id | int_nullable | float | float_nullable |+------+----------+-----+----------+|      1 |         NULL |     2 |           NULL |+------+----------+-----+----------+1 row in set (0,00 sec)", "Retrieving data from the MySQL table in ClickHouse:", "SELECT * FROM odbc('DSN=mysqlconn', 'test', 'test')", "\u250c\u2500int_id\u2500\u252c\u2500int_nullable\u2500\u252c\u2500float\u2500\u252c\u2500float_nullable\u2500\u2510\u2502      1 \u2502            0 \u2502     2 \u2502              0 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518"], "Examples": ["mysql> CREATE USER 'clickhouse'@'localhost' IDENTIFIED BY 'clickhouse';mysql> GRANT ALL PRIVILEGES ON *.* TO 'clickhouse'@'clickhouse' WITH GRANT OPTION;", "SELECT * FROM odbc('DSN=mysqlconn', 'test', 'test')"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/odbc"], "Title": ["See Also"], "Feature": ["See Also"], "Description": ["See Also", "ODBC dictionariesODBC table engine."], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/postgresql"], "Title": ["Implementation Details"], "Feature": ["SELECT name FROM postgresql(`postgres{1|2|3}:5432`, 'postgres_database', 'postgres_table', 'user', 'password');"], "Description": ["Implementation Details", "SELECT queries on PostgreSQL side run as COPY (SELECT ...) TO STDOUT inside read-only PostgreSQL transaction with commit after each SELECT query.", "Simple WHERE clauses such as =, !=, >, >=, <, <=, and IN are executed on the PostgreSQL server.", "All joins, aggregations, sorting, IN [ array ] conditions and the LIMIT sampling constraint are executed in ClickHouse only after the query to PostgreSQL finishes.", "INSERT queries on PostgreSQL side run as COPY \"table_name\" (field1, field2, ... fieldN) FROM STDIN inside PostgreSQL transaction with auto-commit after each INSERT statement.", "PostgreSQL Array types converts into ClickHouse arrays.", "NoteBe careful, in PostgreSQL an array data type column like Integer[] may contain arrays of different dimensions in different rows, but in ClickHouse it is only allowed to have multidimensional arrays of the same dimension in all rows.", "Supports multiple replicas that must be listed by |. For example:", "SELECT name FROM postgresql(`postgres{1|2|3}:5432`, 'postgres_database', 'postgres_table', 'user', 'password');", "or", "SELECT name FROM postgresql(`postgres1:5431|postgres2:5432`, 'postgres_database', 'postgres_table', 'user', 'password');", "Supports replicas priority for PostgreSQL dictionary source. The bigger the number in map, the less the priority. The highest priority is 0."], "Examples": ["SELECT * FROM postgresql('localhost:5432', 'test', 'test', 'postgresql_user', 'password') WHERE str IN ('test');", "CREATE NAMED COLLECTION mypg AS        host = 'localhost',        port = 5432,        database = 'test',        user = 'postgresql_user',        password = 'password';SELECT * FROM postgresql(mypg, table='test') WHERE str IN ('test');", "INSERT INTO TABLE FUNCTION postgresql('localhost:5432', 'test', 'test', 'postgrsql_user', 'password') (int_id, float) VALUES (2, 3);SELECT * FROM postgresql('localhost:5432', 'test', 'test', 'postgresql_user', 'password');", "CREATE TABLE pg_table_schema_with_dots (a UInt32)        ENGINE PostgreSQL('localhost:5432', 'clickhouse', 'nice.table', 'postgrsql_user', 'password', 'nice.schema');"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/postgresql"], "Title": ["Related content"], "Feature": ["Related content"], "Description": ["Related content", "Blog: ClickHouse and PostgreSQL - a match made in data heaven - part 1Blog: ClickHouse and PostgreSQL - a Match Made in Data Heaven - part 2", "Replicating or migrating Postgres data with with PeerDB", "In addition to table functions, you can always use PeerDB by ClickHouse to set up a continuous data pipeline from Postgres to ClickHouse. PeerDB is a tool designed specifically to replicate data from Postgres to ClickHouse using change data capture (CDC)."], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/redis"], "Title": ["Usage Example"], "Feature": ["Usage Example"], "Description": ["Usage Example", "Read from Redis:", "SELECT * FROM redis(    'redis1:6379',    'key',    'key String, v1 String, v2 UInt32')", "Insert into Redis:", "INSERT INTO TABLE FUNCTION redis(    'redis1:6379',    'key',    'key String, v1 String, v2 UInt32') values ('1', '1', 1);", "See Also", "The Redis table engineUsing redis as a dictionary source"], "Examples": ["SELECT * FROM redis(    'redis1:6379',    'key',    'key String, v1 String, v2 UInt32')", "INSERT INTO TABLE FUNCTION redis(    'redis1:6379',    'key',    'key String, v1 String, v2 UInt32') values ('1', '1', 1);"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/remote"], "Title": ["Syntax"], "Feature": ["remote(addresses_expr, [db, table, user [, password], sharding_key])remote(addresses_expr, [db.table, user [, password], sharding_key])remote(named_collection[, option=value [,..]])remoteSecure(addresses_expr, [db, table, user [, password], sharding_key])remoteSecure(addresses_expr, [db.table, user [, password], sharding_key])remoteSecure(named_collection[, option=value [,..]])"], "Description": ["Syntax", "remote(addresses_expr, [db, table, user [, password], sharding_key])remote(addresses_expr, [db.table, user [, password], sharding_key])remote(named_collection[, option=value [,..]])remoteSecure(addresses_expr, [db, table, user [, password], sharding_key])remoteSecure(addresses_expr, [db.table, user [, password], sharding_key])remoteSecure(named_collection[, option=value [,..]])"], "Examples": ["remote(addresses_expr, [db, table, user [, password], sharding_key])remote(addresses_expr, [db.table, user [, password], sharding_key])remote(named_collection[, option=value [,..]])remoteSecure(addresses_expr, [db, table, user [, password], sharding_key])remoteSecure(addresses_expr, [db.table, user [, password], sharding_key])remoteSecure(named_collection[, option=value [,..]])"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/remote"], "Title": ["Parameters"], "Feature": ["Parameters"], "Description": ["Parameters", "addresses_expr \u2014 A remote server address or an expression that generates multiple addresses of remote servers. Format: host or host:port.  The host can be specified as a server name, or as a IPv4 or IPv6 address. An IPv6 address must be specified in square brackets.  The port is the TCP port on the remote server. If the port is omitted, it uses tcp_port from the server config file for table function remote (by default, 9000) and tcp_port_secure for table function remoteSecure (by default, 9440).  For IPv6 addresses, a port is required.  If only parameter addresses_expr is specified, db and table will use system.one by default.  Type: String.db \u2014 Database name. Type: String.table \u2014 Table name. Type: String.user \u2014 User name. If not specified, default is used. Type: String.password \u2014 User password. If not specified, an empty password is used. Type: String.sharding_key \u2014 Sharding key to support distributing data across nodes. For example: insert into remote('127.0.0.1:9000,127.0.0.2', db, table, 'default', rand()). Type: UInt32.", "Arguments also can be passed using named collections."], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/remote"], "Title": ["Returned value"], "Feature": ["Returned value"], "Description": ["Returned value", "A table located on a remote server."], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/remote"], "Title": ["Usage"], "Feature": ["Usage"], "Description": ["Usage", "As table functions remote and remoteSecure re-establish the connection for each request, it is recommended to use a Distributed table instead. Also, if hostnames are set, the names are resolved, and errors are not counted when working with various replicas. When processing a large number of queries, always create the Distributed table ahead of time, and do not use the remote table function.", "The remote table function can be useful in the following cases:", "One-time data migration from one system to anotherAccessing a specific server for data comparison, debugging, and testing, i.e. ad-hoc connections.Queries between various ClickHouse clusters for research purposes.Infrequent distributed requests that are made manually.Distributed requests where the set of servers is re-defined each time.", "Addresses", "example01-01-1example01-01-1:9440example01-01-1:9000localhost127.0.0.1[::]:9440[::]:9000[2a02:6b8:0:1111::11]:9000", "Multiple addresses can be comma-separated. In this case, ClickHouse will use distributed processing and send the query to all specified addresses (like shards with different data). Example:", "example01-01-1,example01-02-1"], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/remote"], "Title": ["Examples"], "Feature": ["Examples"], "Description": [], "Examples": ["SELECT * FROM remote('127.0.0.1', db.remote_engine_table) LIMIT 3;", "CREATE NAMED COLLECTION creds AS        host = '127.0.0.1',        database = 'db';SELECT * FROM remote(creds, table='remote_engine_table') LIMIT 3;", "CREATE TABLE remote_table (name String, value UInt32) ENGINE=Memory;INSERT INTO FUNCTION remote('127.0.0.1', currentDatabase(), 'remote_table') VALUES ('test', 42);SELECT * FROM remote_table;", "INSERT INTO FUNCTIONremoteSecure('remote.clickhouse.cloud:9440', 'imdb.actors', 'USER', 'PASSWORD')SELECT * from imdb.actors"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/remote"], "Title": ["Globbing"], "Feature": ["Globbing"], "Description": ["Globbing", "Patterns in curly brackets { } are used to generate a set of shards and to specify replicas. If there are multiple pairs of curly brackets, then the direct product of the corresponding sets is generated.", "The following pattern types are supported.", "{a,b,c} - Represents any of alternative strings a, b or c. The pattern is replaced with a in the first shard address and replaced with b in the second shard address and so on. For instance, example0{1,2}-1 generates addresses example01-1 and example02-1.{N..M} - A range of numbers. This pattern generates shard addresses with incrementing indices from N to (and including) M. For instance, example0{1..2}-1 generates example01-1 and example02-1.{0n..0m} - A range of numbers with leading zeroes. This pattern preserves leading zeroes in indices. For instance, example{01..03}-1 generates example01-1, example02-1 and example03-1.{a|b} - Any number of variants separated by a |. The pattern specifies replicas. For instance, example01-{1|2} generates replicas example01-1 and example01-2.", "The query will be sent to the first healthy replica. However, for remote the replicas are iterated in the order currently set in the load_balancing setting.\nThe number of generated addresses is limited by table_function_remote_max_addresses setting."], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/s3"], "Title": ["Usage"], "Feature": ["Usage"], "Description": ["Usage", "Suppose that we have several files with following URIs on S3:", "'https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/some_prefix/some_file_1.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/some_prefix/some_file_2.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/some_prefix/some_file_3.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/some_prefix/some_file_4.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/another_prefix/some_file_1.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/another_prefix/some_file_2.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/another_prefix/some_file_3.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/another_prefix/some_file_4.csv'", "Count the amount of rows in files ending with numbers from 1 to 3:", "SELECT count(*)FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/{some,another}_prefix/some_file_{1..3}.csv', 'CSV', 'name String, value UInt32')", "\u250c\u2500count()\u2500\u2510\u2502      18 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "Count the total amount of rows in all files in these two directories:", "SELECT count(*)FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/{some,another}_prefix/*', 'CSV', 'name String, value UInt32')", "\u250c\u2500count()\u2500\u2510\u2502      24 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "TipIf your listing of files contains number ranges with leading zeros, use the construction with braces for each digit separately or use ?.", "Count the total amount of rows in files named file-000.csv, file-001.csv, ... , file-999.csv:", "SELECT count(*)FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/big_prefix/file-{000..999}.csv', 'CSV', 'name String, value UInt32');", "\u250c\u2500count()\u2500\u2510\u2502      12 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "Insert data into file test-data.csv.gz:", "INSERT INTO FUNCTION s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip')VALUES ('test-data', 1), ('test-data-2', 2);", "Insert data into file test-data.csv.gz from existing table:", "INSERT INTO FUNCTION s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip')SELECT name, value FROM existing_table;", "Glob ** can be used for recursive directory traversal. Consider the below example, it will fetch all files from my-test-bucket-768 directory recursively:", "SELECT * FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/**', 'CSV', 'name String, value UInt32', 'gzip');", "The below get data from all test-data.csv.gz files from any folder inside my-test-bucket directory recursively:", "SELECT * FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/**/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip');", "Note. It is possible to specify custom URL mappers in the server configuration file. Example:", "SELECT * FROM s3('s3://clickhouse-public-datasets/my-test-bucket-768/**/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip');", "The URL 's3://clickhouse-public-datasets/my-test-bucket-768/**/test-data.csv.gz' would be replaced to 'http://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/**/test-data.csv.gz'", "Custom mapper can be added into config.xml:", "<url_scheme_mappers>   <s3>      <to>https://{bucket}.s3.amazonaws.com</to>   </s3>   <gs>      <to>https://{bucket}.storage.googleapis.com</to>   </gs>   <oss>      <to>https://{bucket}.oss.aliyuncs.com</to>   </oss></url_scheme_mappers>", "For production use cases it is recommended to use named collections. Here is the example:", "CREATE NAMED COLLECTION creds AS        access_key_id = '***',        secret_access_key = '***';SELECT count(*)FROM s3(creds, url='https://s3-object-url.csv')"], "Examples": ["SELECT count(*)FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/{some,another}_prefix/some_file_{1..3}.csv', 'CSV', 'name String, value UInt32')", "SELECT count(*)FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/{some,another}_prefix/*', 'CSV', 'name String, value UInt32')", "SELECT count(*)FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/big_prefix/file-{000..999}.csv', 'CSV', 'name String, value UInt32');", "INSERT INTO FUNCTION s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip')VALUES ('test-data', 1), ('test-data-2', 2);", "INSERT INTO FUNCTION s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip')SELECT name, value FROM existing_table;", "SELECT * FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/**', 'CSV', 'name String, value UInt32', 'gzip');", "SELECT * FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/**/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip');", "SELECT * FROM s3('s3://clickhouse-public-datasets/my-test-bucket-768/**/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip');", "CREATE NAMED COLLECTION creds AS        access_key_id = '***',        secret_access_key = '***';SELECT count(*)FROM s3(creds, url='https://s3-object-url.csv')"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/s3"], "Title": ["Partitioned Write"], "Feature": ["Partitioned Write"], "Description": ["Partitioned Write", "If you specify PARTITION BY expression when inserting data into S3 table, a separate file is created for each partition value. Splitting the data into separate files helps to improve reading operations efficiency."], "Examples": ["INSERT INTO TABLE FUNCTION    s3('http://bucket.amazonaws.com/my_bucket/file_{_partition_id}.csv', 'CSV', 'a String, b UInt32, c UInt32')    PARTITION BY a VALUES ('x', 2, 3), ('x', 4, 5), ('y', 11, 12), ('y', 13, 14), ('z', 21, 22), ('z', 23, 24);", "INSERT INTO TABLE FUNCTION    s3('http://bucket.amazonaws.com/my_bucket_{_partition_id}/file.csv', 'CSV', 'a UInt32, b UInt32, c UInt32')    PARTITION BY a VALUES (1, 2, 3), (1, 4, 5), (10, 11, 12), (10, 13, 14), (20, 21, 22), (20, 23, 24);"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/s3"], "Title": ["Accessing public buckets"], "Feature": ["Accessing public buckets"], "Description": ["Accessing public buckets", "ClickHouse tries to fetch credentials from many different types of sources.\nSometimes, it can produce problems when accessing some buckets that are public causing the client to return 403 error code.\nThis issue can be avoided by using NOSIGN keyword, forcing the client to ignore all the credentials, and not sign the requests.", "SELECT *FROM s3(   'https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv',   NOSIGN,   'CSVWithNames')LIMIT 5;"], "Examples": ["SELECT *FROM s3(   'https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv',   NOSIGN,   'CSVWithNames')LIMIT 5;"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/s3"], "Title": ["Working with archives"], "Feature": ["Working with archives"], "Description": ["Working with archives", "Suppose that we have several archive files with following URIs on S3:", "'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip''https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip''https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip'", "Extracting data from these archives is possible using ::. Globs can be used both in the url part as well as in the part after :: (responsible for the name of a file inside the archive).", "SELECT *FROM s3(   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv');"], "Examples": ["SELECT *FROM s3(   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv');"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/s3"], "Title": ["Virtual Columns"], "Feature": ["Virtual Columns"], "Description": ["Virtual Columns", "_path \u2014 Path to the file. Type: LowCardinalty(String). In case of archive, shows path in a format: \"{path_to_archive}::{path_to_file_inside_archive}\"_file \u2014 Name of the file. Type: LowCardinalty(String). In case of archive shows name of the file inside the archive._size \u2014 Size of the file in bytes. Type: Nullable(UInt64). If the file size is unknown, the value is NULL. In case of archive shows uncompressed file size of the file inside the archive. _time \u2014 Last modified time of the file. Type: Nullable(DateTime). If the time is unknown, the value is NULL."], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/s3"], "Title": ["Hive-style partitioning"], "Feature": ["Hive-style partitioning"], "Description": ["Hive-style partitioning", "When setting use_hive_partitioning is set to 1, ClickHouse will detect Hive-style partitioning in the path (/name=value/) and will allow to use partition columns as virtual columns in the query. These virtual columns will have the same names as in the partitioned path, but starting with _."], "Examples": ["SET use_hive_partitioning = 1;SELECT * from s3('s3://data/path/date=*/country=*/code=*/*.parquet') where _date > '2020-01-01' and _country = 'Netherlands' and _code = 42;"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/s3"], "Title": ["Storage Settings"], "Feature": ["Storage Settings"], "Description": ["Storage Settings", "s3_truncate_on_insert - allows to truncate file before insert into it. Disabled by default.s3_create_new_file_on_insert - allows to create a new file on each insert if format has suffix. Disabled by default.s3_skip_empty_files - allows to skip empty files while reading. Disabled by default.", "See Also", "S3 engine"], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/url"], "Title": ["Globs in URL"], "Feature": ["Globs in URL"], "Description": ["Globs in URL", "Patterns in curly brackets { } are used to generate a set of shards or to specify failover addresses. Supported pattern types and examples see in the description of the remote function.\nCharacter | inside patterns is used to specify failover addresses. They are iterated in the same order as listed in the pattern. The number of generated addresses is limited by glob_expansion_max_elements setting."], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/url"], "Title": ["Virtual Columns"], "Feature": ["Virtual Columns"], "Description": ["Virtual Columns", "_path \u2014 Path to the URL. Type: LowCardinalty(String)._file \u2014 Resource name of the URL. Type: LowCardinalty(String)._size \u2014 Size of the resource in bytes. Type: Nullable(UInt64). If the size is unknown, the value is NULL._time \u2014 Last modified time of the file. Type: Nullable(DateTime). If the time is unknown, the value is NULL._headers - HTTP response headers. Type: Map(LowCardinality(String), LowCardinality(String))."], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/url"], "Title": ["Hive-style partitioning"], "Feature": ["Hive-style partitioning"], "Description": ["Hive-style partitioning", "When setting use_hive_partitioning is set to 1, ClickHouse will detect Hive-style partitioning in the path (/name=value/) and will allow to use partition columns as virtual columns in the query. These virtual columns will have the same names as in the partitioned path, but starting with _."], "Examples": ["SET use_hive_partitioning = 1;SELECT * from url('http://data/path/date=*/country=*/code=*/*.parquet') where _date > '2020-01-01' and _country = 'Netherlands' and _code = 42;"], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/url"], "Title": ["Storage Settings"], "Feature": ["Storage Settings"], "Description": ["Storage Settings", "engine_url_skip_empty_files - allows to skip empty files while reading. Disabled by default.enable_url_encoding - allows to enable/disable decoding/encoding path in uri. Enabled by default.", "See Also", "Virtual columns"], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/urlCluster"], "Title": ["Globs in URL"], "Feature": ["Globs in URL"], "Description": ["Globs in URL", "Patterns in curly brackets { } are used to generate a set of shards or to specify failover addresses. Supported pattern types and examples see in the description of the remote function.\nCharacter | inside patterns is used to specify failover addresses. They are iterated in the same order as listed in the pattern. The number of generated addresses is limited by glob_expansion_max_elements setting.", "See Also", "HDFS engineURL table function"], "Examples": [], "Category": ["Table Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/table-functions/loop"], "Title": ["loop"], "Feature": ["SELECT ... FROM loop(database, table);SELECT ... FROM loop(database.table);SELECT ... FROM loop(table);SELECT ... FROM loop(other_table_function(...));"], "Description": ["loop", "Syntax", "SELECT ... FROM loop(database, table);SELECT ... FROM loop(database.table);SELECT ... FROM loop(table);SELECT ... FROM loop(other_table_function(...));", "Parameters", "database \u2014 database name.table \u2014 table name.other_table_function(...) \u2014 other table function.\nExample: SELECT * FROM loop(numbers(10));other_table_function(...) here is numbers(10).", "Returned Value", "Infinite loop to return query results."], "Examples": ["SELECT * FROM loop(test_database, test_table);SELECT * FROM loop(test_database.test_table);SELECT * FROM loop(test_table);", "SELECT * FROM loop(numbers(3)) LIMIT 7;   \u250c\u2500number\u2500\u25101. \u2502      0 \u25022. \u2502      1 \u25023. \u2502      2 \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u250c\u2500number\u2500\u25104. \u2502      0 \u25025. \u2502      1 \u25026. \u2502      2 \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u250c\u2500number\u2500\u25107. \u2502      0 \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "SELECT * FROM loop(mysql('localhost:3306', 'test', 'test', 'user', 'password'));..."], "Category": ["Table Functions"]}
