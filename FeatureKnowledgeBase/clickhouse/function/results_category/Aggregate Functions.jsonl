{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/aggthrow"], "Title": ["aggThrow"], "Feature": ["aggThrow(throw_prob)"], "Description": ["aggThrow", "This function can be used for the purpose of testing exception safety. It will throw an exception on creation with the specified probability.", "Syntax", "aggThrow(throw_prob)", "Arguments", "throw_prob \u2014 Probability to throw on creation. Float64.", "Returned value", "An exception: Code: 503. DB::Exception: Aggregate function aggThrow has thrown exception successfully."], "Examples": ["SELECT number % 2 AS even, aggThrow(number) FROM numbers(10) GROUP BY even;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/analysis_of_variance"], "Title": ["analysisOfVariance"], "Feature": ["analysisOfVariance(val, group_no)"], "Description": ["analysisOfVariance", "Provides a statistical test for one-way analysis of variance (ANOVA test). It is a test over several groups of normally distributed observations to find out whether all groups have the same mean or not. ", "Syntax", "analysisOfVariance(val, group_no)", "Aliases: anova", "Parameters", "val: value. group_no : group number that val belongs to.", "NoteGroups are enumerated starting from 0 and there should be at least two groups to perform a test.\nThere should be at least one group with the number of observations greater than one.", "Returned value", "(f_statistic, p_value). Tuple(Float64, Float64)."], "Examples": ["SELECT analysisOfVariance(number, number % 2) FROM numbers(1048575);"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/any"], "Title": ["any"], "Feature": ["any(column) [RESPECT NULLS]"], "Description": ["any", "Selects the first encountered value of a column, ignoring any NULL values.", "Syntax", "any(column) [RESPECT NULLS]", "Aliases: any_value, first_value.", "Parameters", "column: The column name. ", "Returned value", "NoteSupports the RESPECT NULLS modifier after the function name. Using this modifier will ensure the function selects the first value passed, regardless of whether it is NULL or not.", "NoteThe return type of the function is the same as the input, except for LowCardinality which is discarded. This means that given no rows as input it will return the default value of that type (0 for integers, or Null for a Nullable() column). You might use the -OrNull combinator ) to modify this behaviour.", "DangerThe query can be executed in any order and even in a different order each time, so the result of this function is indeterminate.\nTo get a determinate result, you can use the min or max function instead of any.", "Implementation details", "In some cases, you can rely on the order of execution. This applies to cases when SELECT comes from a subquery that uses ORDER BY.", "When a SELECT query has the GROUP BY clause or at least one aggregate function, ClickHouse (in contrast to MySQL) requires that all expressions in the SELECT, HAVING, and ORDER BY clauses be calculated from keys or from aggregate functions. In other words, each column selected from the table must be used either in keys or inside aggregate functions. To get behavior like in MySQL, you can put the other columns in the any aggregate function."], "Examples": ["CREATE TABLE any_nulls (city Nullable(String)) ENGINE=Log;INSERT INTO any_nulls (city) VALUES (NULL), ('Amsterdam'), ('New York'), ('Tokyo'), ('Valencia'), (NULL);SELECT any(city) FROM any_nulls;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/anyheavy"], "Title": ["anyHeavy"], "Feature": ["anyHeavy(column)"], "Description": ["anyHeavy", "Selects a frequently occurring value using the heavy hitters algorithm. If there is a value that occurs more than in half the cases in each of the query\u2019s execution threads, this value is returned. Normally, the result is nondeterministic.", "anyHeavy(column)", "Arguments", "column \u2013 The column name."], "Examples": ["SELECT anyHeavy(AirlineID) AS resFROM ontime"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/anylast"], "Title": ["anyLast"], "Feature": ["anyLast(column) [RESPECT NULLS]"], "Description": ["anyLast", "Selects the last value encountered, ignoring any NULL values by default. The result is just as indeterminate as for the any function.", "Syntax", "anyLast(column) [RESPECT NULLS]", "Parameters", "column: The column name. ", "NoteSupports the RESPECT NULLS modifier after the function name. Using this modifier will ensure the function selects the last value passed, regardless of whether it is NULL or not.", "Returned value", "The last value encountered."], "Examples": ["CREATE TABLE any_last_nulls (city Nullable(String)) ENGINE=Log;INSERT INTO any_last_nulls (city) VALUES ('Amsterdam'),(NULL),('New York'),('Tokyo'),('Valencia'),(NULL);SELECT anyLast(city) FROM any_last_nulls;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/approxtopk"], "Title": ["approx_top_k"], "Feature": ["approx_top_k(N)(column)approx_top_k(N, reserved)(column)"], "Description": ["approx_top_k", "Returns an array of the approximately most frequent values and their counts in the specified column. The resulting array is sorted in descending order of approximate frequency of values (not by the values themselves).", "approx_top_k(N)(column)approx_top_k(N, reserved)(column)", "This function does not provide a guaranteed result. In certain situations, errors might occur and it might return frequent values that aren\u2019t the most frequent values.", "We recommend using the N < 10 value; performance is reduced with large N values. Maximum value of N = 65536.", "Parameters", "N \u2014 The number of elements to return. Optional. Default value: 10.reserved \u2014 Defines, how many cells reserved for values. If uniq(column) > reserved, result of topK function will be approximate. Optional. Default value: N * 3.", "Arguments", "column \u2014 The value to calculate frequency."], "Examples": ["SELECT approx_top_k(2)(k)FROM VALUES('k Char, w UInt64', ('y', 1), ('y', 1), ('x', 5), ('y', 1), ('z', 10));"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/approxtopk"], "Title": ["approx_top_count"], "Feature": ["approx_top_count"], "Description": ["approx_top_count", "Is an alias to approx_top_k function", "See Also", "topKtopKWeightedapprox_top_sum"], "Examples": [], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/approxtopsum"], "Title": ["approx_top_sum"], "Feature": ["approx_top_sum(N)(column, weight)approx_top_sum(N, reserved)(column, weight)"], "Description": ["approx_top_sum", "Returns an array of the approximately most frequent values and their counts in the specified column. The resulting array is sorted in descending order of approximate frequency of values (not by the values themselves). Additionally, the weight of the value is taken into account.", "approx_top_sum(N)(column, weight)approx_top_sum(N, reserved)(column, weight)", "This function does not provide a guaranteed result. In certain situations, errors might occur and it might return frequent values that aren\u2019t the most frequent values.", "We recommend using the N < 10 value; performance is reduced with large N values. Maximum value of N = 65536.", "Parameters", "N \u2014 The number of elements to return. Optional. Default value: 10.reserved \u2014 Defines, how many cells reserved for values. If uniq(column) > reserved, result of topK function will be approximate. Optional. Default value: N * 3.", "Arguments", "column \u2014 The value to calculate frequency.weight \u2014 The weight. Every value is accounted weight times for frequency calculation. UInt64."], "Examples": ["SELECT approx_top_sum(2)(k, w)FROM VALUES('k Char, w UInt64', ('y', 1), ('y', 1), ('x', 5), ('y', 1), ('z', 10))"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/argmax"], "Title": ["argMax"], "Feature": ["argMax(arg, val)"], "Description": ["argMax", "Calculates the arg value for a maximum val value. If there are multiple rows with equal val being the maximum, which of the associated arg is returned is not deterministic.\nBoth parts the arg and the max behave as aggregate functions, they both skip Null during processing and return not Null values if not Null values are available.", "Syntax", "argMax(arg, val)", "Arguments", "arg \u2014 Argument.val \u2014 Value.", "Returned value", "arg value that corresponds to maximum val value.", "Type: matches arg type."], "Examples": ["SELECT argMax(user, salary) FROM salary;", "CREATE TABLE test(    a Nullable(String),    b Nullable(Int64))ENGINE = Memory ASSELECT *FROM VALUES(('a', 1), ('b', 2), ('c', 2), (NULL, 3), (NULL, NULL), ('d', NULL));select * from test;\u250c\u2500a\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500b\u2500\u2510\u2502 a    \u2502    1 \u2502\u2502 b    \u2502    2 \u2502\u2502 c    \u2502    2 \u2502\u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502    3 \u2502\u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502\u2502 d    \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT argMax(a, b), max(b) FROM test;\u250c\u2500argMax(a, b)\u2500\u252c\u2500max(b)\u2500\u2510\u2502 b            \u2502      3 \u2502 -- argMax = 'b' because it the first not Null value, max(b) is from another row!\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT argMax(tuple(a), b) FROM test;\u250c\u2500argMax(tuple(a), b)\u2500\u2510\u2502 (NULL)              \u2502 -- The a `Tuple` that contains only a `NULL` value is not `NULL`, so the aggregate functions won't skip that row because of that `NULL` value\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT (argMax((a, b), b) as t).1 argMaxA, t.2 argMaxB FROM test;\u250c\u2500argMaxA\u2500\u252c\u2500argMaxB\u2500\u2510\u2502 \u1d3a\u1d41\u1d38\u1d38    \u2502       3 \u2502 -- you can use Tuple and get both (all - tuple(*)) columns for the according max(b)\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT argMax(a, b), max(b) FROM test WHERE a IS NULL AND b IS NULL;\u250c\u2500argMax(a, b)\u2500\u252c\u2500max(b)\u2500\u2510\u2502 \u1d3a\u1d41\u1d38\u1d38         \u2502   \u1d3a\u1d41\u1d38\u1d38 \u2502 -- All aggregated rows contains at least one `NULL` value because of the filter, so all rows are skipped, therefore the result will be `NULL`\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT argMax(a, (b,a)) FROM test;\u250c\u2500argMax(a, tuple(b, a))\u2500\u2510\u2502 c                      \u2502 -- There are two rows with b=2, `Tuple` in the `Max` allows to get not the first `arg`\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT argMax(a, tuple(b)) FROM test;\u250c\u2500argMax(a, tuple(b))\u2500\u2510\u2502 b                   \u2502 -- `Tuple` can be used in `Max` to not skip Nulls in `Max`\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/argmin"], "Title": ["argMin"], "Feature": ["argMin(arg, val)"], "Description": ["argMin", "Calculates the arg value for a minimum val value. If there are multiple rows with equal val being the maximum, which of the associated arg is returned is not deterministic.\nBoth parts the arg and the min behave as aggregate functions, they both skip Null during processing and return not Null values if not Null values are available.", "Syntax", "argMin(arg, val)", "Arguments", "arg \u2014 Argument.val \u2014 Value.", "Returned value", "arg value that corresponds to minimum val value.", "Type: matches arg type."], "Examples": ["SELECT argMin(user, salary) FROM salary", "CREATE TABLE test(    a Nullable(String),    b Nullable(Int64))ENGINE = Memory ASSELECT *FROM VALUES((NULL, 0), ('a', 1), ('b', 2), ('c', 2), (NULL, NULL), ('d', NULL));select * from test;\u250c\u2500a\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500b\u2500\u2510\u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502    0 \u2502\u2502 a    \u2502    1 \u2502\u2502 b    \u2502    2 \u2502\u2502 c    \u2502    2 \u2502\u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502\u2502 d    \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT argMin(a, b), min(b) FROM test;\u250c\u2500argMin(a, b)\u2500\u252c\u2500min(b)\u2500\u2510\u2502 a            \u2502      0 \u2502 -- argMin = a because it the first not `NULL` value, min(b) is from another row!\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT argMin(tuple(a), b) FROM test;\u250c\u2500argMin(tuple(a), b)\u2500\u2510\u2502 (NULL)              \u2502 -- The a `Tuple` that contains only a `NULL` value is not `NULL`, so the aggregate functions won't skip that row because of that `NULL` value\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT (argMin((a, b), b) as t).1 argMinA, t.2 argMinB from test;\u250c\u2500argMinA\u2500\u252c\u2500argMinB\u2500\u2510\u2502 \u1d3a\u1d41\u1d38\u1d38    \u2502       0 \u2502 -- you can use `Tuple` and get both (all - tuple(*)) columns for the according max(b)\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT argMin(a, b), min(b) FROM test WHERE a IS NULL and b IS NULL;\u250c\u2500argMin(a, b)\u2500\u252c\u2500min(b)\u2500\u2510\u2502 \u1d3a\u1d41\u1d38\u1d38         \u2502   \u1d3a\u1d41\u1d38\u1d38 \u2502 -- All aggregated rows contains at least one `NULL` value because of the filter, so all rows are skipped, therefore the result will be `NULL`\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT argMin(a, (b, a)), min(tuple(b, a)) FROM test;\u250c\u2500argMin(a, tuple(b, a))\u2500\u252c\u2500min(tuple(b, a))\u2500\u2510\u2502 d                      \u2502 (NULL,NULL)      \u2502 -- 'd' is the first not `NULL` value for the min\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT argMin((a, b), (b, a)), min(tuple(b, a)) FROM test;\u250c\u2500argMin(tuple(a, b), tuple(b, a))\u2500\u252c\u2500min(tuple(b, a))\u2500\u2510\u2502 (NULL,NULL)                      \u2502 (NULL,NULL)      \u2502 -- argMin returns (NULL,NULL) here because `Tuple` allows to don't skip `NULL` and min(tuple(b, a)) in this case is minimal value for this dataset\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT argMin(a, tuple(b)) FROM test;\u250c\u2500argMin(a, tuple(b))\u2500\u2510\u2502 d                   \u2502 -- `Tuple` can be used in `min` to not skip rows with `NULL` values as b.\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/array_concat_agg"], "Title": ["array_concat_agg"], "Feature": ["array_concat_agg"], "Description": ["array_concat_agg", "Alias of groupArrayArray. The function is case insensitive."], "Examples": ["SELECT array_concat_agg(a) AS aFROM t\u250c\u2500a\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 [1,2,3,4,5,6] \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/avg"], "Title": ["avg"], "Feature": ["avg(x)"], "Description": ["avg", "Calculates the arithmetic mean.", "Syntax", "avg(x)", "Arguments", "x \u2014 input values, must be Integer, Float, or Decimal.", "Returned value", "The arithmetic mean, always as Float64.NaN if the input parameter x is empty."], "Examples": ["SELECT avg(x) FROM values('x Int8', 0, 1, 2, 3, 4, 5);", "CREATE table test (t UInt8) ENGINE = Memory;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/avgweighted"], "Title": ["avgWeighted"], "Feature": ["avgWeighted(x, weight)"], "Description": ["avgWeighted", "Calculates the weighted arithmetic mean.", "Syntax", "avgWeighted(x, weight)", "Arguments", "x \u2014 Values.weight \u2014 Weights of the values.", "x and weight must both be\nInteger or floating-point,\nbut may have different types.", "Returned value", "NaN if all the weights are equal to 0 or the supplied weights parameter is empty.Weighted mean otherwise.", "Return type is always Float64."], "Examples": ["SELECT avgWeighted(x, w)FROM values('x Int8, w Int8', (4, 1), (1, 0), (10, 2))", "SELECT avgWeighted(x, w)FROM values('x Int8, w Float64', (4, 1), (1, 0), (10, 2))", "SELECT avgWeighted(x, w)FROM values('x Int8, w Int8', (0, 0), (1, 0), (10, 0))", "CREATE table test (t UInt8) ENGINE = Memory;SELECT avgWeighted(t) FROM test"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/contingency"], "Title": ["contingency"], "Feature": ["contingency(column1, column2)"], "Description": ["contingency", "The contingency function calculates the contingency coefficient, a value that measures the association between two columns in a table. The computation is similar to the cramersV function but with a different denominator in the square root.", "Syntax", "contingency(column1, column2)", "Arguments", "column1 and column2 are the columns to be compared", "Returned value", "a value between 0 and 1. The larger the result, the closer the association of the two columns.", "Return type is always Float64."], "Examples": ["SELECT    cramersV(a, b),    contingency(a ,b)FROM    (        SELECT            number % 10 AS a,            number % 4 AS b        FROM            numbers(150)    );"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/corr"], "Title": ["corr"], "Feature": ["corr(x, y)"], "Description": ["corr", "Calculates the Pearson correlation coefficient:", "\u03a3(x\u2212x\u02c9)(y\u2212y\u02c9)\u03a3(x\u2212x\u02c9)2\u2217\u03a3(y\u2212y\u02c9)2\\frac{\\Sigma{(x - \\bar{x})(y - \\bar{y})}}{\\sqrt{\\Sigma{(x - \\bar{x})^2} * \\Sigma{(y - \\bar{y})^2}}}\u03a3(x\u2212x\u02c9)2\u2217\u03a3(y\u2212y\u02c9)2\u03a3(x\u2212x\u02c9)(y\u2212y\u02c9)", "NoteThis function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the corrStable function. It is slower but provides a more accurate result.", "Syntax", "corr(x, y)", "Arguments", "x \u2014 first variable. (U)Int*, Float*, Decimal.y \u2014 second variable. (U)Int*, Float*, Decimal.", "Returned Value", "The Pearson correlation coefficient. Float64."], "Examples": ["DROP TABLE IF EXISTS series;CREATE TABLE series(    i UInt32,    x_value Float64,    y_value Float64)ENGINE = Memory;INSERT INTO series(i, x_value, y_value) VALUES (1, 5.6, -4.4),(2, -9.6, 3),(3, -1.3, -4),(4, 5.3, 9.7),(5, 4.4, 0.037),(6, -8.6, -7.8),(7, 5.1, 9.3),(8, 7.9, -3.6),(9, -8.2, 0.62),(10, -3, 7.3);", "SELECT corr(x_value, y_value)FROM series;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/corrmatrix"], "Title": ["corrMatrix"], "Feature": ["corrMatrix(x[, ...])"], "Description": ["corrMatrix", "Computes the correlation matrix over N variables.", "Syntax", "corrMatrix(x[, ...])", "Arguments", "x \u2014 a variable number of parameters. (U)Int*, Float*, Decimal.", "Returned value", "Correlation matrix. Array(Array(Float64))."], "Examples": ["DROP TABLE IF EXISTS test;CREATE TABLE test(    a UInt32,    b Float64,    c Float64,    d Float64)ENGINE = Memory;INSERT INTO test(a, b, c, d) VALUES (1, 5.6, -4.4, 2.6), (2, -9.6, 3, 3.3), (3, -1.3, -4, 1.2), (4, 5.3, 9.7, 2.3), (5, 4.4, 0.037, 1.222), (6, -8.6, -7.8, 2.1233), (7, 5.1, 9.3, 8.1222), (8, 7.9, -3.6, 9.837), (9, -8.2, 0.62, 8.43555), (10, -3, 7.3, 6.762);", "SELECT arrayMap(x -> round(x, 3), arrayJoin(corrMatrix(a, b, c, d))) AS corrMatrixFROM test;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/corrstable"], "Title": ["corrStable"], "Feature": ["corrStable(x, y)"], "Description": ["corrStable", "Calculates the Pearson correlation coefficient: ", "\u03a3(x\u2212x\u02c9)(y\u2212y\u02c9)\u03a3(x\u2212x\u02c9)2\u2217\u03a3(y\u2212y\u02c9)2\\frac{\\Sigma{(x - \\bar{x})(y - \\bar{y})}}{\\sqrt{\\Sigma{(x - \\bar{x})^2} * \\Sigma{(y - \\bar{y})^2}}}\u03a3(x\u2212x\u02c9)2\u2217\u03a3(y\u2212y\u02c9)2\u03a3(x\u2212x\u02c9)(y\u2212y\u02c9)", "Similar to the corr function, but uses a numerically stable algorithm. As a result, corrStable is slower than corr but produces a more accurate result.", "Syntax", "corrStable(x, y)", "Arguments", "x \u2014 first variable. (U)Int*, Float*, Decimal.y \u2014 second variable. (U)Int*, Float*, Decimal.", "Returned Value", "The Pearson correlation coefficient. Float64."], "Examples": ["DROP TABLE IF EXISTS series;CREATE TABLE series(    i UInt32,    x_value Float64,    y_value Float64)ENGINE = Memory;INSERT INTO series(i, x_value, y_value) VALUES (1, 5.6, -4.4),(2, -9.6, 3),(3, -1.3, -4),(4, 5.3, 9.7),(5, 4.4, 0.037),(6, -8.6, -7.8),(7, 5.1, 9.3),(8, 7.9, -3.6),(9, -8.2, 0.62),(10, -3, 7.3);", "SELECT corrStable(x_value, y_value)FROM series;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/count"], "Title": ["count"], "Feature": ["count"], "Description": ["count", "Counts the number of rows or not-NULL values.", "ClickHouse supports the following syntaxes for count:", "count(expr) or COUNT(DISTINCT expr).count() or COUNT(*). The count() syntax is ClickHouse-specific.", "Arguments", "The function can take:", "Zero parameters.One expression.", "Returned value", "If the function is called without parameters it counts the number of rows.If the expression is passed, then the function counts how many times this expression returned not null. If the expression returns a Nullable-type value, then the result of count stays not Nullable. The function returns 0 if the expression returned NULL for all the rows.", "In both cases the type of the returned value is UInt64.", "Details", "ClickHouse supports the COUNT(DISTINCT ...) syntax. The behavior of this construction depends on the count_distinct_implementation setting. It defines which of the uniq* functions is used to perform the operation. The default is the uniqExact function.", "The SELECT count() FROM table query is optimized by default using metadata from MergeTree. If you need to use row-level security, disable optimization using the optimize_trivial_count_query setting.", "However SELECT count(nullable_column) FROM table query can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only null subcolumn instead of reading and processing the whole column data. The query SELECT count(n) FROM table transforms to SELECT sum(NOT n.null) FROM table.", "Improving COUNT(DISTINCT expr) performance", "If your COUNT(DISTINCT expr) query is slow, consider adding a GROUP BY clause as this improves parallelization. You can also use a projection to create an index on the target column used with COUNT(DISTINCT target_col)."], "Examples": ["SELECT count() FROM t", "SELECT name, value FROM system.settings WHERE name = 'count_distinct_implementation'", "SELECT count(DISTINCT num) FROM t"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/covarpop"], "Title": ["covarPop"], "Feature": ["covarPop(x, y)"], "Description": ["covarPop", "Calculates the population covariance:", "\u03a3(x\u2212x\u02c9)(y\u2212y\u02c9)n\\frac{\\Sigma{(x - \\bar{x})(y - \\bar{y})}}{n}n\u03a3(x\u2212x\u02c9)(y\u2212y\u02c9)", "NoteThis function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the covarPopStable function. It works slower but provides a lower computational error.", "Syntax", "covarPop(x, y)", "Arguments", "x \u2014 first variable. (U)Int*, Float*, Decimal.y \u2014 second variable. (U)Int*, Float*, Decimal.", "Returned Value", "The population covariance between x and y. Float64."], "Examples": ["DROP TABLE IF EXISTS series;CREATE TABLE series(i UInt32, x_value Float64, y_value Float64) ENGINE = Memory;INSERT INTO series(i, x_value, y_value) VALUES (1, 5.6, -4.4),(2, -9.6, 3),(3, -1.3, -4),(4, 5.3, 9.7),(5, 4.4, 0.037),(6, -8.6, -7.8),(7, 5.1, 9.3),(8, 7.9, -3.6),(9, -8.2, 0.62),(10, -3, 7.3);", "SELECT covarPop(x_value, y_value)FROM series;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/covarpopmatrix"], "Title": ["covarPopMatrix"], "Feature": ["covarPopMatrix(x[, ...])"], "Description": ["covarPopMatrix", "Returns the population covariance matrix over N variables.", "Syntax", "covarPopMatrix(x[, ...])", "Arguments", "x \u2014 a variable number of parameters. (U)Int*, Float*, Decimal.", "Returned Value", "Population covariance matrix. Array(Array(Float64))."], "Examples": ["DROP TABLE IF EXISTS test;CREATE TABLE test(    a UInt32,    b Float64,    c Float64,    d Float64)ENGINE = Memory;INSERT INTO test(a, b, c, d) VALUES (1, 5.6, -4.4, 2.6), (2, -9.6, 3, 3.3), (3, -1.3, -4, 1.2), (4, 5.3, 9.7, 2.3), (5, 4.4, 0.037, 1.222), (6, -8.6, -7.8, 2.1233), (7, 5.1, 9.3, 8.1222), (8, 7.9, -3.6, 9.837), (9, -8.2, 0.62, 8.43555), (10, -3, 7.3, 6.762);", "SELECT arrayMap(x -> round(x, 3), arrayJoin(covarPopMatrix(a, b, c, d))) AS covarPopMatrixFROM test;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/covarpopstable"], "Title": ["covarPopStable"], "Feature": ["covarPop(x, y)"], "Description": ["covarPopStable", "Calculates the value of the population covariance:", "\u03a3(x\u2212x\u02c9)(y\u2212y\u02c9)n\\frac{\\Sigma{(x - \\bar{x})(y - \\bar{y})}}{n}n\u03a3(x\u2212x\u02c9)(y\u2212y\u02c9)", "It is similar to the covarPop function, but uses a numerically stable algorithm. As a result, covarPopStable is slower than covarPop but produces a more accurate result.", "Syntax", "covarPop(x, y)", "Arguments", "x \u2014 first variable. (U)Int*, Float*, Decimal.y \u2014 second variable. (U)Int*, Float*, Decimal.", "Returned Value", "The population covariance between x and y. Float64."], "Examples": ["DROP TABLE IF EXISTS series;CREATE TABLE series(i UInt32, x_value Float64, y_value Float64) ENGINE = Memory;INSERT INTO series(i, x_value, y_value) VALUES (1, 5.6,-4.4),(2, -9.6,3),(3, -1.3,-4),(4, 5.3,9.7),(5, 4.4,0.037),(6, -8.6,-7.8),(7, 5.1,9.3),(8, 7.9,-3.6),(9, -8.2,0.62),(10, -3,7.3);", "SELECT covarPopStable(x_value, y_value)FROM(    SELECT        x_value,        y_value    FROM series);"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/covarsamp"], "Title": ["covarSamp"], "Feature": ["covarSamp(x, y)"], "Description": ["covarSamp", "Calculates the value of \u03a3((x - x\u0305)(y - y\u0305)) / (n - 1).", "NoteThis function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the covarSampStable function. It works slower but provides a lower computational error.", "Syntax", "covarSamp(x, y)", "Arguments", "x \u2014 first variable. (U)Int*, Float*, Decimal.y \u2014 second variable. (U)Int*, Float*, Decimal.", "Returned Value", "The sample covariance between x and y. For n <= 1, nan is returned. Float64."], "Examples": ["DROP TABLE IF EXISTS series;CREATE TABLE series(i UInt32, x_value Float64, y_value Float64) ENGINE = Memory;INSERT INTO series(i, x_value, y_value) VALUES (1, 5.6,-4.4),(2, -9.6,3),(3, -1.3,-4),(4, 5.3,9.7),(5, 4.4,0.037),(6, -8.6,-7.8),(7, 5.1,9.3),(8, 7.9,-3.6),(9, -8.2,0.62),(10, -3,7.3);", "SELECT covarSamp(x_value, y_value)FROM(    SELECT        x_value,        y_value    FROM series);", "SELECT covarSamp(x_value, y_value)FROM(    SELECT        x_value,        y_value    FROM series LIMIT 1);"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/covarsampmatrix"], "Title": ["covarSampMatrix"], "Feature": ["covarSampMatrix(x[, ...])"], "Description": ["covarSampMatrix", "Returns the sample covariance matrix over N variables.", "Syntax", "covarSampMatrix(x[, ...])", "Arguments", "x \u2014 a variable number of parameters. (U)Int*, Float*, Decimal.", "Returned Value", "Sample covariance matrix. Array(Array(Float64))."], "Examples": ["DROP TABLE IF EXISTS test;CREATE TABLE test(    a UInt32,    b Float64,    c Float64,    d Float64)ENGINE = Memory;INSERT INTO test(a, b, c, d) VALUES (1, 5.6, -4.4, 2.6), (2, -9.6, 3, 3.3), (3, -1.3, -4, 1.2), (4, 5.3, 9.7, 2.3), (5, 4.4, 0.037, 1.222), (6, -8.6, -7.8, 2.1233), (7, 5.1, 9.3, 8.1222), (8, 7.9, -3.6, 9.837), (9, -8.2, 0.62, 8.43555), (10, -3, 7.3, 6.762);", "SELECT arrayMap(x -> round(x, 3), arrayJoin(covarSampMatrix(a, b, c, d))) AS covarSampMatrixFROM test;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/covarsampstable"], "Title": ["covarSampStable"], "Feature": ["covarSampStable(x, y)"], "Description": ["covarSampStable", "Calculates the value of \u03a3((x - x\u0305)(y - y\u0305)) / (n - 1). Similar to covarSamp but works slower while providing a lower computational error.", "Syntax", "covarSampStable(x, y)", "Arguments", "x \u2014 first variable. (U)Int*, Float*, Decimal.y \u2014 second variable. (U)Int*, Float*, Decimal.", "Returned Value", "The sample covariance between x and y. For n <= 1, inf is returned. Float64."], "Examples": ["DROP TABLE IF EXISTS series;CREATE TABLE series(i UInt32, x_value Float64, y_value Float64) ENGINE = Memory;INSERT INTO series(i, x_value, y_value) VALUES (1, 5.6,-4.4),(2, -9.6,3),(3, -1.3,-4),(4, 5.3,9.7),(5, 4.4,0.037),(6, -8.6,-7.8),(7, 5.1,9.3),(8, 7.9,-3.6),(9, -8.2,0.62),(10, -3,7.3);", "SELECT covarSampStable(x_value, y_value)FROM(    SELECT        x_value,        y_value    FROM series);", "SELECT covarSampStable(x_value, y_value)FROM(    SELECT        x_value,        y_value    FROM series LIMIT 1);"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/cramersv"], "Title": ["cramersV"], "Feature": ["cramersV(column1, column2)"], "Description": ["cramersV", "Cramer's V (sometimes referred to as Cramer's phi) is a measure of association between two columns in a table. The result of the cramersV function ranges from 0 (corresponding to no association between the variables) to 1 and can reach 1 only when each value is completely determined by the other. It may be viewed as the association between two variables as a percentage of their maximum possible variation.", "NoteFor a bias corrected version of Cramer's V see: cramersVBiasCorrected", "Syntax", "cramersV(column1, column2)", "Parameters", "column1: first column to be compared.column2: second column to be compared.", "Returned value", "a value between 0 (corresponding to no association between the columns' values) to 1 (complete association).", "Type: always Float64."], "Examples": ["SELECT    cramersV(a, b)FROM    (        SELECT            number % 3 AS a,            number % 5 AS b        FROM            numbers(150)    );", "SELECT    cramersV(a, b)FROM    (        SELECT            number % 10 AS a,            number % 5 AS b        FROM            numbers(150)    );"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/cramersvbiascorrected"], "Title": ["cramersVBiasCorrected"], "Feature": ["cramersVBiasCorrected(column1, column2)"], "Description": ["cramersVBiasCorrected", "Cramer's V is a measure of association between two columns in a table. The result of the cramersV function ranges from 0 (corresponding to no association between the variables) to 1 and can reach 1 only when each value is completely determined by the other. The function can be heavily biased, so this version of Cramer's V uses the bias correction.", "Syntax", "cramersVBiasCorrected(column1, column2)", "Parameters", "column1: first column to be compared.column2: second column to be compared.", "Returned value", "a value between 0 (corresponding to no association between the columns' values) to 1 (complete association).", "Type: always Float64."], "Examples": ["SELECT    cramersV(a, b),    cramersVBiasCorrected(a ,b)FROM    (        SELECT            number % 10 AS a,            number % 4 AS b        FROM            numbers(150)    );"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/deltasum"], "Title": ["See Also"], "Feature": ["See Also"], "Description": ["See Also", "runningDifference"], "Examples": [], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/entropy"], "Title": ["entropy"], "Feature": ["entropy(val)"], "Description": ["entropy", "Calculates Shannon entropy of a column of values.", "Syntax", "entropy(val)", "Arguments", "val \u2014 Column of values of any type.", "Returned value", "Shannon entropy.", "Type: Float64."], "Examples": ["CREATE TABLE entropy (`vals` UInt32,`strings` String) ENGINE = Memory;INSERT INTO entropy VALUES (1, 'A'), (1, 'A'), (1,'A'), (1,'A'), (2,'B'), (2,'B'), (2,'C'), (2,'D');SELECT entropy(vals), entropy(strings) FROM entropy;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/exponentialMovingAverage"], "Title": ["exponentialMovingAverage"], "Feature": ["exponentialMovingAverage(x)(value, timeunit)"], "Description": ["exponentialMovingAverage", "Calculates the exponential moving average of values for the determined time.", "Syntax", "exponentialMovingAverage(x)(value, timeunit)", "Each value corresponds to the determinate timeunit. The half-life x is the time lag at which the exponential weights decay by one-half. The function returns a weighted average: the older the time point, the less weight the corresponding value is considered to be.", "Arguments", "value \u2014 Value. Integer, Float or Decimal.timeunit \u2014 Timeunit. Integer, Float or Decimal. Timeunit is not timestamp (seconds), it's -- an index of the time interval. Can be calculated using intDiv.", "Parameters", "x \u2014 Half-life period. Integer, Float or Decimal.", "Returned values", "Returns an exponentially smoothed moving average of the values for the past x time at the latest point of time.", "Type: Float64."], "Examples": ["SELECT exponentialMovingAverage(5)(temperature, timestamp);", "SELECT    value,    time,    round(exp_smooth, 3),    bar(exp_smooth, 0, 1, 50) AS barFROM(    SELECT        (number = 0) OR (number >= 25) AS value,        number AS time,        exponentialMovingAverage(10)(value, time) OVER (Rows BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS exp_smooth    FROM numbers(50))", "CREATE TABLE dataENGINE = Memory ASSELECT    10 AS value,    toDateTime('2020-01-01') + (3600 * number) AS timeFROM numbers_mt(10);-- Calculate timeunit using intDivSELECT    value,    time,    exponentialMovingAverage(1)(value, intDiv(toUInt32(time), 3600)) OVER (ORDER BY time ASC) AS res,    intDiv(toUInt32(time), 3600) AS timeunitFROM dataORDER BY time ASC;\u250c\u2500value\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500time\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500res\u2500\u252c\u2500timeunit\u2500\u2510\u2502    10 \u2502 2020-01-01 00:00:00 \u2502           5 \u2502   438288 \u2502\u2502    10 \u2502 2020-01-01 01:00:00 \u2502         7.5 \u2502   438289 \u2502\u2502    10 \u2502 2020-01-01 02:00:00 \u2502        8.75 \u2502   438290 \u2502\u2502    10 \u2502 2020-01-01 03:00:00 \u2502       9.375 \u2502   438291 \u2502\u2502    10 \u2502 2020-01-01 04:00:00 \u2502      9.6875 \u2502   438292 \u2502\u2502    10 \u2502 2020-01-01 05:00:00 \u2502     9.84375 \u2502   438293 \u2502\u2502    10 \u2502 2020-01-01 06:00:00 \u2502    9.921875 \u2502   438294 \u2502\u2502    10 \u2502 2020-01-01 07:00:00 \u2502   9.9609375 \u2502   438295 \u2502\u2502    10 \u2502 2020-01-01 08:00:00 \u2502  9.98046875 \u2502   438296 \u2502\u2502    10 \u2502 2020-01-01 09:00:00 \u2502 9.990234375 \u2502   438297 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518-- Calculate timeunit using toRelativeHourNumSELECT    value,    time,    exponentialMovingAverage(1)(value, toRelativeHourNum(time)) OVER (ORDER BY time ASC) AS res,    toRelativeHourNum(time) AS timeunitFROM dataORDER BY time ASC;\u250c\u2500value\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500time\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500res\u2500\u252c\u2500timeunit\u2500\u2510\u2502    10 \u2502 2020-01-01 00:00:00 \u2502           5 \u2502   438288 \u2502\u2502    10 \u2502 2020-01-01 01:00:00 \u2502         7.5 \u2502   438289 \u2502\u2502    10 \u2502 2020-01-01 02:00:00 \u2502        8.75 \u2502   438290 \u2502\u2502    10 \u2502 2020-01-01 03:00:00 \u2502       9.375 \u2502   438291 \u2502\u2502    10 \u2502 2020-01-01 04:00:00 \u2502      9.6875 \u2502   438292 \u2502\u2502    10 \u2502 2020-01-01 05:00:00 \u2502     9.84375 \u2502   438293 \u2502\u2502    10 \u2502 2020-01-01 06:00:00 \u2502    9.921875 \u2502   438294 \u2502\u2502    10 \u2502 2020-01-01 07:00:00 \u2502   9.9609375 \u2502   438295 \u2502\u2502    10 \u2502 2020-01-01 08:00:00 \u2502  9.98046875 \u2502   438296 \u2502\u2502    10 \u2502 2020-01-01 09:00:00 \u2502 9.990234375 \u2502   438297 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/exponentialTimeDecayedAvg"], "Title": ["exponentialTimeDecayedAvg"], "Feature": ["exponentialTimeDecayedAvg(x)(v, t)"], "Description": ["exponentialTimeDecayedAvg", "Returns the exponentially smoothed weighted moving average of values of a time series at point t in time.", "Syntax", "exponentialTimeDecayedAvg(x)(v, t)", "Arguments", "v \u2014 Value. Integer, Float or Decimal.t \u2014 Time. Integer, Float or Decimal, DateTime, DateTime64.", "Parameters", "x \u2014 Half-life period. Integer, Float or Decimal.", "Returned values", "Returns an exponentially smoothed weighted moving average at index t in time. Float64."], "Examples": ["SELECT    value,    time,    round(exp_smooth, 3),    bar(exp_smooth, 0, 5, 50) AS barFROM    (    SELECT    (number = 0) OR (number >= 25) AS value,    number AS time,    exponentialTimeDecayedAvg(10)(value, time) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS exp_smooth    FROM numbers(50)    );", "   \u250c\u2500value\u2500\u252c\u2500time\u2500\u252c\u2500round(exp_smooth, 3)\u2500\u252c\u2500bar\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25101. \u2502     1 \u2502    0 \u2502                    1 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u25022. \u2502     0 \u2502    1 \u2502                0.475 \u2502 \u2588\u2588\u2588\u2588\u258a      \u25023. \u2502     0 \u2502    2 \u2502                0.301 \u2502 \u2588\u2588\u2588        \u25024. \u2502     0 \u2502    3 \u2502                0.214 \u2502 \u2588\u2588\u258f        \u25025. \u2502     0 \u2502    4 \u2502                0.162 \u2502 \u2588\u258c         \u25026. \u2502     0 \u2502    5 \u2502                0.128 \u2502 \u2588\u258e         \u25027. \u2502     0 \u2502    6 \u2502                0.104 \u2502 \u2588          \u25028. \u2502     0 \u2502    7 \u2502                0.086 \u2502 \u258a          \u25029. \u2502     0 \u2502    8 \u2502                0.072 \u2502 \u258b          \u25020. \u2502     0 \u2502    9 \u2502                0.061 \u2502 \u258c          \u25021. \u2502     0 \u2502   10 \u2502                0.052 \u2502 \u258c          \u25022. \u2502     0 \u2502   11 \u2502                0.045 \u2502 \u258d          \u25023. \u2502     0 \u2502   12 \u2502                0.039 \u2502 \u258d          \u25024. \u2502     0 \u2502   13 \u2502                0.034 \u2502 \u258e          \u25025. \u2502     0 \u2502   14 \u2502                 0.03 \u2502 \u258e          \u25026. \u2502     0 \u2502   15 \u2502                0.027 \u2502 \u258e          \u25027. \u2502     0 \u2502   16 \u2502                0.024 \u2502 \u258f          \u25028. \u2502     0 \u2502   17 \u2502                0.021 \u2502 \u258f          \u25029. \u2502     0 \u2502   18 \u2502                0.018 \u2502 \u258f          \u25020. \u2502     0 \u2502   19 \u2502                0.016 \u2502 \u258f          \u25021. \u2502     0 \u2502   20 \u2502                0.015 \u2502 \u258f          \u25022. \u2502     0 \u2502   21 \u2502                0.013 \u2502 \u258f          \u25023. \u2502     0 \u2502   22 \u2502                0.012 \u2502            \u25024. \u2502     0 \u2502   23 \u2502                 0.01 \u2502            \u25025. \u2502     0 \u2502   24 \u2502                0.009 \u2502            \u25026. \u2502     1 \u2502   25 \u2502                0.111 \u2502 \u2588          \u25027. \u2502     1 \u2502   26 \u2502                0.202 \u2502 \u2588\u2588         \u25028. \u2502     1 \u2502   27 \u2502                0.283 \u2502 \u2588\u2588\u258a        \u25029. \u2502     1 \u2502   28 \u2502                0.355 \u2502 \u2588\u2588\u2588\u258c       \u25020. \u2502     1 \u2502   29 \u2502                 0.42 \u2502 \u2588\u2588\u2588\u2588\u258f      \u25021. \u2502     1 \u2502   30 \u2502                0.477 \u2502 \u2588\u2588\u2588\u2588\u258a      \u25022. \u2502     1 \u2502   31 \u2502                0.529 \u2502 \u2588\u2588\u2588\u2588\u2588\u258e     \u25023. \u2502     1 \u2502   32 \u2502                0.576 \u2502 \u2588\u2588\u2588\u2588\u2588\u258a     \u25024. \u2502     1 \u2502   33 \u2502                0.618 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u258f    \u25025. \u2502     1 \u2502   34 \u2502                0.655 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u258c    \u25026. \u2502     1 \u2502   35 \u2502                0.689 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2589    \u25027. \u2502     1 \u2502   36 \u2502                0.719 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   \u25028. \u2502     1 \u2502   37 \u2502                0.747 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d   \u25029. \u2502     1 \u2502   38 \u2502                0.771 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b   \u25020. \u2502     1 \u2502   39 \u2502                0.793 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589   \u25021. \u2502     1 \u2502   40 \u2502                0.813 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  \u25022. \u2502     1 \u2502   41 \u2502                0.831 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  \u25023. \u2502     1 \u2502   42 \u2502                0.848 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  \u25024. \u2502     1 \u2502   43 \u2502                0.862 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  \u25025. \u2502     1 \u2502   44 \u2502                0.876 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  \u25026. \u2502     1 \u2502   45 \u2502                0.888 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  \u25027. \u2502     1 \u2502   46 \u2502                0.898 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  \u25028. \u2502     1 \u2502   47 \u2502                0.908 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u25029. \u2502     1 \u2502   48 \u2502                0.917 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f \u25020. \u2502     1 \u2502   49 \u2502                0.925 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/exponentialTimeDecayedCount"], "Title": ["exponentialTimeDecayedCount"], "Feature": ["exponentialTimeDecayedCount(x)(t)"], "Description": ["exponentialTimeDecayedCount", "Returns the cumulative exponential decay over a time series at the index t in time.", "Syntax", "exponentialTimeDecayedCount(x)(t)", "Arguments", "t \u2014 Time. Integer, Float or Decimal, DateTime, DateTime64.", "Parameters", "x \u2014 Half-life period. Integer, Float or Decimal.", "Returned values", "Returns the cumulative exponential decay at the given point in time. Float64."], "Examples": ["SELECT    value,    time,    round(exp_smooth, 3),    bar(exp_smooth, 0, 20, 50) AS barFROM(    SELECT        (number % 5) = 0 AS value,        number AS time,        exponentialTimeDecayedCount(10)(time) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS exp_smooth    FROM numbers(50));"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/exponentialTimeDecayedMax"], "Title": ["exponentialTimeDecayedMax"], "Feature": ["exponentialTimeDecayedMax(x)(value, timeunit)"], "Description": ["exponentialTimeDecayedMax", "Returns the maximum of the computed exponentially smoothed moving average at index t in time with that at t-1. ", "Syntax", "exponentialTimeDecayedMax(x)(value, timeunit)", "Arguments", "value \u2014 Value. Integer, Float or Decimal.timeunit \u2014 Timeunit. Integer, Float or Decimal, DateTime, DateTime64.", "Parameters", "x \u2014 Half-life period. Integer, Float or Decimal.", "Returned values", "Returns the maximum of the exponentially smoothed weighted moving average at t and t-1. Float64."], "Examples": ["SELECT    value,    time,    round(exp_smooth, 3),    bar(exp_smooth, 0, 5, 50) AS barFROM    (    SELECT    (number = 0) OR (number >= 25) AS value,    number AS time,    exponentialTimeDecayedMax(10)(value, time) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS exp_smooth    FROM numbers(50)    );"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/exponentialTimeDecayedSum"], "Title": ["exponentialTimeDecayedSum"], "Feature": ["exponentialTimeDecayedSum(x)(v, t)"], "Description": ["exponentialTimeDecayedSum", "Returns the sum of exponentially smoothed moving average values of a time series at the index t in time.", "Syntax", "exponentialTimeDecayedSum(x)(v, t)", "Arguments", "v \u2014 Value. Integer, Float or Decimal.t \u2014 Time. Integer, Float or Decimal, DateTime, DateTime64.", "Parameters", "x \u2014 Half-life period. Integer, Float or Decimal.", "Returned values", "Returns the sum of exponentially smoothed moving average values at the given point in time. Float64."], "Examples": ["SELECT    value,    time,    round(exp_smooth, 3),    bar(exp_smooth, 0, 10, 50) AS barFROM    (    SELECT    (number = 0) OR (number >= 25) AS value,    number AS time,    exponentialTimeDecayedSum(10)(value, time) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS exp_smooth    FROM numbers(50)    );"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/flame_graph"], "Title": ["Syntax"], "Feature": ["flameGraph(traces, [size], [ptr])"], "Description": ["Syntax", "flameGraph(traces, [size], [ptr])"], "Examples": ["flameGraph(traces, [size], [ptr])"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/flame_graph"], "Title": ["Parameters"], "Feature": ["Parameters"], "Description": ["Parameters", "traces \u2014 a stacktrace. Array(UInt64).size \u2014 an allocation size for memory profiling. (optional - default 1). UInt64.ptr \u2014 an allocation address. (optional - default 0). UInt64.", "NoteIn the case where ptr != 0, a flameGraph will map allocations (size > 0) and deallocations (size < 0) with the same size and ptr.\nOnly allocations which were not freed are shown. Non mapped deallocations are ignored."], "Examples": [], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/flame_graph"], "Title": ["Returned value"], "Feature": ["Returned value"], "Description": ["Returned value", "An array of strings for use with flamegraph.pl utility. Array(String)."], "Examples": [], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/flame_graph"], "Title": ["Examples"], "Feature": ["Examples"], "Description": [], "Examples": ["SET query_profiler_cpu_time_period_ns=10000000;SELECT SearchPhrase, COUNT(DISTINCT UserID) AS u FROM hits WHERE SearchPhrase <> '' GROUP BY SearchPhrase ORDER BY u DESC LIMIT 10;", "SET memory_profiler_sample_probability=1, max_untracked_memory=1;SELECT SearchPhrase, COUNT(DISTINCT UserID) AS u FROM hits WHERE SearchPhrase <> '' GROUP BY SearchPhrase ORDER BY u DESC LIMIT 10;", "SET memory_profiler_sample_probability=1, max_untracked_memory=1, use_uncompressed_cache=1, merge_tree_max_rows_to_use_cache=100000000000, merge_tree_max_bytes_to_use_cache=1000000000000;SELECT SearchPhrase, COUNT(DISTINCT UserID) AS u FROM hits WHERE SearchPhrase <> '' GROUP BY SearchPhrase ORDER BY u DESC LIMIT 10;", "SET memory_profiler_sample_probability=1, max_untracked_memory=1;SELECT SearchPhrase, COUNT(DISTINCT UserID) AS u FROM hits WHERE SearchPhrase <> '' GROUP BY SearchPhrase ORDER BY u DESC LIMIT 10;", "SELECT event_time, m, formatReadableSize(max(s) as m) FROM (SELECT event_time, sum(size) OVER (ORDER BY event_time) AS s FROM system.trace_log WHERE query_id = 'xxx' AND trace_type = 'MemorySample') GROUP BY event_time ORDER BY event_time;", "SELECT argMax(event_time, s), max(s) FROM (SELECT event_time, sum(size) OVER (ORDER BY event_time) AS s FROM system.trace_log WHERE query_id = 'xxx' AND trace_type = 'MemorySample');"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/grouparray"], "Title": ["groupArray"], "Feature": ["groupArray"], "Description": ["groupArray", "Syntax: groupArray(x) or groupArray(max_size)(x)", "Creates an array of argument values.\nValues can be added to the array in any (indeterminate) order.", "The second version (with the max_size parameter) limits the size of the resulting array to max_size elements. For example, groupArray(1)(x) is equivalent to [any (x)].", "In some cases, you can still rely on the order of execution. This applies to cases when SELECT comes from a subquery that uses ORDER BY if the subquery result is small enough."], "Examples": ["select id, groupArray(10)(name) from default.ck group by id;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/grouparrayinsertat"], "Title": ["groupArrayInsertAt"], "Feature": ["groupArrayInsertAt(default_x, size)(x, pos)"], "Description": ["groupArrayInsertAt", "Inserts a value into the array at the specified position.", "Syntax", "groupArrayInsertAt(default_x, size)(x, pos)", "If in one query several values are inserted into the same position, the function behaves in the following ways:", "If a query is executed in a single thread, the first one of the inserted values is used.If a query is executed in multiple threads, the resulting value is an undetermined one of the inserted values.", "Arguments", "x \u2014 Value to be inserted. Expression resulting in one of the supported data types.pos \u2014 Position at which the specified element x is to be inserted. Index numbering in the array starts from zero. UInt32.default_x \u2014 Default value for substituting in empty positions. Optional parameter. Expression resulting in the data type configured for the x parameter. If default_x is not defined, the default values are used.size \u2014 Length of the resulting array. Optional parameter. When using this parameter, the default value default_x must be specified. UInt32.", "Returned value", "Array with inserted values.", "Type: Array."], "Examples": ["SELECT groupArrayInsertAt(toString(number), number * 2) FROM numbers(5);", "SELECT groupArrayInsertAt('-')(toString(number), number * 2) FROM numbers(5);", "SELECT groupArrayInsertAt('-', 5)(toString(number), number * 2) FROM numbers(5);", "SELECT groupArrayInsertAt(number, 0) FROM numbers_mt(10) SETTINGS max_block_size = 1;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/grouparrayintersect"], "Title": ["groupArrayIntersect"], "Feature": ["groupArrayIntersect(x)"], "Description": ["groupArrayIntersect", "Return an intersection of given arrays (Return all items of arrays, that are in all given arrays).", "Syntax", "groupArrayIntersect(x)", "Arguments", "x \u2014 Argument (column name or expression).", "Returned values", "Array that contains elements that are in all arrays.", "Type: Array."], "Examples": ["SELECT groupArrayIntersect(a) as intersection FROM numbers;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/grouparraylast"], "Title": ["groupArrayLast"], "Feature": ["groupArrayLast"], "Description": ["groupArrayLast", "Syntax: groupArrayLast(max_size)(x)", "Creates an array of last argument values.\nFor example, groupArrayLast(1)(x) is equivalent to [anyLast (x)].", "In some cases, you can still rely on the order of execution. This applies to cases when SELECT comes from a subquery that uses ORDER BY if the subquery result is small enough."], "Examples": ["select groupArrayLast(2)(number+1) numbers from numbers(10)", "select groupArray(2)(number+1) numbers from numbers(10)"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/grouparraymovingavg"], "Title": ["groupArrayMovingAvg"], "Feature": ["groupArrayMovingAvg(numbers_for_summing)groupArrayMovingAvg(window_size)(numbers_for_summing)"], "Description": ["groupArrayMovingAvg", "Calculates the moving average of input values.", "groupArrayMovingAvg(numbers_for_summing)groupArrayMovingAvg(window_size)(numbers_for_summing)", "The function can take the window size as a parameter. If left unspecified, the function takes the window size equal to the number of rows in the column.", "Arguments", "numbers_for_summing \u2014 Expression resulting in a numeric data type value.window_size \u2014 Size of the calculation window.", "Returned values", "Array of the same size and type as the input data.", "The function uses rounding towards zero. It truncates the decimal places insignificant for the resulting data type."], "Examples": ["CREATE TABLE t(    `int` UInt8,    `float` Float32,    `dec` Decimal32(2))ENGINE = TinyLog", "SELECT    groupArrayMovingAvg(int) AS I,    groupArrayMovingAvg(float) AS F,    groupArrayMovingAvg(dec) AS DFROM t", "SELECT    groupArrayMovingAvg(2)(int) AS I,    groupArrayMovingAvg(2)(float) AS F,    groupArrayMovingAvg(2)(dec) AS DFROM t"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/grouparraymovingsum"], "Title": ["groupArrayMovingSum"], "Feature": ["groupArrayMovingSum(numbers_for_summing)groupArrayMovingSum(window_size)(numbers_for_summing)"], "Description": ["groupArrayMovingSum", "Calculates the moving sum of input values.", "groupArrayMovingSum(numbers_for_summing)groupArrayMovingSum(window_size)(numbers_for_summing)", "The function can take the window size as a parameter. If left unspecified, the function takes the window size equal to the number of rows in the column.", "Arguments", "numbers_for_summing \u2014 Expression resulting in a numeric data type value.window_size \u2014 Size of the calculation window.", "Returned values", "Array of the same size and type as the input data."], "Examples": ["CREATE TABLE t(    `int` UInt8,    `float` Float32,    `dec` Decimal32(2))ENGINE = TinyLog", "SELECT    groupArrayMovingSum(int) AS I,    groupArrayMovingSum(float) AS F,    groupArrayMovingSum(dec) AS DFROM t", "SELECT    groupArrayMovingSum(2)(int) AS I,    groupArrayMovingSum(2)(float) AS F,    groupArrayMovingSum(2)(dec) AS DFROM t"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/grouparraysample"], "Title": ["groupArraySample"], "Feature": ["groupArraySample(max_size[, seed])(x)"], "Description": ["groupArraySample", "Creates an array of sample argument values. The size of the resulting array is limited to max_size elements. Argument values are selected and added to the array randomly.", "Syntax", "groupArraySample(max_size[, seed])(x)", "Arguments", "max_size \u2014 Maximum size of the resulting array. UInt64.seed \u2014 Seed for the random number generator. Optional. UInt64. Default value: 123456.x \u2014 Argument (column name or expression).", "Returned values", "Array of randomly selected x arguments.", "Type: Array."], "Examples": ["SELECT groupArraySample(3)(color) as newcolors FROM colors;", "SELECT groupArraySample(3, 987654321)(color) as newcolors FROM colors;", "SELECT groupArraySample(3)(concat('light-', color)) as newcolors FROM colors;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/grouparraysorted"], "Title": ["groupArraySorted"], "Feature": ["groupArraySorted"], "Description": ["groupArraySorted", " Returns an array with the first N items in ascending order.", "groupArraySorted(N)(column)", " Arguments", "N \u2013 The number of elements to return.column \u2013 The value (Integer, String, Float and other Generic types).ExampleGets the first 10 numbers:SELECT groupArraySorted(10)(number) FROM numbers(100)\u250c\u2500groupArraySorted(10)(number)\u2500\u2510\u2502 [0,1,2,3,4,5,6,7,8,9]        \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", " Gets all the String implementations of all numbers in column:", "SELECT groupArraySorted(5)(str) FROM (SELECT toString(number) as str FROM numbers(5));", "\u250c\u2500groupArraySorted(5)(str)\u2500\u2510\u2502 ['0','1','2','3','4']    \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518"], "Examples": ["groupArraySorted(N)(column)", "SELECT groupArraySorted(5)(str) FROM (SELECT toString(number) as str FROM numbers(5));"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/groupbitand"], "Title": ["groupBitAnd"], "Feature": ["groupBitAnd(expr)"], "Description": ["groupBitAnd", "Applies bit-wise AND for series of numbers.", "groupBitAnd(expr)", "Arguments", "expr \u2013 An expression that results in UInt* or Int* type.", "Return value", "Value of the UInt* or Int* type."], "Examples": ["SELECT groupBitAnd(num) FROM t"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/groupbitmap"], "Title": ["groupBitmap"], "Feature": ["groupBitmap(expr)"], "Description": ["groupBitmap", "Bitmap or Aggregate calculations from a unsigned integer column, return cardinality of type UInt64, if add suffix -State, then return bitmap object.", "groupBitmap(expr)", "Arguments", "expr \u2013 An expression that results in UInt* type.", "Return value", "Value of the UInt64 type."], "Examples": ["SELECT groupBitmap(UserID) as num FROM t"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/groupbitor"], "Title": ["groupBitOr"], "Feature": ["groupBitOr(expr)"], "Description": ["groupBitOr", "Applies bit-wise OR for series of numbers.", "groupBitOr(expr)", "Arguments", "expr \u2013 An expression that results in UInt* or Int* type.", "Returned value", "Value of the UInt* or Int* type."], "Examples": ["SELECT groupBitOr(num) FROM t"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/groupbitxor"], "Title": ["groupBitXor"], "Feature": ["groupBitXor(expr)"], "Description": ["groupBitXor", "Applies bit-wise XOR for series of numbers.", "groupBitXor(expr)", "Arguments", "expr \u2013 An expression that results in UInt* or Int* type.", "Return value", "Value of the UInt* or Int* type."], "Examples": ["SELECT groupBitXor(num) FROM t"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/groupuniqarray"], "Title": ["groupUniqArray"], "Feature": ["groupUniqArray"], "Description": ["groupUniqArray", "Syntax: groupUniqArray(x) or groupUniqArray(max_size)(x)", "Creates an array from different argument values. Memory consumption is the same as for the uniqExact function.", "The second version (with the max_size parameter) limits the size of the resulting array to max_size elements.\nFor example, groupUniqArray(1)(x) is equivalent to [any(x)]."], "Examples": [], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/kolmogorovsmirnovtest"], "Title": ["kolmogorovSmirnovTest"], "Feature": ["kolmogorovSmirnovTest([alternative, computation_method])(sample_data, sample_index)"], "Description": ["kolmogorovSmirnovTest", "Applies Kolmogorov-Smirnov's test to samples from two populations.", "Syntax", "kolmogorovSmirnovTest([alternative, computation_method])(sample_data, sample_index)", "Values of both samples are in the sample_data column. If sample_index equals to 0 then the value in that row belongs to the sample from the first population. Otherwise it belongs to the sample from the second population.\nSamples must belong to continuous, one-dimensional probability distributions.", "Arguments", "sample_data \u2014 Sample data. Integer, Float or Decimal.sample_index \u2014 Sample index. Integer.", "Parameters", "alternative \u2014 alternative hypothesis. (Optional, default: 'two-sided'.) String.\nLet F(x) and G(x) be the CDFs of the first and second distributions respectively.'two-sided'\nThe null hypothesis is that samples come from the same distribution, e.g. F(x) = G(x) for all x.\nAnd the alternative is that the distributions are not identical.'greater'\nThe null hypothesis is that values in the first sample are stochastically smaller than those in the second one,\ne.g. the CDF of first distribution lies above and hence to the left of that for the second one.\nWhich in fact means that F(x) >= G(x) for all x. And the alternative in this case is that F(x) < G(x) for at least one x.'less'.\nThe null hypothesis is that values in the first sample are stochastically greater than those in the second one,\ne.g. the CDF of first distribution lies below and hence to the right of that for the second one.\nWhich in fact means that F(x) <= G(x) for all x. And the alternative in this case is that F(x) > G(x) for at least one x.computation_method \u2014 the method used to compute p-value. (Optional, default: 'auto'.) String.'exact' - calculation is performed using precise probability distribution of the test statistics. Compute intensive and wasteful except for small samples.'asymp' ('asymptotic') - calculation is performed using an approximation. For large sample sizes, the exact and asymptotic p-values are very similar.'auto'  - the 'exact' method is used when a maximum number of samples is less than 10'000.", "Returned values", "Tuple with two elements:", "calculated statistic. Float64.calculated p-value. Float64."], "Examples": ["SELECT kolmogorovSmirnovTest('less', 'exact')(value, num)FROM(    SELECT        randNormal(0, 10) AS value,        0 AS num    FROM numbers(10000)    UNION ALL    SELECT        randNormal(0, 10) AS value,        1 AS num    FROM numbers(10000))", "SELECT kolmogorovSmirnovTest('two-sided', 'exact')(value, num)FROM(    SELECT        randStudentT(10) AS value,        0 AS num    FROM numbers(100)    UNION ALL    SELECT        randNormal(0, 10) AS value,        1 AS num    FROM numbers(100))"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/kurtpop"], "Title": ["kurtPop"], "Feature": ["kurtPop(expr)"], "Description": ["kurtPop", "Computes the kurtosis of a sequence.", "kurtPop(expr)", "Arguments", "expr \u2014 Expression returning a number.", "Returned value", "The kurtosis of the given distribution. Type \u2014 Float64"], "Examples": ["SELECT kurtPop(value) FROM series_with_value_column;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/kurtsamp"], "Title": ["kurtSamp"], "Feature": ["kurtSamp(expr)"], "Description": ["kurtSamp", "Computes the sample kurtosis of a sequence.", "It represents an unbiased estimate of the kurtosis of a random variable if passed values form its sample.", "kurtSamp(expr)", "Arguments", "expr \u2014 Expression returning a number.", "Returned value", "The kurtosis of the given distribution. Type \u2014 Float64. If n <= 1 (n is a size of the sample), then the function returns nan."], "Examples": ["SELECT kurtSamp(value) FROM series_with_value_column;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/largestTriangleThreeBuckets"], "Title": ["largestTriangleThreeBuckets"], "Feature": ["largestTriangleThreeBuckets(n)(x, y)"], "Description": ["largestTriangleThreeBuckets", "Applies the Largest-Triangle-Three-Buckets algorithm to the input data.\nThe algorithm is used for downsampling time series data for visualization. It is designed to operate on series sorted by x coordinate.\nIt works by dividing the sorted series into buckets and then finding the largest triangle in each bucket. The number of buckets is equal to the number of points in the resulting series.\nthe function will sort data by x and then apply the downsampling algorithm to the sorted data.", "Syntax", "largestTriangleThreeBuckets(n)(x, y)", "Alias: lttb.", "Arguments", "x \u2014 x coordinate. Integer , Float , Decimal  , Date, Date32, DateTime, DateTime64.y \u2014 y coordinate. Integer , Float , Decimal  , Date, Date32, DateTime, DateTime64.", "NaNs are ignored in the provided series, meaning that any NaN values will be excluded from the analysis. This ensures that the function operates only on valid numerical data.", "Parameters", "n \u2014 number of points in the resulting series. UInt64.", "Returned values", "Array of Tuple with two elements:"], "Examples": ["SELECT largestTriangleThreeBuckets(4)(x, y) FROM largestTriangleThreeBuckets_test;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/mannwhitneyutest"], "Title": ["mannWhitneyUTest"], "Feature": ["mannWhitneyUTest[(alternative[, continuity_correction])](sample_data, sample_index)"], "Description": ["mannWhitneyUTest", "Applies the Mann-Whitney rank test to samples from two populations.", "Syntax", "mannWhitneyUTest[(alternative[, continuity_correction])](sample_data, sample_index)", "Values of both samples are in the sample_data column. If sample_index equals to 0 then the value in that row belongs to the sample from the first population. Otherwise it belongs to the sample from the second population.\nThe null hypothesis is that two populations are stochastically equal. Also one-sided hypothesises can be tested. This test does not assume that data have normal distribution.", "Arguments", "sample_data \u2014 sample data. Integer, Float or Decimal.sample_index \u2014 sample index. Integer.", "Parameters", "alternative \u2014 alternative hypothesis. (Optional, default: 'two-sided'.) String.'two-sided';'greater';'less'.continuity_correction \u2014 if not 0 then continuity correction in the normal approximation for the p-value is applied. (Optional, default: 1.) UInt64.", "Returned values", "Tuple with two elements:", "calculated U-statistic. Float64.calculated p-value. Float64."], "Examples": ["SELECT mannWhitneyUTest('greater')(sample_data, sample_index) FROM mww_ttest;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/maxintersections"], "Title": ["maxIntersections"], "Feature": ["maxIntersections(start_column, end_column)"], "Description": ["maxIntersections", "Aggregate function that calculates the maximum number of times that a group of intervals intersects each other (if all the intervals intersect at least once).", "The syntax is:", "maxIntersections(start_column, end_column)", "Arguments", "start_column \u2013 the numeric column that represents the start of each interval. If start_column is NULL or 0 then the interval will be skipped.end_column - the numeric column that represents the end of each interval. If end_column is NULL or 0 then the interval will be skipped.", "Returned value", "Returns the maximum number of intersected intervals."], "Examples": ["CREATE TABLE my_events (    start UInt32,    end UInt32)Engine = MergeTreeORDER BY tuple();INSERT INTO my_events VALUES   (1, 3),   (1, 6),   (2, 5),   (3, 7);", "SELECT maxIntersections(start, end) FROM my_events;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/maxintersectionsposition"], "Title": ["maxIntersectionsPosition"], "Feature": ["maxIntersectionsPosition(start_column, end_column)"], "Description": ["maxIntersectionsPosition", "Aggregate function that calculates the positions of the occurrences of the maxIntersections function.", "The syntax is:", "maxIntersectionsPosition(start_column, end_column)", "Arguments", "start_column \u2013 the numeric column that represents the start of each interval. If start_column is NULL or 0 then the interval will be skipped.end_column - the numeric column that represents the end of each interval. If end_column is NULL or 0 then the interval will be skipped.", "Returned value", "Returns the start positions of the maximum number of intersected intervals."], "Examples": ["CREATE TABLE my_events (    start UInt32,    end UInt32)Engine = MergeTreeORDER BY tuple();INSERT INTO my_events VALUES   (1, 3),   (1, 6),   (2, 5),   (3, 7);", "SELECT maxIntersectionsPosition(start, end) FROM my_events;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/maxmap"], "Title": ["maxMap"], "Feature": ["maxMap(key, value)"], "Description": ["maxMap", "Calculates the maximum from value array according to the keys specified in the key array.", "Syntax", "maxMap(key, value)", "or", "maxMap(Tuple(key, value))", "Alias: maxMappedArrays", "NotePassing a tuple of keys and value arrays is identical to passing two arrays of keys and values.The number of elements in key and value must be the same for each row that is totaled.", "Parameters", "key \u2014 Array of keys. Array.value \u2014 Array of values. Array.", "Returned value", "Returns a tuple of two arrays: keys in sorted order, and values calculated for the corresponding keys. Tuple(Array, Array)."], "Examples": ["SELECT maxMap(a, b)FROM values('a Array(Char), b Array(Int64)', (['x', 'y'], [2, 2]), (['y', 'z'], [3, 1]))"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/meanztest"], "Title": ["meanZTest"], "Feature": ["meanZTest(population_variance_x, population_variance_y, confidence_level)(sample_data, sample_index)"], "Description": ["meanZTest", "Applies mean z-test to samples from two populations.", "Syntax", "meanZTest(population_variance_x, population_variance_y, confidence_level)(sample_data, sample_index)", "Values of both samples are in the sample_data column. If sample_index equals to 0 then the value in that row belongs to the sample from the first population. Otherwise it belongs to the sample from the second population.\nThe null hypothesis is that means of populations are equal. Normal distribution is assumed. Populations may have unequal variance and the variances are known.", "Arguments", "sample_data \u2014 Sample data. Integer, Float or Decimal.sample_index \u2014 Sample index. Integer.", "Parameters", "population_variance_x \u2014 Variance for population x. Float.population_variance_y \u2014 Variance for population y. Float.confidence_level \u2014 Confidence level in order to calculate confidence intervals. Float.", "Returned values", "Tuple with four elements:", "calculated t-statistic. Float64.calculated p-value. Float64.calculated confidence-interval-low. Float64.calculated confidence-interval-high. Float64."], "Examples": ["SELECT meanZTest(0.7, 0.45, 0.95)(sample_data, sample_index) FROM mean_ztest"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/median"], "Title": ["median"], "Feature": ["median"], "Description": ["median", "The median* functions are the aliases for the corresponding quantile* functions. They calculate median of a numeric data sample.", "Functions:", "median \u2014 Alias for quantile.medianDeterministic \u2014 Alias for quantileDeterministic.medianExact \u2014 Alias for quantileExact.medianExactWeighted \u2014 Alias for quantileExactWeighted.medianTiming \u2014 Alias for quantileTiming.medianTimingWeighted \u2014 Alias for quantileTimingWeighted.medianTDigest \u2014 Alias for quantileTDigest.medianTDigestWeighted \u2014 Alias for quantileTDigestWeighted.medianBFloat16 \u2014 Alias for quantileBFloat16.medianDD \u2014 Alias for quantileDD."], "Examples": ["SELECT medianDeterministic(val, 1) FROM t;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/minmap"], "Title": ["minMap"], "Feature": ["`minMap(key, value)`"], "Description": ["minMap", "Calculates the minimum from value array according to the keys specified in the key array.", "Syntax", "`minMap(key, value)`", "or", "minMap(Tuple(key, value))", "Alias: minMappedArrays", "NotePassing a tuple of keys and value arrays is identical to passing an array of keys and an array of values.The number of elements in key and value must be the same for each row that is totaled.", "Parameters", "key \u2014 Array of keys. Array.value \u2014 Array of values. Array.", "Returned value", "Returns a tuple of two arrays: keys in sorted order, and values calculated for the corresponding keys. Tuple(Array, Array)."], "Examples": ["SELECT minMap(a, b)FROM values('a Array(Int32), b Array(Int64)', ([1, 2], [2, 2]), ([2, 3], [1, 1]))"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantile"], "Title": ["quantile"], "Feature": ["quantile(level)(expr)"], "Description": ["quantile", "Computes an approximate quantile of a numeric data sequence.", "This function applies reservoir sampling with a reservoir size up to 8192 and a random number generator for sampling. The result is non-deterministic. To get an exact quantile, use the quantileExact function.", "When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function.", "Note that for an empty numeric sequence, quantile will return NaN, but its quantile* variants will return either NaN or a default value for the sequence type, depending on the variant.", "Syntax", "quantile(level)(expr)", "Alias: median.", "Arguments", "level \u2014 Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr \u2014 Expression over the column values resulting in numeric data types, Date or DateTime.", "Returned value", "Approximate quantile of the specified level.", "Type:", "Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type."], "Examples": ["SELECT quantile(val) FROM t"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantilebfloat16"], "Title": ["quantileBFloat16Weighted"], "Feature": ["quantileBFloat16Weighted"], "Description": ["quantileBFloat16Weighted", "Like quantileBFloat16 but takes into account the weight of each sequence member.", "See Also", "medianquantiles"], "Examples": [], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantiledeterministic"], "Title": ["quantileDeterministic"], "Feature": ["quantileDeterministic(level)(expr, determinator)"], "Description": ["quantileDeterministic", "Computes an approximate quantile of a numeric data sequence.", "This function applies reservoir sampling with a reservoir size up to 8192 and deterministic algorithm of sampling. The result is deterministic. To get an exact quantile, use the quantileExact function.", "When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function.", "Syntax", "quantileDeterministic(level)(expr, determinator)", "Alias: medianDeterministic.", "Arguments", "level \u2014 Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr \u2014 Expression over the column values resulting in numeric data types, Date or DateTime.determinator \u2014 Number whose hash is used instead of a random number generator in the reservoir sampling algorithm to make the result of sampling deterministic. As a determinator you can use any deterministic positive number, for example, a user id or an event id. If the same determinator value occurs too often, the function works incorrectly.", "Returned value", "Approximate quantile of the specified level.", "Type:", "Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type."], "Examples": ["SELECT quantileDeterministic(val, 1) FROM t"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantileexact"], "Title": ["quantileExact"], "Feature": ["quantileExact(level)(expr)"], "Description": ["quantileExact", "Exactly computes the quantile of a numeric data sequence.", "To get exact value, all the passed values are combined into an array, which is then partially sorted. Therefore, the function consumes O(n) memory, where n is a number of values that were passed. However, for a small number of values, the function is very effective.", "When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function.", "Syntax", "quantileExact(level)(expr)", "Alias: medianExact.", "Arguments", "level \u2014 Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr \u2014 Expression over the column values resulting in numeric data types, Date or DateTime.", "Returned value", "Quantile of the specified level.", "Type:", "Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type."], "Examples": ["SELECT quantileExact(number) FROM numbers(10)"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantileexact"], "Title": ["quantileExactLow"], "Feature": ["quantileExactLow(level)(expr)"], "Description": ["quantileExactLow", "Similar to quantileExact, this computes the exact quantile of a numeric data sequence.", "To get the exact value, all the passed values are combined into an array, which is then fully sorted.  The sorting algorithm's complexity is O(N\u00b7log(N)), where N = std::distance(first, last) comparisons.", "The return value depends on the quantile level and the number of elements in the selection, i.e. if the level is 0.5, then the function returns the lower median value for an even number of elements and the middle median value for an odd number of elements. Median is calculated similarly to the median_low implementation which is used in python.", "For all other levels, the element at the index corresponding to the value of level * size_of_array is returned. For example:", "SELECT quantileExactLow(0.1)(number) FROM numbers(10)\u250c\u2500quantileExactLow(0.1)(number)\u2500\u2510\u2502                             1 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function.", "Syntax", "quantileExactLow(level)(expr)", "Alias: medianExactLow.", "Arguments", "level \u2014 Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr \u2014 Expression over the column values resulting in numeric data types, Date or DateTime.", "Returned value", "Quantile of the specified level.", "Type:", "Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type."], "Examples": ["SELECT quantileExactLow(number) FROM numbers(10)"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantileexact"], "Title": ["quantileExactHigh"], "Feature": ["quantileExactHigh(level)(expr)"], "Description": ["quantileExactHigh", "Similar to quantileExact, this computes the exact quantile of a numeric data sequence.", "All the passed values are combined into an array, which is then fully sorted, to get the exact value.  The sorting algorithm's complexity is O(N\u00b7log(N)), where N = std::distance(first, last) comparisons.", "The return value depends on the quantile level and the number of elements in the selection, i.e. if the level is 0.5, then the function returns the higher median value for an even number of elements and the middle median value for an odd number of elements. Median is calculated similarly to the median_high implementation which is used in python. For all other levels, the element at the index corresponding to the value of level * size_of_array is returned.", "This implementation behaves exactly similar to the current quantileExact implementation.", "When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function.", "Syntax", "quantileExactHigh(level)(expr)", "Alias: medianExactHigh.", "Arguments", "level \u2014 Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr \u2014 Expression over the column values resulting in numeric data types, Date or DateTime.", "Returned value", "Quantile of the specified level.", "Type:", "Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type."], "Examples": ["SELECT quantileExactHigh(number) FROM numbers(10)"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantileexact"], "Title": ["quantileExactExclusive"], "Feature": ["quantileExactExclusive(level)(expr)"], "Description": ["quantileExactExclusive", "Exactly computes the quantile of a numeric data sequence.", "To get exact value, all the passed values are combined into an array, which is then partially sorted. Therefore, the function consumes O(n) memory, where n is a number of values that were passed. However, for a small number of values, the function is very effective.", "This function is equivalent to PERCENTILE.EXC Excel function, (type R6).", "When using multiple quantileExactExclusive functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantilesExactExclusive function.", "Syntax", "quantileExactExclusive(level)(expr)", "Arguments", "expr \u2014 Expression over the column values resulting in numeric data types, Date or DateTime.", "Parameters", "level \u2014 Level of quantile. Optional. Possible values: (0, 1) \u2014 bounds not included. Default value: 0.5. At level=0.5 the function calculates median. Float.", "Returned value", "Quantile of the specified level.", "Type:", "Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type."], "Examples": ["CREATE TABLE num AS numbers(1000);SELECT quantileExactExclusive(0.6)(x) FROM (SELECT number AS x FROM num);"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantileexact"], "Title": ["quantileExactInclusive"], "Feature": ["quantileExactInclusive(level)(expr)"], "Description": ["quantileExactInclusive", "Exactly computes the quantile of a numeric data sequence.", "To get exact value, all the passed values are combined into an array, which is then partially sorted. Therefore, the function consumes O(n) memory, where n is a number of values that were passed. However, for a small number of values, the function is very effective.", "This function is equivalent to PERCENTILE.INC Excel function, (type R7).", "When using multiple quantileExactInclusive functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantilesExactInclusive function.", "Syntax", "quantileExactInclusive(level)(expr)", "Arguments", "expr \u2014 Expression over the column values resulting in numeric data types, Date or DateTime.", "Parameters", "level \u2014 Level of quantile. Optional. Possible values: [0, 1] \u2014 bounds included. Default value: 0.5. At level=0.5 the function calculates median. Float.", "Returned value", "Quantile of the specified level.", "Type:", "Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type."], "Examples": ["CREATE TABLE num AS numbers(1000);SELECT quantileExactInclusive(0.6)(x) FROM (SELECT number AS x FROM num);"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantileexactweighted"], "Title": ["quantileExactWeighted"], "Feature": ["quantileExactWeighted(level)(expr, weight)"], "Description": ["quantileExactWeighted", "Exactly computes the quantile of a numeric data sequence, taking into account the weight of each element.", "To get exact value, all the passed values are combined into an array, which is then partially sorted. Each value is counted with its weight, as if it is present weight times. A hash table is used in the algorithm. Because of this, if the passed values are frequently repeated, the function consumes less RAM than quantileExact. You can use this function instead of quantileExact and specify the weight 1.", "When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function.", "Syntax", "quantileExactWeighted(level)(expr, weight)", "Alias: medianExactWeighted.", "Arguments", "level \u2014 Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr \u2014 Expression over the column values resulting in numeric data types, Date or DateTime.weight \u2014 Column with weights of sequence members. Weight is a number of value occurrences with Unsigned integer types.", "Returned value", "Quantile of the specified level.", "Type:", "Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type."], "Examples": ["SELECT quantileExactWeighted(n, val) FROM t"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantileGK"], "Title": ["quantileGK"], "Feature": ["quantileGK(accuracy, level)(expr)"], "Description": ["quantileGK", "Computes the quantile of a numeric data sequence using the Greenwald-Khanna algorithm. The Greenwald-Khanna algorithm is an algorithm used to compute quantiles on a stream of data in a highly efficient manner. It was introduced by Michael Greenwald and Sanjeev Khanna in 2001. It is widely used in databases and big data systems where computing accurate quantiles on a large stream of data in real-time is necessary. The algorithm is highly efficient, taking only O(log n) space and O(log log n) time per item (where n is the size of the input). It is also highly accurate, providing an approximate quantile value with high probability.", "quantileGK is different from other quantile functions in ClickHouse, because it enables user to control the accuracy of the approximate quantile result.", "Syntax", "quantileGK(accuracy, level)(expr)", "Alias: medianGK.", "Arguments", "accuracy \u2014 Accuracy of quantile. Constant positive integer. Larger accuracy value means less error. For example, if the accuracy argument is set to 100, the computed quantile will have an error no greater than 1% with high probability. There is a trade-off between the accuracy of the computed quantiles and the computational complexity of the algorithm. A larger accuracy requires more memory and computational resources to compute the quantile accurately, while a smaller accuracy argument allows for a faster and more memory-efficient computation but with a slightly lower accuracy.level \u2014 Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. Default value: 0.5. At level=0.5 the function calculates median.expr \u2014 Expression over the column values resulting in numeric data types, Date or DateTime.", "Returned value", "Quantile of the specified level and accuracy.", "Type:", "Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type."], "Examples": ["SELECT quantileGK(1, 0.25)(number + 1)FROM numbers(1000)\u250c\u2500quantileGK(1, 0.25)(plus(number, 1))\u2500\u2510\u2502                                    1 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT quantileGK(10, 0.25)(number + 1)FROM numbers(1000)\u250c\u2500quantileGK(10, 0.25)(plus(number, 1))\u2500\u2510\u2502                                   156 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT quantileGK(100, 0.25)(number + 1)FROM numbers(1000)\u250c\u2500quantileGK(100, 0.25)(plus(number, 1))\u2500\u2510\u2502                                    251 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT quantileGK(1000, 0.25)(number + 1)FROM numbers(1000)\u250c\u2500quantileGK(1000, 0.25)(plus(number, 1))\u2500\u2510\u2502                                     249 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantileExactWeightedInterpolated"], "Title": ["quantileExactWeightedInterpolated"], "Feature": ["quantileExactWeightedInterpolated(level)(expr, weight)"], "Description": ["quantileExactWeightedInterpolated", "Computes quantile of a numeric data sequence using linear interpolation, taking into account the weight of each element.", "To get the interpolated value, all the passed values are combined into an array, which are then sorted by their corresponding weights. Quantile interpolation is then performed using the weighted percentile method by building a cumulative distribution based on weights and then a linear interpolation is performed using the weights and the values to compute the quantiles.", "When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function.", "We strongly recommend using quantileExactWeightedInterpolated instead of quantileInterpolatedWeighted because quantileExactWeightedInterpolated is more accurate than quantileInterpolatedWeighted. Here is an example:", "SELECT    quantileExactWeightedInterpolated(0.99)(number, 1),    quantile(0.99)(number),    quantileInterpolatedWeighted(0.99)(number, 1)FROM numbers(9)\u250c\u2500quantileExactWeightedInterpolated(0.99)(number, 1)\u2500\u252c\u2500quantile(0.99)(number)\u2500\u252c\u2500quantileInterpolatedWeighted(0.99)(number, 1)\u2500\u2510\u2502                                               7.92 \u2502                   7.92 \u2502                                             8 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518", "Syntax", "quantileExactWeightedInterpolated(level)(expr, weight)", "Alias: medianExactWeightedInterpolated.", "Arguments", "level \u2014 Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr \u2014 Expression over the column values resulting in numeric data types, Date or DateTime.weight \u2014 Column with weights of sequence members. Weight is a number of value occurrences with Unsigned integer types.", "Returned value", "Quantile of the specified level.", "Type:", "Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type."], "Examples": [], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantileInterpolatedWeighted"], "Title": ["quantileInterpolatedWeighted"], "Feature": ["quantileInterpolatedWeighted(level)(expr, weight)"], "Description": ["quantileInterpolatedWeighted", "Computes quantile of a numeric data sequence using linear interpolation, taking into account the weight of each element.", "To get the interpolated value, all the passed values are combined into an array, which are then sorted by their corresponding weights. Quantile interpolation is then performed using the weighted percentile method by building a cumulative distribution based on weights and then a linear interpolation is performed using the weights and the values to compute the quantiles.", "When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function.", "Syntax", "quantileInterpolatedWeighted(level)(expr, weight)", "Alias: medianInterpolatedWeighted.", "Arguments", "level \u2014 Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr \u2014 Expression over the column values resulting in numeric data types, Date or DateTime.weight \u2014 Column with weights of sequence members. Weight is a number of value occurrences.", "Returned value", "Quantile of the specified level.", "Type:", "Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type."], "Examples": ["SELECT quantileInterpolatedWeighted(n, val) FROM t"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantiles"], "Title": ["quantiles"], "Feature": ["quantiles"], "Description": ["quantiles", "Syntax: quantiles(level1, level2, ...)(x)", "All the quantile functions also have corresponding quantiles functions: quantiles, quantilesDeterministic, quantilesTiming, quantilesTimingWeighted, quantilesExact, quantilesExactWeighted, quantileExactWeightedInterpolated, quantileInterpolatedWeighted, quantilesTDigest, quantilesBFloat16, quantilesDD. These functions calculate all the quantiles of the listed levels in one pass, and return an array of the resulting values."], "Examples": [], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantiles"], "Title": ["quantilesExactExclusive"], "Feature": ["quantilesExactExclusive(level1, level2, ...)(expr)"], "Description": ["quantilesExactExclusive", "Exactly computes the quantiles of a numeric data sequence.", "To get exact value, all the passed values are combined into an array, which is then partially sorted. Therefore, the function consumes O(n) memory, where n is a number of values that were passed. However, for a small number of values, the function is very effective.", "This function is equivalent to PERCENTILE.EXC Excel function, (type R6).", "Works more efficiently with sets of levels than quantileExactExclusive.", "Syntax", "quantilesExactExclusive(level1, level2, ...)(expr)", "Arguments", "expr \u2014 Expression over the column values resulting in numeric data types, Date or DateTime.", "Parameters", "level \u2014 Levels of quantiles. Possible values: (0, 1) \u2014 bounds not included. Float.", "Returned value", "Array of quantiles of the specified levels.", "Type of array values:", "Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type."], "Examples": ["CREATE TABLE num AS numbers(1000);SELECT quantilesExactExclusive(0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999)(x) FROM (SELECT number AS x FROM num);"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantiles"], "Title": ["quantilesExactInclusive"], "Feature": ["quantilesExactInclusive(level1, level2, ...)(expr)"], "Description": ["quantilesExactInclusive", "Exactly computes the quantiles of a numeric data sequence.", "To get exact value, all the passed values are combined into an array, which is then partially sorted. Therefore, the function consumes O(n) memory, where n is a number of values that were passed. However, for a small number of values, the function is very effective.", "This function is equivalent to PERCENTILE.INC Excel function, (type R7).", "Works more efficiently with sets of levels than quantileExactInclusive.", "Syntax", "quantilesExactInclusive(level1, level2, ...)(expr)", "Arguments", "expr \u2014 Expression over the column values resulting in numeric data types, Date or DateTime.", "Parameters", "level \u2014 Levels of quantiles. Possible values: [0, 1] \u2014 bounds included. Float.", "Returned value", "Array of quantiles of the specified levels.", "Type of array values:", "Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type."], "Examples": ["CREATE TABLE num AS numbers(1000);SELECT quantilesExactInclusive(0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999)(x) FROM (SELECT number AS x FROM num);"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantiles"], "Title": ["quantilesGK"], "Feature": ["quantilesGK(accuracy, level1, level2, ...)(expr)"], "Description": ["quantilesGK", "quantilesGK works similarly with quantileGK but allows us to calculate quantities at different levels simultaneously and returns an array.", "Syntax", "quantilesGK(accuracy, level1, level2, ...)(expr)", "Returned value", "Array of quantiles of the specified levels.", "Type of array values:", "Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type."], "Examples": ["SELECT quantilesGK(1, 0.25, 0.5, 0.75)(number + 1)FROM numbers(1000)\u250c\u2500quantilesGK(1, 0.25, 0.5, 0.75)(plus(number, 1))\u2500\u2510\u2502 [1,1,1]                                          \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT quantilesGK(10, 0.25, 0.5, 0.75)(number + 1)FROM numbers(1000)\u250c\u2500quantilesGK(10, 0.25, 0.5, 0.75)(plus(number, 1))\u2500\u2510\u2502 [156,413,659]                                     \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT quantilesGK(100, 0.25, 0.5, 0.75)(number + 1)FROM numbers(1000)\u250c\u2500quantilesGK(100, 0.25, 0.5, 0.75)(plus(number, 1))\u2500\u2510\u2502 [251,498,741]                                      \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518SELECT quantilesGK(1000, 0.25, 0.5, 0.75)(number + 1)FROM numbers(1000)\u250c\u2500quantilesGK(1000, 0.25, 0.5, 0.75)(plus(number, 1))\u2500\u2510\u2502 [249,499,749]                                       \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantiletdigest"], "Title": ["quantileTDigest"], "Feature": ["quantileTDigest(level)(expr)"], "Description": ["quantileTDigest", "Computes an approximate quantile of a numeric data sequence using the t-digest algorithm.", "Memory consumption is log(n), where n is a number of values. The result depends on the order of running the query, and is nondeterministic.", "The performance of the function is lower than performance of quantile or quantileTiming. In terms of the ratio of State size to precision, this function is much better than quantile.", "When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function.", "Syntax", "quantileTDigest(level)(expr)", "Alias: medianTDigest.", "Arguments", "level \u2014 Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr \u2014 Expression over the column values resulting in numeric data types, Date or DateTime.", "Returned value", "Approximate quantile of the specified level.", "Type:", "Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type."], "Examples": ["SELECT quantileTDigest(number) FROM numbers(10)"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantiletdigestweighted"], "Title": ["quantileTDigestWeighted"], "Feature": ["quantileTDigestWeighted(level)(expr, weight)"], "Description": ["quantileTDigestWeighted", "Computes an approximate quantile of a numeric data sequence using the t-digest algorithm. The function takes into account the weight of each sequence member. The maximum error is 1%. Memory consumption is log(n), where n is a number of values.", "The performance of the function is lower than performance of quantile or quantileTiming. In terms of the ratio of State size to precision, this function is much better than quantile.", "The result depends on the order of running the query, and is nondeterministic.", "When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function.", "NoteUsing quantileTDigestWeighted is not recommended for tiny data sets and can lead to significant error. In this case, consider possibility of using quantileTDigest instead.", "Syntax", "quantileTDigestWeighted(level)(expr, weight)", "Alias: medianTDigestWeighted.", "Arguments", "level \u2014 Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr \u2014 Expression over the column values resulting in numeric data types, Date or DateTime.weight \u2014 Column with weights of sequence elements. Weight is a number of value occurrences.", "Returned value", "Approximate quantile of the specified level.", "Type:", "Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type."], "Examples": ["SELECT quantileTDigestWeighted(number, 1) FROM numbers(10)"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantiletiming"], "Title": ["quantileTiming"], "Feature": ["quantileTiming(level)(expr)"], "Description": ["quantileTiming", "With the determined precision computes the quantile of a numeric data sequence.", "The result is deterministic (it does not depend on the query processing order). The function is optimized for working with sequences which describe distributions like loading web pages times or backend response times.", "When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function.", "Syntax", "quantileTiming(level)(expr)", "Alias: medianTiming.", "Arguments", "level \u2014 Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr \u2014 Expression over a column values returning a Float*-type number.If negative values are passed to the function, the behavior is undefined.If the value is greater than 30,000 (a page loading time of more than 30 seconds), it is assumed to be 30,000.", "Accuracy", "The calculation is accurate if:", "Total number of values does not exceed 5670.Total number of values exceeds 5670, but the page loading time is less than 1024ms.", "Otherwise, the result of the calculation is rounded to the nearest multiple of 16 ms.", "NoteFor calculating page loading time quantiles, this function is more effective and accurate than quantile.", "Returned value", "Quantile of the specified level.", "Type: Float32.", "NoteIf no values are passed to the function (when using quantileTimingIf), NaN is returned. The purpose of this is to differentiate these cases from cases that result in zero. See ORDER BY clause for notes on sorting NaN values."], "Examples": ["SELECT quantileTiming(response_time) FROM t"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantiletimingweighted"], "Title": ["quantileTimingWeighted"], "Feature": ["quantileTimingWeighted(level)(expr, weight)"], "Description": ["quantileTimingWeighted", "With the determined precision computes the quantile of a numeric data sequence according to the weight of each sequence member.", "The result is deterministic (it does not depend on the query processing order). The function is optimized for working with sequences which describe distributions like loading web pages times or backend response times.", "When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function.", "Syntax", "quantileTimingWeighted(level)(expr, weight)", "Alias: medianTimingWeighted.", "Arguments", "level \u2014 Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr \u2014 Expression over a column values returning a Float*-type number.  - If negative values are passed to the function, the behavior is undefined.  - If the value is greater than 30,000 (a page loading time of more than 30 seconds), it is assumed to be 30,000.weight \u2014 Column with weights of sequence elements. Weight is a number of value occurrences.", "Accuracy", "The calculation is accurate if:", "Total number of values does not exceed 5670.Total number of values exceeds 5670, but the page loading time is less than 1024ms.", "Otherwise, the result of the calculation is rounded to the nearest multiple of 16 ms.", "NoteFor calculating page loading time quantiles, this function is more effective and accurate than quantile.", "Returned value", "Quantile of the specified level.", "Type: Float32.", "NoteIf no values are passed to the function (when using quantileTimingIf), NaN is returned. The purpose of this is to differentiate these cases from cases that result in zero. See ORDER BY clause for notes on sorting NaN values."], "Examples": ["SELECT quantileTimingWeighted(response_time, weight) FROM t"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/quantiletimingweighted"], "Title": ["quantilesTimingWeighted"], "Feature": ["quantilesTimingWeighted"], "Description": ["quantilesTimingWeighted", "Same as quantileTimingWeighted, but accept multiple parameters with quantile levels and return an Array filled with many values of that quantiles."], "Examples": ["SELECT quantilesTimingWeighted(0,5, 0.99)(response_time, weight) FROM t"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/rankCorr"], "Title": ["rankCorr"], "Feature": ["rankCorr(x, y)"], "Description": ["rankCorr", "Computes a rank correlation coefficient.", "Syntax", "rankCorr(x, y)", "Arguments", "x \u2014 Arbitrary value. Float32 or Float64.y \u2014 Arbitrary value. Float32 or Float64.", "Returned value(s)", "Returns a rank correlation coefficient of the ranks of x and y. The value of the correlation coefficient ranges from -1 to +1. If less than two arguments are passed, the function will return an exception. The value close to +1 denotes a high linear relationship, and with an increase of one random variable, the second random variable also increases. The value close to -1 denotes a high linear relationship, and with an increase of one random variable, the second random variable decreases. The value close or equal to 0 denotes no relationship between the two random variables.", "Type: Float64."], "Examples": ["SELECT rankCorr(number, number) FROM numbers(100);", "SELECT roundBankers(rankCorr(exp(number), sin(number)), 3) FROM numbers(100);"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/simplelinearregression"], "Title": ["simpleLinearRegression"], "Feature": ["simpleLinearRegression(x, y)"], "Description": ["simpleLinearRegression", "Performs simple (unidimensional) linear regression.", "simpleLinearRegression(x, y)", "Parameters:", "x \u2014 Column with explanatory variable values.y \u2014 Column with dependent variable values.", "Returned values:", "Constants (k, b) of the resulting line y = k*x + b."], "Examples": ["SELECT arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [0, 1, 2, 3])", "SELECT arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [3, 4, 5, 6])"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/singlevalueornull"], "Title": ["singleValueOrNull"], "Feature": ["singleValueOrNull(x)"], "Description": ["singleValueOrNull", "The aggregate function singleValueOrNull is used to implement subquery operators, such as x = ALL (SELECT ...). It checks if there is only one unique non-NULL value in the data.\nIf there is only one unique value, it returns it. If there are zero or at least two distinct values, it returns NULL.", "Syntax", "singleValueOrNull(x)", "Parameters", "x \u2014 Column of any data type (except Map, Array or Tuple which cannot be of type Nullable).", "Returned values", "The unique value, if there is only one unique non-NULL value in x.NULL, if there are zero or at least two distinct values."], "Examples": ["CREATE TABLE test (x UInt8 NULL) ENGINE=Log;INSERT INTO test (x) VALUES (NULL), (NULL), (5), (NULL), (NULL);SELECT singleValueOrNull(x) FROM test;", "INSERT INTO test (x) VALUES (10);SELECT singleValueOrNull(x) FROM test;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/skewpop"], "Title": ["skewPop"], "Feature": ["skewPop(expr)"], "Description": ["skewPop", "Computes the skewness of a sequence.", "skewPop(expr)", "Arguments", "expr \u2014 Expression returning a number.", "Returned value", "The skewness of the given distribution. Type \u2014 Float64"], "Examples": ["SELECT skewPop(value) FROM series_with_value_column;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/skewsamp"], "Title": ["skewSamp"], "Feature": ["skewSamp(expr)"], "Description": ["skewSamp", "Computes the sample skewness of a sequence.", "It represents an unbiased estimate of the skewness of a random variable if passed values form its sample.", "skewSamp(expr)", "Arguments", "expr \u2014 Expression returning a number.", "Returned value", "The skewness of the given distribution. Type \u2014 Float64. If n <= 1 (n is the size of the sample), then the function returns nan."], "Examples": ["SELECT skewSamp(value) FROM series_with_value_column;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/sparkbar"], "Title": ["sparkbar"], "Feature": ["sparkbar(buckets[, min_x, max_x])(x, y)"], "Description": ["sparkbar", "The function plots a frequency histogram for values x and the repetition rate y of these values over the interval [min_x, max_x].\nRepetitions for all x falling into the same bucket are averaged, so data should be pre-aggregated.\nNegative repetitions are ignored.", "If no interval is specified, then the minimum x is used as the interval start, and the maximum x \u2014 as the interval end.\nOtherwise, values outside the interval are ignored.", "Syntax", "sparkbar(buckets[, min_x, max_x])(x, y)", "Parameters", "buckets \u2014 The number of segments. Type: Integer.min_x \u2014 The interval start. Optional parameter.max_x \u2014 The interval end. Optional parameter.", "Arguments", "x \u2014 The field with values.y \u2014 The field with the frequency of values.", "Returned value", "The frequency histogram."], "Examples": ["CREATE TABLE spark_bar_data (`value` Int64, `event_date` Date) ENGINE = MergeTree ORDER BY event_date;INSERT INTO spark_bar_data VALUES (1,'2020-01-01'), (3,'2020-01-02'), (4,'2020-01-02'), (-3,'2020-01-02'), (5,'2020-01-03'), (2,'2020-01-04'), (3,'2020-01-05'), (7,'2020-01-06'), (6,'2020-01-07'), (8,'2020-01-08'), (2,'2020-01-11');SELECT sparkbar(9)(event_date,cnt) FROM (SELECT sum(value) as cnt, event_date FROM spark_bar_data GROUP BY event_date);SELECT sparkbar(9, toDate('2020-01-01'), toDate('2020-01-10'))(event_date,cnt) FROM (SELECT sum(value) as cnt, event_date FROM spark_bar_data GROUP BY event_date);"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/stddevpop"], "Title": ["stddevPop"], "Feature": ["stddevPop(x)"], "Description": ["stddevPop", "The result is equal to the square root of varPop.", "Aliases: STD, STDDEV_POP.", "NoteThis function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the stddevPopStable function. It works slower but provides a lower computational error.", "Syntax", "stddevPop(x)", "Parameters", "x: Population of values to find the standard deviation of. (U)Int*, Float*, Decimal*.", "Returned value", "Square root of standard deviation of x. Float64."], "Examples": ["DROP TABLE IF EXISTS test_data;CREATE TABLE test_data(    population UInt8,)ENGINE = Log;INSERT INTO test_data VALUES (3),(3),(3),(4),(4),(5),(5),(7),(11),(15);SELECT    stddevPop(population) AS stddevFROM test_data;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/stddevpopstable"], "Title": ["stddevPopStable"], "Feature": ["stddevPopStable(x)"], "Description": ["stddevPopStable", "The result is equal to the square root of varPop. Unlike stddevPop, this function uses a numerically stable algorithm. It works slower but provides a lower computational error.", "Syntax", "stddevPopStable(x)", "Parameters", "x: Population of values to find the standard deviation of. (U)Int*, Float*, Decimal*.", "Returned value", "Square root of standard deviation of x. Float64."], "Examples": ["DROP TABLE IF EXISTS test_data;CREATE TABLE test_data(    population Float64,)ENGINE = Log;INSERT INTO test_data SELECT randUniform(5.5, 10) FROM numbers(1000000)SELECT    stddevPopStable(population) AS stddevFROM test_data;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/stddevsamp"], "Title": ["stddevSamp"], "Feature": ["stddevSamp(x)"], "Description": ["stddevSamp", "The result is equal to the square root of varSamp.", "Alias: STDDEV_SAMP.", "NoteThis function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the stddevSampStable function. It works slower but provides a lower computational error.", "Syntax", "stddevSamp(x)", "Parameters", "x: Values for which to find the square root of sample variance. (U)Int*, Float*, Decimal*.", "Returned value", "Square root of sample variance of x. Float64."], "Examples": ["DROP TABLE IF EXISTS test_data;CREATE TABLE test_data(    population UInt8,)ENGINE = Log;INSERT INTO test_data VALUES (3),(3),(3),(4),(4),(5),(5),(7),(11),(15);SELECT    stddevSamp(population)FROM test_data;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/stddevsampstable"], "Title": ["stddevSampStable"], "Feature": ["stddevSampStable(x)"], "Description": ["stddevSampStable", "The result is equal to the square root of varSamp. Unlike stddevSamp This function uses a numerically stable algorithm. It works slower but provides a lower computational error.", "Syntax", "stddevSampStable(x)", "Parameters", "x: Values for which to find the square root of sample variance. (U)Int*, Float*, Decimal*.", "Returned value", "Square root of sample variance of x. Float64."], "Examples": ["DROP TABLE IF EXISTS test_data;CREATE TABLE test_data(    population UInt8,)ENGINE = Log;INSERT INTO test_data VALUES (3),(3),(3),(4),(4),(5),(5),(7),(11),(15);SELECT    stddevSampStable(population)FROM test_data;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/stochasticlinearregression"], "Title": ["stochasticLinearRegression"], "Feature": ["stochasticLinearRegression"], "Description": ["stochasticLinearRegression", "This function implements stochastic linear regression. It supports custom parameters for learning rate, L2 regularization coefficient, mini-batch size, and has a few methods for updating weights (Adam (used by default), simple SGD, Momentum, and Nesterov).", "Parameters", "There are 4 customizable parameters. They are passed to the function sequentially, but there is no need to pass all four - default values will be used, however good model required some parameter tuning.", "stochasticLinearRegression(0.00001, 0.1, 15, 'Adam')", "learning rate is the coefficient on step length, when the gradient descent step is performed. A learning rate that is too big may cause infinite weights of the model. Default is 0.00001.l2 regularization coefficient which may help to prevent overfitting. Default is 0.1.mini-batch size sets the number of elements, which gradients will be computed and summed to perform one step of gradient descent. Pure stochastic descent uses one element, however, having small batches (about 10 elements) makes gradient steps more stable. Default is 15.method for updating weights, they are: Adam (by default), SGD, Momentum, and Nesterov. Momentum and Nesterov require a little bit more computations and memory, however, they happen to be useful in terms of speed of convergence and stability of stochastic gradient methods.", "Usage", "stochasticLinearRegression is used in two steps: fitting the model and predicting on new data. In order to fit the model and save its state for later usage, we use the -State combinator, which saves the state (e.g. model weights).\nTo predict, we use the function evalMLMethod, which takes a state as an argument as well as features to predict on.", "1. Fitting", "Such query may be used.", "CREATE TABLE IF NOT EXISTS train_data(    param1 Float64,    param2 Float64,    target Float64) ENGINE = Memory;CREATE TABLE your_model ENGINE = Memory AS SELECTstochasticLinearRegressionState(0.1, 0.0, 5, 'SGD')(target, param1, param2)AS state FROM train_data;", "Here, we also need to insert data into the train_data table. The number of parameters is not fixed, it depends only on the number of arguments passed into linearRegressionState. They all must be numeric values.\nNote that the column with target value (which we would like to learn to predict) is inserted as the first argument.", "2. Predicting", "After saving a state into the table, we may use it multiple times for prediction or even merge with other states and create new, even better models.", "WITH (SELECT state FROM your_model) AS model SELECTevalMLMethod(model, param1, param2) FROM test_data", "The query will return a column of predicted values. Note that first argument of evalMLMethod is AggregateFunctionState object, next are columns of features.", "test_data is a table like train_data but may not contain target value.", "Notes", "To merge two models user may create such query:\nsql  SELECT state1 + state2 FROM your_models\nwhere your_models table contains both models. This query will return new AggregateFunctionState object.User may fetch weights of the created model for its own purposes without saving the model if no -State combinator is used.\nsql  SELECT stochasticLinearRegression(0.01)(target, param1, param2) FROM train_data\nSuch query will fit the model and return its weights - first are weights, which correspond to the parameters of the model, the last one is bias. So in the example above the query will return a column with 3 values.", "See Also", "stochasticLogisticRegressionDifference between linear and logistic regressions"], "Examples": ["CREATE TABLE IF NOT EXISTS train_data(    param1 Float64,    param2 Float64,    target Float64) ENGINE = Memory;CREATE TABLE your_model ENGINE = Memory AS SELECTstochasticLinearRegressionState(0.1, 0.0, 5, 'SGD')(target, param1, param2)AS state FROM train_data;", "WITH (SELECT state FROM your_model) AS model SELECTevalMLMethod(model, param1, param2) FROM test_data"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/stochasticlogisticregression"], "Title": ["stochasticLogisticRegression"], "Feature": ["stochasticLogisticRegression"], "Description": ["stochasticLogisticRegression", "This function implements stochastic logistic regression. It can be used for binary classification problem, supports the same custom parameters as stochasticLinearRegression and works the same way.", "Parameters", "Parameters are exactly the same as in stochasticLinearRegression:\nlearning rate, l2 regularization coefficient, mini-batch size, method for updating weights.\nFor more information see parameters.", "stochasticLogisticRegression(1.0, 1.0, 10, 'SGD')", "1. Fitting", "See the `Fitting` section in the [stochasticLinearRegression](#stochasticlinearregression-usage-fitting) description.Predicted labels have to be in \\[-1, 1\\].", "2. Predicting", "Using saved state we can predict probability of object having label `1`.``` sqlWITH (SELECT state FROM your_model) AS model SELECTevalMLMethod(model, param1, param2) FROM test_data```The query will return a column of probabilities. Note that first argument of `evalMLMethod` is `AggregateFunctionState` object, next are columns of features.We can also set a bound of probability, which assigns elements to different labels.``` sqlSELECT ans < 1.1 AND ans > 0.5 FROM(WITH (SELECT state FROM your_model) AS model SELECTevalMLMethod(model, param1, param2) AS ans FROM test_data)```Then the result will be labels.`test_data` is a table like `train_data` but may not contain target value.", "See Also", "stochasticLinearRegressionDifference between linear and logistic regressions."], "Examples": [], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/studentttest"], "Title": ["studentTTest"], "Feature": ["studentTTest([confidence_level])(sample_data, sample_index)"], "Description": ["studentTTest", "Applies Student's t-test to samples from two populations.", "Syntax", "studentTTest([confidence_level])(sample_data, sample_index)", "Values of both samples are in the sample_data column. If sample_index equals to 0 then the value in that row belongs to the sample from the first population. Otherwise it belongs to the sample from the second population.\nThe null hypothesis is that means of populations are equal. Normal distribution with equal variances is assumed.", "Arguments", "sample_data \u2014 Sample data. Integer, Float or Decimal.sample_index \u2014 Sample index. Integer.", "Parameters", "confidence_level \u2014 Confidence level in order to calculate confidence intervals. Float.", "Returned values", "Tuple with two or four elements (if the optional confidence_level is specified):", "calculated t-statistic. Float64.calculated p-value. Float64.[calculated confidence-interval-low. Float64.][calculated confidence-interval-high. Float64.]"], "Examples": ["SELECT studentTTest(sample_data, sample_index) FROM student_ttest;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/sum"], "Title": ["sum"], "Feature": ["sum(num)"], "Description": ["sum", "Calculates the sum. Only works for numbers.", "Syntax", "sum(num)", "Parameters", "num: Column of numeric values. (U)Int*, Float*, Decimal*.", "Returned value", "The sum of the values. (U)Int*, Float*, Decimal*."], "Examples": ["CREATE TABLE employees(    `id` UInt32,    `name` String,    `salary` UInt32)ENGINE = Log", "INSERT INTO employees VALUES    (87432, 'John Smith', 45680),    (59018, 'Jane Smith', 72350),    (20376, 'Ivan Ivanovich', 58900),    (71245, 'Anastasia Ivanovna', 89210);", "SELECT sum(salary) FROM employees;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/summap"], "Title": ["sumMap"], "Feature": ["CREATE TABLE sum_map(    date Date,    timeslot DateTime,    statusMap Nested(        status UInt16,        requests UInt64    ),    statusMapTuple Tuple(Array(Int32), Array(Int32))) ENGINE = Log;"], "Description": ["sumMap", "Totals a value array according to the keys specified in the key array. Returns a tuple of two arrays: keys in sorted order, and values summed for the corresponding keys without overflow.", "Syntax", "sumMap(key <Array>, value <Array>) Array type.sumMap(Tuple(key <Array>, value <Array>)) Tuple type.", "Alias: sumMappedArrays.", "Arguments ", "key: Array of keys.value: Array of values.", "Passing a tuple of key and value arrays is a synonym to passing separately an array of keys and an array of values.", "NoteThe number of elements in key and value must be the same for each row that is totaled.", "Returned Value ", "Returns a tuple of two arrays: keys in sorted order, and values summed for the corresponding keys."], "Examples": ["CREATE TABLE sum_map(    date Date,    timeslot DateTime,    statusMap Nested(        status UInt16,        requests UInt64    ),    statusMapTuple Tuple(Array(Int32), Array(Int32))) ENGINE = Log;", "INSERT INTO sum_map VALUES    ('2000-01-01', '2000-01-01 00:00:00', [1, 2, 3], [10, 10, 10], ([1, 2, 3], [10, 10, 10])),    ('2000-01-01', '2000-01-01 00:00:00', [3, 4, 5], [10, 10, 10], ([3, 4, 5], [10, 10, 10])),    ('2000-01-01', '2000-01-01 00:01:00', [4, 5, 6], [10, 10, 10], ([4, 5, 6], [10, 10, 10])),    ('2000-01-01', '2000-01-01 00:01:00', [6, 7, 8], [10, 10, 10], ([6, 7, 8], [10, 10, 10]));", "SELECT    timeslot,    sumMap(statusMap.status, statusMap.requests),    sumMap(statusMapTuple)FROM sum_mapGROUP BY timeslot"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/summapwithoverflow"], "Title": ["sumMapWithOverflow"], "Feature": ["CREATE TABLE sum_map(    date Date,    timeslot DateTime,    statusMap Nested(        status UInt8,        requests UInt8    ),    statusMapTuple Tuple(Array(Int8), Array(Int8))) ENGINE = Log;"], "Description": ["sumMapWithOverflow", "Totals a value array according to the keys specified in the key array. Returns a tuple of two arrays: keys in sorted order, and values summed for the corresponding keys.\nIt differs from the sumMap function in that it does summation with overflow - i.e. returns the same data type for the summation as the argument data type.", "Syntax", "sumMapWithOverflow(key <Array>, value <Array>) Array type.sumMapWithOverflow(Tuple(key <Array>, value <Array>)) Tuple type.", "Arguments ", "key: Array of keys.value: Array of values.", "Passing a tuple of key and value arrays is a synonym to passing separately an array of keys and an array of values.", "NoteThe number of elements in key and value must be the same for each row that is totaled.", "Returned Value ", "Returns a tuple of two arrays: keys in sorted order, and values summed for the corresponding keys."], "Examples": ["CREATE TABLE sum_map(    date Date,    timeslot DateTime,    statusMap Nested(        status UInt8,        requests UInt8    ),    statusMapTuple Tuple(Array(Int8), Array(Int8))) ENGINE = Log;", "INSERT INTO sum_map VALUES    ('2000-01-01', '2000-01-01 00:00:00', [1, 2, 3], [10, 10, 10], ([1, 2, 3], [10, 10, 10])),    ('2000-01-01', '2000-01-01 00:00:00', [3, 4, 5], [10, 10, 10], ([3, 4, 5], [10, 10, 10])),    ('2000-01-01', '2000-01-01 00:01:00', [4, 5, 6], [10, 10, 10], ([4, 5, 6], [10, 10, 10])),    ('2000-01-01', '2000-01-01 00:01:00', [6, 7, 8], [10, 10, 10], ([6, 7, 8], [10, 10, 10]));", "SELECT    timeslot,    toTypeName(sumMap(statusMap.status, statusMap.requests)),    toTypeName(sumMapWithOverflow(statusMap.status, statusMap.requests)),FROM sum_mapGROUP BY timeslot", "SELECT    timeslot,    toTypeName(sumMap(statusMapTuple)),    toTypeName(sumMapWithOverflow(statusMapTuple)),FROM sum_mapGROUP BY timeslot"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/sumwithoverflow"], "Title": ["sumWithOverflow"], "Feature": ["sumWithOverflow(num)"], "Description": ["sumWithOverflow", "Computes the sum of the numbers, using the same data type for the result as for the input parameters. If the sum exceeds the maximum value for this data type, it is calculated with overflow.", "Only works for numbers.", "Syntax", "sumWithOverflow(num)", "Parameters", "num: Column of numeric values. (U)Int*, Float*, Decimal*.", "Returned value", "The sum of the values. (U)Int*, Float*, Decimal*."], "Examples": ["CREATE TABLE employees(    `id` UInt32,    `name` String,    `monthly_salary` UInt16)ENGINE = Log", "SELECT    sum(monthly_salary) AS no_overflow,    sumWithOverflow(monthly_salary) AS overflow,    toTypeName(no_overflow),    toTypeName(overflow)FROM employees", "SELECT     sum(monthly_salary) AS no_overflow,    sumWithOverflow(monthly_salary) AS overflow,    toTypeName(no_overflow),    toTypeName(overflow),    FROM employees;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/theilsu"], "Title": ["theilsU"], "Feature": ["theilsU(column1, column2)"], "Description": ["theilsU", "The theilsU function calculates the Theil's U uncertainty coefficient, a value that measures the association between two columns in a table. Its values range from \u22121.0 (100% negative association, or perfect inversion) to +1.0 (100% positive association, or perfect agreement). A value of 0.0 indicates the absence of association.", "Syntax", "theilsU(column1, column2)", "Arguments", "column1 and column2 are the columns to be compared", "Returned value", "a value between -1 and 1", "Return type is always Float64."], "Examples": ["SELECT    theilsU(a ,b)FROM    (        SELECT            number % 10 AS a,            number % 4 AS b        FROM            numbers(150)    );"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/topk"], "Title": ["topK"], "Feature": ["topK(N)(column)topK(N, load_factor)(column)topK(N, load_factor, 'counts')(column)"], "Description": ["topK", "Returns an array of the approximately most frequent values in the specified column. The resulting array is sorted in descending order of approximate frequency of values (not by the values themselves).", "Implements the Filtered Space-Saving algorithm for analyzing TopK, based on the reduce-and-combine algorithm from Parallel Space Saving.", "topK(N)(column)topK(N, load_factor)(column)topK(N, load_factor, 'counts')(column)", "This function does not provide a guaranteed result. In certain situations, errors might occur and it might return frequent values that aren\u2019t the most frequent values.", "We recommend using the N < 10 value; performance is reduced with large N values. Maximum value of N = 65536.", "Parameters", "N \u2014 The number of elements to return. Optional. Default value: 10.load_factor \u2014 Defines, how many cells reserved for values. If uniq(column) > N * load_factor, result of topK function will be approximate. Optional. Default value: 3.counts \u2014 Defines, should result contain approximate count and error value.", "Arguments", "column \u2014 The value to calculate frequency."], "Examples": ["SELECT topK(3)(AirlineID) AS resFROM ontime"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/topkweighted"], "Title": ["topKWeighted"], "Feature": ["topKWeighted(N)(column, weight)topKWeighted(N, load_factor)(column, weight)topKWeighted(N, load_factor, 'counts')(column, weight)"], "Description": ["topKWeighted", "Returns an array of the approximately most frequent values in the specified column. The resulting array is sorted in descending order of approximate frequency of values (not by the values themselves). Additionally, the weight of the value is taken into account.", "Syntax", "topKWeighted(N)(column, weight)topKWeighted(N, load_factor)(column, weight)topKWeighted(N, load_factor, 'counts')(column, weight)", "Parameters", "N \u2014 The number of elements to return. Optional. Default value: 10.load_factor \u2014 Defines, how many cells reserved for values. If uniq(column) > N * load_factor, result of topK function will be approximate. Optional. Default value: 3.counts \u2014 Defines, should result contain approximate count and error value.", "Arguments", "column \u2014 The value.weight \u2014 The weight. Every value is accounted weight times for frequency calculation. UInt64.", "Returned value", "Returns an array of the values with maximum approximate sum of weights."], "Examples": ["SELECT topKWeighted(2)(k, w) FROMVALUES('k Char, w UInt64', ('y', 1), ('y', 1), ('x', 5), ('y', 1), ('z', 10))", "SELECT topKWeighted(2, 10, 'counts')(k, w)FROM VALUES('k Char, w UInt64', ('y', 1), ('y', 1), ('x', 5), ('y', 1), ('z', 10))"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/uniq"], "Title": ["uniq"], "Feature": ["uniq"], "Description": ["uniq", "Calculates the approximate number of different values of the argument.", "uniq(x[, ...])", "Arguments", "The function takes a variable number of parameters. Parameters can be Tuple, Array, Date, DateTime, String, or numeric types.", "Returned value", "A UInt64-type number.", "Implementation details", "Function:", "Calculates a hash for all parameters in the aggregate, then uses it in calculations.Uses an adaptive sampling algorithm. For the calculation state, the function uses a sample of element hash values up to 65536. This algorithm is very accurate and very efficient on the CPU. When the query contains several of these functions, using uniq is almost as fast as using other aggregate functions.Provides the result deterministically (it does not depend on the query processing order).", "We recommend using this function in almost all scenarios.", "See Also", "uniqCombineduniqCombined64uniqHLL12uniqExactuniqTheta"], "Examples": ["uniq(x[, ...])"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/uniqcombined"], "Title": ["uniqCombined"], "Feature": ["uniqCombined(HLL_precision)(x[, ...])"], "Description": ["uniqCombined", "Calculates the approximate number of different argument values.", "uniqCombined(HLL_precision)(x[, ...])", "The uniqCombined function is a good choice for calculating the number of different values.", "Arguments", "HLL_precision: The base-2 logarithm of the number of cells in HyperLogLog. Optional, you can use the function as uniqCombined(x[, ...]). The default value for HLL_precision is 17, which is effectively 96 KiB of space (2^17 cells, 6 bits each).X: A variable number of parameters. Parameters can be Tuple, Array, Date, DateTime, String, or numeric types.", "Returned value", "A number UInt64-type number.", "Implementation details", "The uniqCombined function:", "Calculates a hash (64-bit hash for String and 32-bit otherwise) for all parameters in the aggregate, then uses it in calculations.Uses a combination of three algorithms: array, hash table, and HyperLogLog with an error correction table.For a small number of distinct elements, an array is used. When the set size is larger, a hash table is used. For a larger number of elements, HyperLogLog is used, which will occupy a fixed amount of memory.Provides the result deterministically (it does not depend on the query processing order).", "NoteSince it uses a 32-bit hash for non-String types, the result will have very high error for cardinalities significantly larger than UINT_MAX (error will raise quickly after a few tens of billions of distinct values), hence in this case you should use uniqCombined64.", "Compared to the uniq function, the uniqCombined function:", "Consumes several times less memory.Calculates with several times higher accuracy.Usually has slightly lower performance. In some scenarios, uniqCombined can perform better than uniq, for example, with distributed queries that transmit a large number of aggregation states over the network."], "Examples": ["SELECT uniqCombined(number) FROM numbers(1e6);"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/uniqcombined64"], "Title": ["uniqCombined64"], "Feature": ["uniqCombined64(HLL_precision)(x[, ...])"], "Description": ["uniqCombined64", "Calculates the approximate number of different argument values. It is the same as uniqCombined, but uses a 64-bit hash for all data types rather than just for the String data type.", "uniqCombined64(HLL_precision)(x[, ...])", "Parameters", "HLL_precision: The base-2 logarithm of the number of cells in HyperLogLog. Optionally, you can use the function as uniqCombined64(x[, ...]). The default value for HLL_precision is 17, which is effectively 96 KiB of space (2^17 cells, 6 bits each).X: A variable number of parameters. Parameters can be Tuple, Array, Date, DateTime, String, or numeric types.", "Returned value", "A number UInt64-type number.", "Implementation details", "The uniqCombined64 function:", "Calculates a hash (64-bit hash for all data types) for all parameters in the aggregate, then uses it in calculations.Uses a combination of three algorithms: array, hash table, and HyperLogLog with an error correction table.For a small number of distinct elements, an array is used. When the set size is larger, a hash table is used. For a larger number of elements, HyperLogLog is used, which will occupy a fixed amount of memory.Provides the result deterministically (it does not depend on the query processing order).", "NoteSince it uses 64-bit hash for all types, the result does not suffer from very high error for cardinalities significantly larger than UINT_MAX like uniqCombined does, which uses a 32-bit hash for non-String types.", "Compared to the uniq function, the uniqCombined64 function:", "Consumes several times less memory.Calculates with several times higher accuracy."], "Examples": ["SELECT uniqCombined64(number) FROM numbers(1e10);", "SELECT uniqCombined(number) FROM numbers(1e10);"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/uniqexact"], "Title": ["uniqExact"], "Feature": ["uniqExact"], "Description": ["uniqExact", "Calculates the exact number of different argument values.", "uniqExact(x[, ...])", "Use the uniqExact function if you absolutely need an exact result. Otherwise use the uniq function.", "The uniqExact function uses more memory than uniq, because the size of the state has unbounded growth as the number of different values increases.", "Arguments", "The function takes a variable number of parameters. Parameters can be Tuple, Array, Date, DateTime, String, or numeric types.", "See Also", "uniquniqCombineduniqHLL12uniqTheta"], "Examples": ["uniqExact(x[, ...])"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/uniqhll12"], "Title": ["uniqHLL12"], "Feature": ["uniqHLL12"], "Description": ["uniqHLL12", "Calculates the approximate number of different argument values, using the HyperLogLog algorithm.", "uniqHLL12(x[, ...])", "Arguments", "The function takes a variable number of parameters. Parameters can be Tuple, Array, Date, DateTime, String, or numeric types.", "Returned value", "A UInt64-type number.", "Implementation details", "Function:", "Calculates a hash for all parameters in the aggregate, then uses it in calculations.Uses the HyperLogLog algorithm to approximate the number of different argument values.  2^12 5-bit cells are used. The size of the state is slightly more than 2.5 KB. The result is not very accurate (up to ~10% error) for small data sets (<10K elements). However, the result is fairly accurate for high-cardinality data sets (10K-100M), with a maximum error of ~1.6%. Starting from 100M, the estimation error increases, and the function will return very inaccurate results for data sets with extremely high cardinality (1B+ elements).Provides the determinate result (it does not depend on the query processing order).", "We do not recommend using this function. In most cases, use the uniq or uniqCombined function.", "See Also", "uniquniqCombineduniqExactuniqTheta"], "Examples": ["uniqHLL12(x[, ...])"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/varPop"], "Title": ["varPop"], "Feature": ["varPop(x)"], "Description": ["varPop", "Calculates the population variance.", "Syntax", "varPop(x)", "Alias: VAR_POP.", "Parameters", "x: Population of values to find the population variance of. (U)Int*, Float*, Decimal*.", "Returned value", "Returns the population variance of x. Float64."], "Examples": ["DROP TABLE IF EXISTS test_data;CREATE TABLE test_data(    x UInt8,)ENGINE = Memory;INSERT INTO test_data VALUES (3), (3), (3), (4), (4), (5), (5), (7), (11), (15);SELECT    varPop(x) AS var_popFROM test_data;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/varpopstable"], "Title": ["varPopStable"], "Feature": ["varPopStable(x)"], "Description": ["varPopStable", "Returns the population variance. Unlike varPop, this function uses a numerically stable algorithm. It works slower but provides a lower computational error.", "Syntax", "varPopStable(x)", "Alias: VAR_POP_STABLE.", "Parameters", "x: Population of values to find the population variance of. (U)Int*, Float*, Decimal*.", "Returned value", "Returns the population variance of x. Float64."], "Examples": ["DROP TABLE IF EXISTS test_data;CREATE TABLE test_data(    x UInt8,)ENGINE = Memory;INSERT INTO test_data VALUES (3),(3),(3),(4),(4),(5),(5),(7),(11),(15);SELECT    varPopStable(x) AS var_pop_stableFROM test_data;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/varSamp"], "Title": ["varSamp"], "Feature": ["varSamp(x)"], "Description": ["varSamp", "Calculate the sample variance of a data set.", "Syntax", "varSamp(x)", "Alias: VAR_SAMP.", "Parameters", "x: The population for which you want to calculate the sample variance. (U)Int*, Float*, Decimal*.", "Returned value", "Returns the sample variance of the input data set x. Float64.", "Implementation details", "The varSamp function calculates the sample variance using the following formula:", "\u2211(x\u2212mean(x))2(n\u22121)\\sum\\frac{(x - \\text{mean}(x))^2}{(n - 1)}\u2211(n\u22121)(x\u2212mean(x))2", "Where:", "x is each individual data point in the data set.mean(x) is the arithmetic mean of the data set.n is the number of data points in the data set.", "The function assumes that the input data set represents a sample from a larger population. If you want to calculate the variance of the entire population (when you have the complete data set), you should use varPop instead."], "Examples": ["DROP TABLE IF EXISTS test_data;CREATE TABLE test_data(    x Float64)ENGINE = Memory;INSERT INTO test_data VALUES (10.5), (12.3), (9.8), (11.2), (10.7);SELECT round(varSamp(x),3) AS var_samp FROM test_data;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/varsampstable"], "Title": ["varSampStable"], "Feature": ["varSampStable(x)"], "Description": ["varSampStable", "Calculate the sample variance of a data set. Unlike varSamp, this function uses a numerically stable algorithm. It works slower but provides a lower computational error.", "Syntax", "varSampStable(x)", "Alias: VAR_SAMP_STABLE", "Parameters", "x: The population for which you want to calculate the sample variance. (U)Int*, Float*, Decimal*.", "Returned value", "Returns the sample variance of the input data set. Float64.", "Implementation details", "The varSampStable function calculates the sample variance using the same formula as the varSamp:", "\u2211(x\u2212mean(x))2(n\u22121)\\sum\\frac{(x - \\text{mean}(x))^2}{(n - 1)}\u2211(n\u22121)(x\u2212mean(x))2", "Where:", "x is each individual data point in the data set.mean(x) is the arithmetic mean of the data set.n is the number of data points in the data set."], "Examples": ["DROP TABLE IF EXISTS test_data;CREATE TABLE test_data(    x Float64)ENGINE = Memory;INSERT INTO test_data VALUES (10.5), (12.3), (9.8), (11.2), (10.7);SELECT round(varSampStable(x),3) AS var_samp_stable FROM test_data;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/welchttest"], "Title": ["welchTTest"], "Feature": ["welchTTest([confidence_level])(sample_data, sample_index)"], "Description": ["welchTTest", "Applies Welch's t-test to samples from two populations.", "Syntax", "welchTTest([confidence_level])(sample_data, sample_index)", "Values of both samples are in the sample_data column. If sample_index equals to 0 then the value in that row belongs to the sample from the first population. Otherwise it belongs to the sample from the second population.\nThe null hypothesis is that means of populations are equal. Normal distribution is assumed. Populations may have unequal variance.", "Arguments", "sample_data \u2014 Sample data. Integer, Float or Decimal.sample_index \u2014 Sample index. Integer.", "Parameters", "confidence_level \u2014 Confidence level in order to calculate confidence intervals. Float.", "Returned values", "Tuple with two or four elements (if the optional confidence_level is specified)", "calculated t-statistic. Float64.calculated p-value. Float64.calculated confidence-interval-low. Float64.calculated confidence-interval-high. Float64."], "Examples": ["SELECT welchTTest(sample_data, sample_index) FROM welch_ttest;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/distinctdynamictypes"], "Title": ["distinctDynamicTypes"], "Feature": ["distinctDynamicTypes(dynamic)"], "Description": ["distinctDynamicTypes", "Calculates the list of distinct data types stored in Dynamic column.", "Syntax", "distinctDynamicTypes(dynamic)", "Arguments", "dynamic \u2014 Dynamic column.", "Returned Value", "The sorted list of data type names Array(String)."], "Examples": ["DROP TABLE IF EXISTS test_dynamic;CREATE TABLE test_dynamic(d Dynamic) ENGINE = Memory;INSERT INTO test_dynamic VALUES (42), (NULL), ('Hello'), ([1, 2, 3]), ('2020-01-01'), (map(1, 2)), (43), ([4, 5]), (NULL), ('World'), (map(3, 4))", "SELECT distinctDynamicTypes(d) FROM test_dynamic;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/distinctjsonpaths"], "Title": ["distinctJSONPaths"], "Feature": ["distinctJSONPaths(json)"], "Description": ["distinctJSONPaths", "Calculates the list of distinct paths stored in JSON column.", "Syntax", "distinctJSONPaths(json)", "Arguments", "json \u2014 JSON column.", "Returned Value", "The sorted list of paths Array(String)."], "Examples": ["DROP TABLE IF EXISTS test_json;CREATE TABLE test_json(json JSON) ENGINE = Memory;INSERT INTO test_json VALUES ('{\"a\" : 42, \"b\" : \"Hello\"}'), ('{\"b\" : [1, 2, 3], \"c\" : {\"d\" : {\"e\" : \"2020-01-01\"}}}'), ('{\"a\" : 43, \"c\" : {\"d\" : {\"f\" : [{\"g\" : 42}]}}}')", "SELECT distinctJSONPaths(json) FROM test_json;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/distinctjsonpaths"], "Title": ["distinctJSONPathsAndTypes"], "Feature": ["distinctJSONPathsAndTypes(json)"], "Description": ["distinctJSONPathsAndTypes", "Calculates the list of distinct paths and their types stored in JSON column.", "Syntax", "distinctJSONPathsAndTypes(json)", "Arguments", "json \u2014 JSON column.", "Returned Value", "The sorted map of paths and types Map(String, Array(String))."], "Examples": ["DROP TABLE IF EXISTS test_json;CREATE TABLE test_json(json JSON) ENGINE = Memory;INSERT INTO test_json VALUES ('{\"a\" : 42, \"b\" : \"Hello\"}'), ('{\"b\" : [1, 2, 3], \"c\" : {\"d\" : {\"e\" : \"2020-01-01\"}}}'), ('{\"a\" : 43, \"c\" : {\"d\" : {\"f\" : [{\"g\" : 42}]}}}')", "SELECT distinctJSONPathsAndTypes(json) FROM test_json;", "DROP TABLE IF EXISTS test_json;CREATE TABLE test_json(json JSON(a UInt32)) ENGINE = Memory;INSERT INTO test_json VALUES ('{\"b\" : \"Hello\"}'), ('{\"b\" : \"World\", \"c\" : [1, 2, 3]}');", "SELECT json FROM test_json;", "SELECT distinctJSONPaths(json) FROM test_json;", "SELECT distinctJSONPathsAndTypes(json) FROM test_json;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/parametric-functions"], "Title": ["histogram"], "Feature": ["histogram(number_of_bins)(values)"], "Description": ["histogram", "Calculates an adaptive histogram. It does not guarantee precise results.", "histogram(number_of_bins)(values)", "The functions uses A Streaming Parallel Decision Tree Algorithm. The borders of histogram bins are adjusted as new data enters a function. In common case, the widths of bins are not equal.", "Arguments", "values \u2014 Expression resulting in input values.", "Parameters", "number_of_bins \u2014 Upper limit for the number of bins in the histogram. The function automatically calculates the number of bins. It tries to reach the specified number of bins, but if it fails, it uses fewer bins.", "Returned values", "Array of Tuples of the following format:  ```  [(lower_1, upper_1, height_1), ... (lower_N, upper_N, height_N)]  ```  - `lower` \u2014 Lower bound of the bin.  - `upper` \u2014 Upper bound of the bin.  - `height` \u2014 Calculated height of the bin."], "Examples": ["SELECT histogram(5)(number + 1)FROM (    SELECT *    FROM system.numbers    LIMIT 20)", "WITH histogram(5)(rand() % 100) AS histSELECT    arrayJoin(hist).3 AS height,    bar(height, 0, 6, 5) AS barFROM(    SELECT *    FROM system.numbers    LIMIT 20)"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/parametric-functions"], "Title": ["sequenceMatch"], "Feature": ["sequenceMatch(pattern)(timestamp, cond1, cond2, ...)"], "Description": ["sequenceMatch", "Checks whether the sequence contains an event chain that matches the pattern.", "Syntax", "sequenceMatch(pattern)(timestamp, cond1, cond2, ...)", "NoteEvents that occur at the same second may lay in the sequence in an undefined order affecting the result.", "Arguments", "timestamp \u2014 Column considered to contain time data. Typical data types are Date and DateTime. You can also use any of the supported UInt data types.cond1, cond2 \u2014 Conditions that describe the chain of events. Data type: UInt8. You can pass up to 32 condition arguments. The function takes only the events described in these conditions into account. If the sequence contains data that isn\u2019t described in a condition, the function skips them.", "Parameters", "pattern \u2014 Pattern string. See Pattern syntax.", "Returned values", "1, if the pattern is matched.0, if the pattern isn\u2019t matched.", "Type: UInt8.", "Pattern syntax", "(?N) \u2014 Matches the condition argument at position N. Conditions are numbered in the [1, 32] range. For example, (?1) matches the argument passed to the cond1 parameter..* \u2014 Matches any number of events. You do not need conditional arguments to match this element of the pattern.(?t operator value) \u2014 Sets the time in seconds that should separate two events. For example, pattern (?1)(?t>1800)(?2) matches events that occur more than 1800 seconds from each other. An arbitrary number of any events can lay between these events. You can use the >=, >, <, <=, == operators."], "Examples": ["SELECT sequenceMatch('(?1)(?2)')(time, number = 1, number = 2) FROM t", "SELECT sequenceMatch('(?1)(?2)')(time, number = 1, number = 2, number = 3) FROM t", "SELECT sequenceMatch('(?1)(?2)')(time, number = 1, number = 2, number = 4) FROM t"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/parametric-functions"], "Title": ["sequenceCount"], "Feature": ["sequenceCount(pattern)(timestamp, cond1, cond2, ...)"], "Description": ["sequenceCount", "Counts the number of event chains that matched the pattern. The function searches event chains that do not overlap. It starts to search for the next chain after the current chain is matched.", "NoteEvents that occur at the same second may lay in the sequence in an undefined order affecting the result.", "Syntax", "sequenceCount(pattern)(timestamp, cond1, cond2, ...)", "Arguments", "timestamp \u2014 Column considered to contain time data. Typical data types are Date and DateTime. You can also use any of the supported UInt data types.cond1, cond2 \u2014 Conditions that describe the chain of events. Data type: UInt8. You can pass up to 32 condition arguments. The function takes only the events described in these conditions into account. If the sequence contains data that isn\u2019t described in a condition, the function skips them.", "Parameters", "pattern \u2014 Pattern string. See Pattern syntax.", "Returned values", "Number of non-overlapping event chains that are matched.", "Type: UInt64."], "Examples": ["SELECT sequenceCount('(?1).*(?2)')(time, number = 1, number = 2) FROM t"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/parametric-functions"], "Title": ["windowFunnel"], "Feature": ["windowFunnel(window, [mode, [mode, ... ]])(timestamp, cond1, cond2, ..., condN)"], "Description": ["windowFunnel", "Searches for event chains in a sliding time window and calculates the maximum number of events that occurred from the chain.", "The function works according to the algorithm:", "The function searches for data that triggers the first condition in the chain and sets the event counter to 1. This is the moment when the sliding window starts.If events from the chain occur sequentially within the window, the counter is incremented. If the sequence of events is disrupted, the counter isn\u2019t incremented.If the data has multiple event chains at varying points of completion, the function will only output the size of the longest chain.", "Syntax", "windowFunnel(window, [mode, [mode, ... ]])(timestamp, cond1, cond2, ..., condN)", "Arguments", "timestamp \u2014 Name of the column containing the timestamp. Data types supported: Date, DateTime and other unsigned integer types (note that even though timestamp supports the UInt64 type, it\u2019s value can\u2019t exceed the Int64 maximum, which is 2^63 - 1).cond \u2014 Conditions or data describing the chain of events. UInt8.", "Parameters", "window \u2014 Length of the sliding window, it is the time interval between the first and the last condition. The unit of window depends on the timestamp itself and varies. Determined using the expression timestamp of cond1 <= timestamp of cond2 <= ... <= timestamp of condN <= timestamp of cond1 + window.mode \u2014 It is an optional argument. One or more modes can be set.'strict_deduplication' \u2014 If the same condition holds for the sequence of events, then such repeating event interrupts further processing. Note: it may work unexpectedly if several conditions hold for the same event.'strict_order' \u2014 Don't allow interventions of other events. E.g. in the case of A->B->D->C, it stops finding A->B->C at the D and the max event level is 2.'strict_increase' \u2014 Apply conditions only to events with strictly increasing timestamps.'strict_once' \u2014 Count each event only once in the chain even if it meets the condition several times", "Returned value", "The maximum number of consecutive triggered conditions from the chain within the sliding time window.\nAll the chains in the selection are analyzed.", "Type: Integer."], "Examples": ["SELECT    level,    count() AS cFROM(    SELECT        user_id,        windowFunnel(6048000000000000)(timestamp, eventID = 1003, eventID = 1009, eventID = 1007, eventID = 1010) AS level    FROM trend    WHERE (event_date >= '2019-01-01') AND (event_date <= '2019-02-02')    GROUP BY user_id)GROUP BY levelORDER BY level ASC;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/parametric-functions"], "Title": ["retention"], "Feature": ["retention(cond1, cond2, ..., cond32);"], "Description": ["retention", "The function takes as arguments a set of conditions from 1 to 32 arguments of type UInt8 that indicate whether a certain condition was met for the event.\nAny condition can be specified as an argument (as in WHERE).", "The conditions, except the first, apply in pairs: the result of the second will be true if the first and second are true, of the third if the first and third are true, etc.", "Syntax", "retention(cond1, cond2, ..., cond32);", "Arguments", "cond \u2014 An expression that returns a UInt8 result (1 or 0).", "Returned value", "The array of 1 or 0.", "1 \u2014 Condition was met for the event.0 \u2014 Condition wasn\u2019t met for the event.", "Type: UInt8."], "Examples": ["CREATE TABLE retention_test(date Date, uid Int32) ENGINE = Memory;INSERT INTO retention_test SELECT '2020-01-01', number FROM numbers(5);INSERT INTO retention_test SELECT '2020-01-02', number FROM numbers(10);INSERT INTO retention_test SELECT '2020-01-03', number FROM numbers(15);", "SELECT * FROM retention_test", "SELECT    uid,    retention(date = '2020-01-01', date = '2020-01-02', date = '2020-01-03') AS rFROM retention_testWHERE date IN ('2020-01-01', '2020-01-02', '2020-01-03')GROUP BY uidORDER BY uid ASC", "SELECT    sum(r[1]) AS r1,    sum(r[2]) AS r2,    sum(r[3]) AS r3FROM(    SELECT        uid,        retention(date = '2020-01-01', date = '2020-01-02', date = '2020-01-03') AS r    FROM retention_test    WHERE date IN ('2020-01-01', '2020-01-02', '2020-01-03')    GROUP BY uid)"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/parametric-functions"], "Title": ["uniqUpTo(N)(x)"], "Feature": ["uniqUpTo(N)(x)"], "Description": ["uniqUpTo(N)(x)", "Calculates the number of different values of the argument up to a specified limit, N. If the number of different argument values is greater than N, this function returns N + 1, otherwise it calculates the exact value.", "Recommended for use with small Ns, up to 10. The maximum value of N is 100.", "For the state of an aggregate function, this function uses the amount of memory equal to 1 + N * the size of one value of bytes.\nWhen dealing with strings, this function stores a non-cryptographic hash of 8 bytes; the calculation is approximated for strings.", "For example, if you had a table that logs every search query made by users on your website. Each row in the table represents a single search query, with columns for the user ID, the search query, and the timestamp of the query. You can use uniqUpTo to generate a report that shows only the keywords that produced at least 5 unique users.", "SELECT SearchPhraseFROM SearchLogGROUP BY SearchPhraseHAVING uniqUpTo(4)(UserID) >= 5", "uniqUpTo(4)(UserID) calculates the number of unique UserID values for each SearchPhrase, but it only counts up to 4 unique values. If there are more than 4 unique UserID values for a SearchPhrase, the function returns 5 (4 + 1). The HAVING clause then filters out the SearchPhrase values for which the number of unique UserID values is less than 5. This will give you a list of search keywords that were used by at least 5 unique users."], "Examples": ["SELECT SearchPhraseFROM SearchLogGROUP BY SearchPhraseHAVING uniqUpTo(4)(UserID) >= 5"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/parametric-functions"], "Title": ["sumMapFiltered"], "Feature": ["CREATE TABLE sum_map(    `date` Date,    `timeslot` DateTime,    `statusMap` Nested(status UInt16, requests UInt64))ENGINE = LogINSERT INTO sum_map VALUES    ('2000-01-01', '2000-01-01 00:00:00', [1, 2, 3], [10, 10, 10]),    ('2000-01-01', '2000-01-01 00:00:00', [3, 4, 5], [10, 10, 10]),    ('2000-01-01', '2000-01-01 00:01:00', [4, 5, 6], [10, 10, 10]),    ('2000-01-01', '2000-01-01 00:01:00', [6, 7, 8], [10, 10, 10]);"], "Description": ["sumMapFiltered", "This function behaves the same as sumMap except that it also accepts an array of keys to filter with as a parameter. This can be especially useful when working with a high cardinality of keys.", "Syntax", "sumMapFiltered(keys_to_keep)(keys, values)", "Parameters", "keys_to_keep: Array of keys to filter with.keys: Array of keys.values: Array of values.", "Returned Value", "Returns a tuple of two arrays: keys in sorted order, and values summed for the corresponding keys."], "Examples": ["CREATE TABLE sum_map(    `date` Date,    `timeslot` DateTime,    `statusMap` Nested(status UInt16, requests UInt64))ENGINE = LogINSERT INTO sum_map VALUES    ('2000-01-01', '2000-01-01 00:00:00', [1, 2, 3], [10, 10, 10]),    ('2000-01-01', '2000-01-01 00:00:00', [3, 4, 5], [10, 10, 10]),    ('2000-01-01', '2000-01-01 00:01:00', [4, 5, 6], [10, 10, 10]),    ('2000-01-01', '2000-01-01 00:01:00', [6, 7, 8], [10, 10, 10]);", "SELECT sumMapFiltered([1, 4, 8])(statusMap.status, statusMap.requests) FROM sum_map;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/parametric-functions"], "Title": ["sumMapFilteredWithOverflow"], "Feature": ["CREATE TABLE sum_map(    `date` Date,    `timeslot` DateTime,    `statusMap` Nested(status UInt8, requests UInt8))ENGINE = LogINSERT INTO sum_map VALUES    ('2000-01-01', '2000-01-01 00:00:00', [1, 2, 3], [10, 10, 10]),    ('2000-01-01', '2000-01-01 00:00:00', [3, 4, 5], [10, 10, 10]),    ('2000-01-01', '2000-01-01 00:01:00', [4, 5, 6], [10, 10, 10]),    ('2000-01-01', '2000-01-01 00:01:00', [6, 7, 8], [10, 10, 10]);"], "Description": ["sumMapFilteredWithOverflow", "This function behaves the same as sumMap except that it also accepts an array of keys to filter with as a parameter. This can be especially useful when working with a high cardinality of keys. It differs from the sumMapFiltered function in that it does summation with overflow - i.e. returns the same data type for the summation as the argument data type.", "Syntax", "sumMapFilteredWithOverflow(keys_to_keep)(keys, values)", "Parameters", "keys_to_keep: Array of keys to filter with.keys: Array of keys.values: Array of values.", "Returned Value", "Returns a tuple of two arrays: keys in sorted order, and values summed for the corresponding keys."], "Examples": ["CREATE TABLE sum_map(    `date` Date,    `timeslot` DateTime,    `statusMap` Nested(status UInt8, requests UInt8))ENGINE = LogINSERT INTO sum_map VALUES    ('2000-01-01', '2000-01-01 00:00:00', [1, 2, 3], [10, 10, 10]),    ('2000-01-01', '2000-01-01 00:00:00', [3, 4, 5], [10, 10, 10]),    ('2000-01-01', '2000-01-01 00:01:00', [4, 5, 6], [10, 10, 10]),    ('2000-01-01', '2000-01-01 00:01:00', [6, 7, 8], [10, 10, 10]);", "SELECT sumMapFilteredWithOverflow([1, 4, 8])(statusMap.status, statusMap.requests) as summap_overflow, toTypeName(summap_overflow) FROM sum_map;", "SELECT sumMapFiltered([1, 4, 8])(statusMap.status, statusMap.requests) as summap, toTypeName(summap) FROM sum_map;"], "Category": ["Aggregate Functions"]}
{"HTML": ["https://clickhouse.com/docs/en/sql-reference/aggregate-functions/parametric-functions"], "Title": ["sequenceNextNode"], "Feature": ["sequenceNextNode(direction, base)(timestamp, event_column, base_condition, event1, event2, event3, ...)"], "Description": ["sequenceNextNode", "Returns a value of the next event that matched an event chain.", "Experimental function, SET allow_experimental_funnel_functions = 1 to enable it.", "Syntax", "sequenceNextNode(direction, base)(timestamp, event_column, base_condition, event1, event2, event3, ...)", "Parameters", "direction \u2014 Used to navigate to directions.forward \u2014 Moving forward.backward \u2014 Moving backward.base \u2014 Used to set the base point.head \u2014 Set the base point to the first event.tail \u2014 Set the base point to the last event.first_match \u2014 Set the base point to the first matched event1.last_match \u2014 Set the base point to the last matched event1.", "Arguments", "timestamp \u2014 Name of the column containing the timestamp. Data types supported: Date, DateTime and other unsigned integer types.event_column \u2014 Name of the column containing the value of the next event to be returned. Data types supported: String and Nullable(String).base_condition \u2014 Condition that the base point must fulfill.event1, event2, ... \u2014 Conditions describing the chain of events. UInt8.", "Returned values", "event_column[next_index] \u2014 If the pattern is matched and next value exists.NULL - If the pattern isn\u2019t matched or next value doesn't exist.", "Type: Nullable(String)."], "Examples": ["CREATE TABLE test_flow (    dt DateTime,    id int,    page String)ENGINE = MergeTree()PARTITION BY toYYYYMMDD(dt)ORDER BY id;INSERT INTO test_flow VALUES (1, 1, 'A') (2, 1, 'B') (3, 1, 'C') (4, 1, 'D') (5, 1, 'E');SELECT id, sequenceNextNode('forward', 'head')(dt, page, page = 'A', page = 'A', page = 'B') as next_flow FROM test_flow GROUP BY id;", "ALTER TABLE test_flow DELETE WHERE 1 = 1 settings mutations_sync = 1;INSERT INTO test_flow VALUES (1, 1, 'Home') (2, 1, 'Gift') (3, 1, 'Exit');INSERT INTO test_flow VALUES (1, 2, 'Home') (2, 2, 'Home') (3, 2, 'Gift') (4, 2, 'Basket');INSERT INTO test_flow VALUES (1, 3, 'Gift') (2, 3, 'Home') (3, 3, 'Gift') (4, 3, 'Basket');", "SELECT id, sequenceNextNode('forward', 'head')(dt, page, page = 'Home', page = 'Home', page = 'Gift') FROM test_flow GROUP BY id;                  dt   id   page 1970-01-01 09:00:01    1   Home // Base point, Matched with Home 1970-01-01 09:00:02    1   Gift // Matched with Gift 1970-01-01 09:00:03    1   Exit // The result 1970-01-01 09:00:01    2   Home // Base point, Matched with Home 1970-01-01 09:00:02    2   Home // Unmatched with Gift 1970-01-01 09:00:03    2   Gift 1970-01-01 09:00:04    2   Basket 1970-01-01 09:00:01    3   Gift // Base point, Unmatched with Home 1970-01-01 09:00:02    3   Home 1970-01-01 09:00:03    3   Gift 1970-01-01 09:00:04    3   Basket", "SELECT id, sequenceNextNode('backward', 'tail')(dt, page, page = 'Basket', page = 'Basket', page = 'Gift') FROM test_flow GROUP BY id;                 dt   id   page1970-01-01 09:00:01    1   Home1970-01-01 09:00:02    1   Gift1970-01-01 09:00:03    1   Exit // Base point, Unmatched with Basket1970-01-01 09:00:01    2   Home1970-01-01 09:00:02    2   Home // The result1970-01-01 09:00:03    2   Gift // Matched with Gift1970-01-01 09:00:04    2   Basket // Base point, Matched with Basket1970-01-01 09:00:01    3   Gift1970-01-01 09:00:02    3   Home // The result1970-01-01 09:00:03    3   Gift // Base point, Matched with Gift1970-01-01 09:00:04    3   Basket // Base point, Matched with Basket", "SELECT id, sequenceNextNode('forward', 'first_match')(dt, page, page = 'Gift', page = 'Gift') FROM test_flow GROUP BY id;                 dt   id   page1970-01-01 09:00:01    1   Home1970-01-01 09:00:02    1   Gift // Base point1970-01-01 09:00:03    1   Exit // The result1970-01-01 09:00:01    2   Home1970-01-01 09:00:02    2   Home1970-01-01 09:00:03    2   Gift // Base point1970-01-01 09:00:04    2   Basket  The result1970-01-01 09:00:01    3   Gift // Base point1970-01-01 09:00:02    3   Home // The result1970-01-01 09:00:03    3   Gift1970-01-01 09:00:04    3   Basket", "SELECT id, sequenceNextNode('forward', 'first_match')(dt, page, page = 'Gift', page = 'Gift', page = 'Home') FROM test_flow GROUP BY id;                 dt   id   page1970-01-01 09:00:01    1   Home1970-01-01 09:00:02    1   Gift // Base point1970-01-01 09:00:03    1   Exit // Unmatched with Home1970-01-01 09:00:01    2   Home1970-01-01 09:00:02    2   Home1970-01-01 09:00:03    2   Gift // Base point1970-01-01 09:00:04    2   Basket // Unmatched with Home1970-01-01 09:00:01    3   Gift // Base point1970-01-01 09:00:02    3   Home // Matched with Home1970-01-01 09:00:03    3   Gift // The result1970-01-01 09:00:04    3   Basket", "SELECT id, sequenceNextNode('backward', 'last_match')(dt, page, page = 'Gift', page = 'Gift') FROM test_flow GROUP BY id;                 dt   id   page1970-01-01 09:00:01    1   Home // The result1970-01-01 09:00:02    1   Gift // Base point1970-01-01 09:00:03    1   Exit1970-01-01 09:00:01    2   Home1970-01-01 09:00:02    2   Home // The result1970-01-01 09:00:03    2   Gift // Base point1970-01-01 09:00:04    2   Basket1970-01-01 09:00:01    3   Gift1970-01-01 09:00:02    3   Home // The result1970-01-01 09:00:03    3   Gift // Base point1970-01-01 09:00:04    3   Basket", "SELECT id, sequenceNextNode('backward', 'last_match')(dt, page, page = 'Gift', page = 'Gift', page = 'Home') FROM test_flow GROUP BY id;                 dt   id   page1970-01-01 09:00:01    1   Home // Matched with Home, the result is null1970-01-01 09:00:02    1   Gift // Base point1970-01-01 09:00:03    1   Exit1970-01-01 09:00:01    2   Home // The result1970-01-01 09:00:02    2   Home // Matched with Home1970-01-01 09:00:03    2   Gift // Base point1970-01-01 09:00:04    2   Basket1970-01-01 09:00:01    3   Gift // The result1970-01-01 09:00:02    3   Home // Matched with Home1970-01-01 09:00:03    3   Gift // Base point1970-01-01 09:00:04    3   Basket", "CREATE TABLE test_flow_basecond(    `dt` DateTime,    `id` int,    `page` String,    `ref` String)ENGINE = MergeTreePARTITION BY toYYYYMMDD(dt)ORDER BY id;INSERT INTO test_flow_basecond VALUES (1, 1, 'A', 'ref4') (2, 1, 'A', 'ref3') (3, 1, 'B', 'ref2') (4, 1, 'B', 'ref1');", "SELECT id, sequenceNextNode('forward', 'head')(dt, page, ref = 'ref1', page = 'A') FROM test_flow_basecond GROUP BY id;                  dt   id   page   ref 1970-01-01 09:00:01    1   A      ref4 // The head can not be base point because the ref column of the head unmatched with 'ref1'. 1970-01-01 09:00:02    1   A      ref3 1970-01-01 09:00:03    1   B      ref2 1970-01-01 09:00:04    1   B      ref1", "SELECT id, sequenceNextNode('backward', 'tail')(dt, page, ref = 'ref4', page = 'B') FROM test_flow_basecond GROUP BY id;                  dt   id   page   ref 1970-01-01 09:00:01    1   A      ref4 1970-01-01 09:00:02    1   A      ref3 1970-01-01 09:00:03    1   B      ref2 1970-01-01 09:00:04    1   B      ref1 // The tail can not be base point because the ref column of the tail unmatched with 'ref4'.", "SELECT id, sequenceNextNode('forward', 'first_match')(dt, page, ref = 'ref3', page = 'A') FROM test_flow_basecond GROUP BY id;                  dt   id   page   ref 1970-01-01 09:00:01    1   A      ref4 // This row can not be base point because the ref column unmatched with 'ref3'. 1970-01-01 09:00:02    1   A      ref3 // Base point 1970-01-01 09:00:03    1   B      ref2 // The result 1970-01-01 09:00:04    1   B      ref1", "SELECT id, sequenceNextNode('backward', 'last_match')(dt, page, ref = 'ref2', page = 'B') FROM test_flow_basecond GROUP BY id;                  dt   id   page   ref 1970-01-01 09:00:01    1   A      ref4 1970-01-01 09:00:02    1   A      ref3 // The result 1970-01-01 09:00:03    1   B      ref2 // Base point 1970-01-01 09:00:04    1   B      ref1 // This row can not be base point because the ref column unmatched with 'ref2'."], "Category": ["Aggregate Functions"]}
