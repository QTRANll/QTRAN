一句话总结

* 测试基础transfer llm的mariadb->postgres效果，为mutate lln的微调准备训练集和测试集

本周工作：
1. transfer llm：测试mariadb->postgres
* 修改增加：编写postgres的ddl语句；修改添加mariadb->postgres的few-shot样例；简化error-iteration过程中返回给llm的error message
* 测试效果：（ error_iteration=True，sqls_type="simple"，FewShot=True ）的200条数据，长度区间在1-240的101条数据以及长度区间在240-400的100条数据
* 分析语法和语义正确性：分析transferred sql的语义正确性低的具体原因
2. mutate llm：为pinolo的mutate strategies构造训练集和测试集
* 为18种mutate strategies构造大小为110的微调训练集和大小为180的测试集，验证正确性
* 编写18种mutate strategies的system message
* 形成符合微调任务的格式化training data和testing data并预测成本
3. [FSE'24] Evaluating and Improving ChatGPT for Unit Test Generation。重点看intro,motivation以及实验设计就好（下周三开会）



下周计划：
* 解决openai官方密钥问题，完成mutate llm的微调与微调模型效果评估
* empirical study 细粒度的element transfer不好做的话，可以考虑使用RAG
* 有空也可以看看这篇论文，我觉得他的这个思路挺好的，就是没有公开源码：Mallet SQL Dialect Translation with LLM Rule Generation
5. 此想法效果不大，还是要用domin knowledge，具体原因见官网关于fine-tuning的说明，微调可以覆盖一些不存在feature对应关系的边缘情况（- **Handling many edge cases in specific ways**）。      微调替代few-shot（transfer llm）：few-shot对效果（尤其是对于执行结果一样的比例）有较好的提升（主要去尝试mariadb转postgres的），微调可以考虑作为边缘知识的补充
6. 对于长度过长的sql，可以考虑拆分执行，再合并！！
7. 对于transfer llm的首次转换，可以考虑加意图提示（作用估计不大，因为sql语句不像method有明显的意图可供提取）

* mysql_like的ddl文件把insert语句都修改了一下（ok），重新把对应的数据表删除又建了一下（ok），后面应该要重新跑数据（还没重新跑mariadb to mysql）





先测能找到例子的八种（mariadb，mysql，tidb），在system里添加数据库类型信息，加一句角色扮演提示词
SQLancer

* 关于微调后效果：和数据集中的mutatedSqlsim比较；openai官网提到的微调模型评估
* 