## 重要链接
工具链接：
*  [openai/evals: Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks. (github.com)](https://github.com/openai/evals/tree/main/)

工具使用：
* [How To Best Leverage OpenAI’s Evals Framework | by Aparna Dhinakaran | Towards Data Science](https://towardsdatascience.com/how-to-best-leverage-openais-evals-framework-c38bcef0ec47)
* [evals · PyPI](https://pypi.org/project/evals/)
* [The Guide To LLM Evals: How To Build and Benchmark Your Evals | by Aparna Dhinakaran | Towards Data Science](https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)
* [Mastering OpenAI’s ‘evals’: A Deep Dive into Evaluating LLMs | by Xinzhe Li, PhD in Language Intelligence | Level Up Coding (gitconnected.com)](https://levelup.gitconnected.com/mastering-openais-evals-a-deep-dive-into-evaluating-llms-22433097bc99)
* [掌握 OpenAI 的 "evals"：深入评估LLMs - 腾讯云开发者社区-腾讯云 (tencent.com)](https://cloud.tencent.com/developer/news/1170800)
* [OpenAI Evals Demo: Using W&B Prompts to Run Evaluations | openai-evals – Weights & Biases (wandb.ai)](https://wandb.ai/wandb_fc/openai-evals/reports/OpenAI-Evals-Demo-Using-W-B-Prompts-to-Run-Evaluations--Vmlldzo0MTI4ODA3)

## 使用教程
1. [[Evals_README]], [[Evals_README_ch]]
2. [[How To Best Leverage OpenAI’s Evals Framework]]
3. [[Mastering OpenAI’s ‘evals’_A Deep Dive into Evaluating LLMs]]